{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJcQ9h-_xj4o"
      },
      "outputs": [],
      "source": [
        "# default_exp core"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdJ6lqAe2Qbz"
      },
      "source": [
        "# 01_core Objectives\n",
        "To create end-to-end multimodal classifers based on Fastai-tabular, Fastai-text and Fastai-vision.\n",
        "\n",
        "Specifically, I will construct 3 types of multimodal model:\n",
        "\n",
        "- `early concat`: concatinate cnt, cat, txt, img after data loading and data preprocessing, followed by a learner of choice (e.g. fastai tabular).\n",
        "- `middle concat`: concatinate the embeddings from each of the trained tab (cnt+cat), txt, img models, followed by a learner of choice.\n",
        "- `late concat`: concatinate the probability predictions from each of the trained tab(cnt+cat), txt, img models, followed by a learner of choice.\n",
        "\n",
        "Using a few benchmark datasets, I will compare the 3 types of multimodal models on their\n",
        "\n",
        "- computation efficiency\n",
        "- ML performance\n",
        "- interpretability\n",
        "\n",
        "\n",
        "**Note**: this notebook is inheriated from [03_tech_nontech_classification.ipynb](https://colab.research.google.com/drive/1H23iYu2UNNMC4XMqQF72IJcQS6yrrHcB?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OfTVjm6D891"
      },
      "source": [
        "## dev plan\n",
        "\n",
        "**Features to Build**\n",
        "- [/5] For iu data:  use the majority vote of k nearest neightbors as recommendation/prediction. Here neighbors can be selected from both the iu population (knn of iu_embs) and same person's past meeting (knn of i_embs).\n",
        "\n",
        "Reference: [calpal-recommenders-part1-fastai.ipynb](https://colab.research.google.com/drive/19HlugtvFmzarBi0WRYmc4_xW4_9QD3lH?usp=sharing)\n",
        "- [/5] Error Analysis for end2end model, confusion matrix, classification report, df_FP, df_FN\n",
        "- [/5] `early concat` method: create hybrid dataloader\n",
        "- [/5] XAI feature importance for both population and individual; what-if analysis\n",
        "- [/5] Feature selection\n",
        "- [/5] hyperparameter-tuning \n",
        "- [/5] learning from big data by dask https://gdmarmerola.github.io/big-data-ml-training/\n",
        "\n",
        "**Features Built**\n",
        "- [5/5] modeling txt_cols: train_fastai_text_classifier(), get_fastai_docs_embs()\n",
        "\n",
        "- [5/5] modeling img_cols: train_fastai_image_classifier(), get_fastai_imgs_embs()\n",
        "\n",
        "- [5/5] modeling tab_cols=cnt_cols+cat_cols: train_fastai_tabular_classifier(), get_fastai_tab_embs()\n",
        "- [5/5] ensembled modeling embs_ls, probs_ls: train_ensembled_classifier()\n",
        "\n",
        "- [5/5] end to end modeling txt_cols, img_cols, tab_cols: Fastai_Multimodal_Classifier()\n",
        "\n",
        "- [5/5] construct 4 benchmark datasets: \n",
        "   - dataset0 (cnt, cat): income_level\n",
        "   - dataset1 (txt, img) : entailment \n",
        "   - dataset2 (cnt, cat, txt): pet adoption speed\n",
        "   - dataset3 (cnt, cat, txt): salary\n",
        "\n",
        "- [5/5] experiment configuration: \n",
        "   - i: select which dataset\n",
        "   - nrows: select the size of df (for fast prototyping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmioYTbGLZ3J"
      },
      "source": [
        "# install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCT5HAPSxj4q",
        "outputId": "16a4732f-a00e-4942-d2b1-231ebd99ad48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nbdev in /usr/local/lib/python3.7/dist-packages (1.2.5)\n",
            "Requirement already satisfied: jupyter-client<8 in /usr/local/lib/python3.7/dist-packages (from nbdev) (7.2.2)\n",
            "Requirement already satisfied: Jinja2<3.1.0 in /usr/local/lib/python3.7/dist-packages (from nbdev) (2.11.3)\n",
            "Requirement already satisfied: fastcore>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbdev) (1.4.1)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from nbdev) (21.1.3)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from nbdev) (4.10.1)\n",
            "Requirement already satisfied: fastrelease in /usr/local/lib/python3.7/dist-packages (from nbdev) (0.1.12)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from nbdev) (1.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from nbdev) (3.13)\n",
            "Requirement already satisfied: ghapi in /usr/local/lib/python3.7/dist-packages (from nbdev) (0.1.20)\n",
            "Requirement already satisfied: nbconvert>=6.1 in /usr/local/lib/python3.7/dist-packages (from nbdev) (6.4.5)\n",
            "Requirement already satisfied: nbformat>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from nbdev) (5.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from nbdev) (21.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.1.0->nbdev) (2.0.1)\n",
            "Requirement already satisfied: tornado>=6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8->nbdev) (6.1)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8->nbdev) (5.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8->nbdev) (2.8.2)\n",
            "Requirement already satisfied: nest-asyncio>=1.5.4 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8->nbdev) (1.5.4)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8->nbdev) (4.9.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8->nbdev) (0.4)\n",
            "Requirement already satisfied: pyzmq>=22.3 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8->nbdev) (22.3.0)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (0.1.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (4.6.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (1.5.0)\n",
            "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (0.5.13)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (4.1.0)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (2.6.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (0.6.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.1->nbdev) (0.7.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.4.0->nbdev) (4.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4.0->nbdev) (3.10.0.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4.0->nbdev) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4.0->nbdev) (5.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4.0->nbdev) (4.11.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4.0->nbdev) (21.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4.0->nbdev) (3.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.8.2->jupyter-client<8->nbdev) (1.15.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert>=6.1->nbdev) (0.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->nbdev) (5.5.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->nbdev) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->nbdev) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->nbdev) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->nbdev) (57.4.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->nbdev) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->nbdev) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->nbdev) (0.2.5)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->nbdev) (5.2.2)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->nbdev) (7.7.0)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->nbdev) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->nbdev) (5.3.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->nbdev) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->nbdev) (1.1.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->nbdev) (3.6.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->nbdev) (0.13.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->nbdev) (1.8.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->nbdev) (0.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->nbdev) (3.0.7)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->nbdev) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nbdev\n",
        "\n",
        "# install most updated fastai & utils\n",
        "! [ -e /content ] && pip install -Uqq fastai  \n",
        "#!pip install git+https://github.com/fastai/fastai # to deal with Error:  found at least two devices, cuda:0 and cpu\n",
        "\"\"\"\n",
        "!pip install fastai wwf bayesian-optimization -q --upgrade\n",
        "!pip install autogluon\n",
        "\"\"\"\n",
        "\n",
        "# auto 'RESET RUNTIME'\n",
        "try:\n",
        "  import nbdev\n",
        "except ImportError:\n",
        "  import os\n",
        "  os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EWfy_J8L6C2"
      },
      "source": [
        "# nbdev setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRHkWir0xj4r"
      },
      "source": [
        "Since we don't have access to our Drive yet, be sure to hit the `Mount Drive` to mount it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrnEPRHpxj4r",
        "outputId": "ce7492d8-8238-4470-8ed9-8a09c6166903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4spqi2Uxj4r"
      },
      "source": [
        "Now let's work out of our new library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dpXmkyTAxj4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36282bbb-dabb-4859-8336-c27211ecc785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Rj685Pijxj4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a00741e-63d6-4aac-e677-9326bab73cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/fastai_multimodal\n"
          ]
        }
      ],
      "source": [
        "git_path = Path('drive/My Drive/fastai_multimodal')\n",
        "#git_path = Path('drive/My Drive/techskills')\n",
        "os.chdir(git_path)\n",
        "!pwd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4TgYJyF8xj4t"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "from nbdev_colab.core import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9o24Rz0xj4t"
      },
      "source": [
        "We'll make a quick addition function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcs0sQGaxj4t"
      },
      "source": [
        "Now let's put in our hooks and update our library. We can just work out of our local directory now as we changed our working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5JyOCAMvxj4u"
      },
      "outputs": [],
      "source": [
        "#colab\n",
        "setup_git('.', 'fastai_multimodal', 'wjlgatech', 'my-github-token', 'wjlgatech@gmail.com')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BB2yGM2bxj4u"
      },
      "outputs": [],
      "source": [
        "#colab\n",
        "#git_push('.', '01 after simplify and re-organize this notebook')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Yute0AVy0PNN"
      },
      "outputs": [],
      "source": [
        "start = os.getcwd()\n",
        "os.chdir('.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl-KFWTI0hKN",
        "outputId": "190c5aeb-d421-4830-da63-8729d975093d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing: git config --local include.path ../.gitconfig\n",
            "Success: hooks are installed and repo's .gitconfig is now trusted\n",
            "Converted 00_core.ipynb.\n",
            "Converted 01_modules.ipynb.\n",
            "Converted fastai2_multimodal_tabtxt_public.ipynb.\n",
            "Converted fastai_multimodal.ipynb.\n",
            "Converted index.ipynb.\n"
          ]
        }
      ],
      "source": [
        "!nbdev_install_git_hooks\n",
        "!nbdev_build_lib\n",
        "!git add *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA1QUKyJ047T",
        "outputId": "66a6b47a-2c36-4457-f3ce-d33e278554eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[master e6f3cf4] 04/01/22 5:40pm add Error Analysis & bigdata ML 2 solutions\n",
            " 11 files changed, 3685 insertions(+), 485 deletions(-)\n",
            " rewrite model/tabular_ensemble_enbeddings.pth (94%)\n",
            " rewrite model/tabular_model.pth (89%)\n",
            "remote: Invalid username or password.\n",
            "fatal: Authentication failed for 'https://wjlgatech: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@github.com/wjlgatech/fastai_multimodal.git/'\n"
          ]
        }
      ],
      "source": [
        "!git commit -m \"04/01/22 5:40pm add Error Analysis & bigdata ML 2 solutions\"\n",
        "!git push origin master"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPmRu4D5yUhy"
      },
      "source": [
        "# load packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AAtnoCl-ioI",
        "outputId": "475ecda5-9c43-41dc-a985-aa7ff418a257"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 7.6 MB/s \n",
            "\u001b[?25h  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install fastai wwf bayesian-optimization -q --upgrade \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ybmjzR0m1qWt"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import requests\n",
        "import re\n",
        "import os\n",
        "\n",
        "from fastai.tabular.all import *\n",
        "from fastai.text.all import *\n",
        "from fastai.vision.all import *\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRww0exkN4MS"
      },
      "source": [
        "# setup experiment\n",
        "\n",
        "Now set up experiemnt by choosing these experiment configs:\n",
        "\n",
        "- i: which dataset to choose\n",
        "- nrows: what large the dataset is\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        },
        "id": "ItVbrgfhN9o4",
        "outputId": "c4666020-6031-46f7-93cf-862c8ce367b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========experiment config===========\n",
            " nrows=500 \n",
            " data_file=df_entailment.csv\n",
            " label_col=label\n",
            " img_path=/root/.keras/datasets/tweet_images\n",
            "===============================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                       text_1  \\\n",
              "495                                                                                                            Friends, interested all go to have a look!\\n@KaitlinNichole8 @Durm_hmmm @NWSCC_PTK @cook1_hannah @Offical_acount_ @thoughtsshifted @kylia1317 @AddieHeaps @tia_hollis_ @Gabe_Armidano3 https://t.co/aGijewqxUr   \n",
              "496  The clouds arrived today, and light #rain &amp; #snow showers have begun around northern #California. After a brief break much of #Saturday, a better chance for more rain &amp; snow arrives on #Sunday. @JasonStiff has your complete Storm Tracker #Weather forecast: https://t.co/qGIrrcfAir https://t.co/AHw2xeDNll   \n",
              "497                                                                                                                                                                                                             I have 23 new followers from Nigeria, and more last week. See https://t.co/Teyxx9xcAZ https://t.co/WVIQuPaGY4   \n",
              "498                                                                                                                                                                                                                           #angeduturfL13012 hastag 👈 \\n\\nLigue 1 🥇\\n\\n@angeduturf \\n@PMU_Hippique https://t.co/b1vDi2nzbG   \n",
              "499                                                                                                                                                                                                                                                          What's the pick? #MTG #P1P1 #MTGZendikar https://t.co/ikBYt16esm   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                  text_2  \\\n",
              "495                                                                                                                   Friends, interested all go to have a look!\\n@joewdutra @KevinScullinNYC @ArturoAlbarrn2 @BrScho2 @thckwth_davicks @Harry2671 @VMHRN99 @RB33973035 @DaleJBuchanan @VishalA7 https://t.co/AqgVlCSvJ1   \n",
              "496  After #rain &amp; #snow on #Sunday, many parts of northern #California awoke to a chilly #Monday morning with wet / snowy roads and #fog. #Tuesday will also begin chilly with areas of fog. @JasonStiff has your complete Storm Tracker Weather forecast: https://t.co/IhGRaBg4DG #weather https://t.co/E01PgZEenv   \n",
              "497                                                                                                                                                                                                  I have 58 new followers from Malaysia, UK., and more last week. See https://t.co/feUcRhuuEJ https://t.co/RccdVWtBaz   \n",
              "498                                                                                                                                                                                               #angeduturfL2100121 hastag 👈 \\n\\nLigue 2 🥈\\n\\n@angeduturf \\n@PMU_Hippique \\n@ChristopheEscuder https://t.co/OWmbBhGyRo   \n",
              "499                                                                                                                                                                                                                            Choose wisely! #MTG #P1P1 #MTGZendikar\\n\\nhttps://t.co/zjLCqwoltj https://t.co/dAX5tlIuvd   \n",
              "\n",
              "            label                                                image_1_path  \\\n",
              "495  NoEntailment  /root/.keras/datasets/tweet_images/1369402302881206275.jpg   \n",
              "496  NoEntailment  /root/.keras/datasets/tweet_images/1337584390394818561.jpg   \n",
              "497  NoEntailment  /root/.keras/datasets/tweet_images/1382007535947943939.jpg   \n",
              "498  NoEntailment  /root/.keras/datasets/tweet_images/1344015899863490561.jpg   \n",
              "499  NoEntailment  /root/.keras/datasets/tweet_images/1340265626087075840.jpg   \n",
              "\n",
              "                                                   image_2_path  \n",
              "495  /root/.keras/datasets/tweet_images/1376730559867613185.jpg  \n",
              "496  /root/.keras/datasets/tweet_images/1338671609851355137.jpg  \n",
              "497  /root/.keras/datasets/tweet_images/1383373369249202186.jpg  \n",
              "498  /root/.keras/datasets/tweet_images/1347987010615713793.jpg  \n",
              "499  /root/.keras/datasets/tweet_images/1344282090553143296.jpg  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ced63c13-8093-401f-9a9a-e2e81ccd3744\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_1</th>\n",
              "      <th>text_2</th>\n",
              "      <th>label</th>\n",
              "      <th>image_1_path</th>\n",
              "      <th>image_2_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>Friends, interested all go to have a look!\\n@KaitlinNichole8 @Durm_hmmm @NWSCC_PTK @cook1_hannah @Offical_acount_ @thoughtsshifted @kylia1317 @AddieHeaps @tia_hollis_ @Gabe_Armidano3 https://t.co/aGijewqxUr</td>\n",
              "      <td>Friends, interested all go to have a look!\\n@joewdutra @KevinScullinNYC @ArturoAlbarrn2 @BrScho2 @thckwth_davicks @Harry2671 @VMHRN99 @RB33973035 @DaleJBuchanan @VishalA7 https://t.co/AqgVlCSvJ1</td>\n",
              "      <td>NoEntailment</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1369402302881206275.jpg</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1376730559867613185.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>The clouds arrived today, and light #rain &amp;amp; #snow showers have begun around northern #California. After a brief break much of #Saturday, a better chance for more rain &amp;amp; snow arrives on #Sunday. @JasonStiff has your complete Storm Tracker #Weather forecast: https://t.co/qGIrrcfAir https://t.co/AHw2xeDNll</td>\n",
              "      <td>After #rain &amp;amp; #snow on #Sunday, many parts of northern #California awoke to a chilly #Monday morning with wet / snowy roads and #fog. #Tuesday will also begin chilly with areas of fog. @JasonStiff has your complete Storm Tracker Weather forecast: https://t.co/IhGRaBg4DG #weather https://t.co/E01PgZEenv</td>\n",
              "      <td>NoEntailment</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1337584390394818561.jpg</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1338671609851355137.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>I have 23 new followers from Nigeria, and more last week. See https://t.co/Teyxx9xcAZ https://t.co/WVIQuPaGY4</td>\n",
              "      <td>I have 58 new followers from Malaysia, UK., and more last week. See https://t.co/feUcRhuuEJ https://t.co/RccdVWtBaz</td>\n",
              "      <td>NoEntailment</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1382007535947943939.jpg</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1383373369249202186.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>#angeduturfL13012 hastag 👈 \\n\\nLigue 1 🥇\\n\\n@angeduturf \\n@PMU_Hippique https://t.co/b1vDi2nzbG</td>\n",
              "      <td>#angeduturfL2100121 hastag 👈 \\n\\nLigue 2 🥈\\n\\n@angeduturf \\n@PMU_Hippique \\n@ChristopheEscuder https://t.co/OWmbBhGyRo</td>\n",
              "      <td>NoEntailment</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1344015899863490561.jpg</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1347987010615713793.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>What's the pick? #MTG #P1P1 #MTGZendikar https://t.co/ikBYt16esm</td>\n",
              "      <td>Choose wisely! #MTG #P1P1 #MTGZendikar\\n\\nhttps://t.co/zjLCqwoltj https://t.co/dAX5tlIuvd</td>\n",
              "      <td>NoEntailment</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1340265626087075840.jpg</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1344282090553143296.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ced63c13-8093-401f-9a9a-e2e81ccd3744')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ced63c13-8093-401f-9a9a-e2e81ccd3744 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ced63c13-8093-401f-9a9a-e2e81ccd3744');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "#choose the ith dataset\n",
        "i = 1\n",
        "#choose df size\n",
        "nrows=5*10**2\n",
        "\n",
        "#creat experiment config df\n",
        "\n",
        "config_df = pd.DataFrame({'nrows': [nrows]*5, \n",
        "                       'data_file': ['df_income.csv','df_entailment.csv', 'df_adoption.csv','df_salary.csv','iu_2022_101_325.csv'],\n",
        "                       'label_col':[\"income_level\", 'label', 'AdoptionSpeed', 'salary','response_status'],\n",
        "                       'img_path':[None,'/root/.keras/datasets/tweet_images',None, None,None ]\n",
        "                       })\n",
        "\n",
        "print(f\"===========experiment config===========\\n nrows={config_df.loc[i, 'nrows']} \\n data_file={config_df.loc[i,'data_file']}\\n label_col={config_df.loc[i, 'label_col']}\\n img_path={config_df.loc[i, 'img_path']}\\n===============================\")\n",
        "data_path=['/content/drive/MyDrive/fastai_multimodal/datasets/', '/content/drive/MyDrive/tf_multimodal/datasets/', '/content/drive/MyDrive/techskills_data/'][0]\n",
        "# define df, label_col, img_path\n",
        "df = pd.read_csv(data_path+config_df.loc[i,'data_file'], nrows=nrows, index_col=0)\n",
        "label_col=config_df.loc[i, 'label_col']\n",
        "img_path=config_df.loc[i, 'img_path']\n",
        "\n",
        "# keep a copy of experiment config\n",
        "config = {'df':df,\n",
        "          'data_path':data_path,\n",
        "          'data_file':config_df.loc[i,'data_file'],\n",
        "          'label_col':label_col,\n",
        "          'img_path':img_path,\n",
        "          'nrows':nrows}\n",
        "\n",
        "df.tail()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5AbxzjwbHik",
        "outputId": "5249db2f-3690-43ed-b5a9-379ced5e912c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NoEntailment     0.718\n",
              "Implies          0.150\n",
              "Contradictory    0.132\n",
              "Name: label, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df[label_col].value_counts()/df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MPha3r5-vS4"
      },
      "source": [
        "# train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RNZ-Aw51_NNU"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "def split_train_valid_test(df, train_valid_test=[0.7,0.15, 0.15], target='response_status', random_state=123, sort_split_by_col='start_datetime'):\n",
        "    '''Splits a Pandas Dataframe into training, evaluation and serving sets, stratifying on target column.\n",
        "\n",
        "    Args:\n",
        "            df : pandas dataframe to split\n",
        "            train_valid_test: a list of 3 positive numbers, each being either float or integer\n",
        "            target (string): the name of target column\n",
        "            random_state (int or None): the random seed to shuffle df; if None, do not shuffle df\n",
        "            sort_split_by_col (str or list of str) e.g.'index', 'start_datetime' or ['start_datetime','event_end_datetime']\n",
        "    Returns:\n",
        "            train_df: Training dataframe(i.e. 70% of the entire dataset)\n",
        "            valid_df: Evaluation dataframe (i.e. 15% of the entire dataset) \n",
        "            test_df: Serving dataframe (i.e. 15% of the entire dataset, label column dropped)\n",
        "            keep_datetime_order (default True): after splitting data into train < validation < serving\n",
        "    Ref:\n",
        "        C2W1_assignment.ipynb using TFDV to visulize, validate and moritor data at scale\n",
        "    '''\n",
        "    if len(train_valid_test)==3 and not any(x < 0 for x in train_valid_test):\n",
        "        tot = sum(train_valid_test)\n",
        "        train_valid_test = [x/tot for x in train_valid_test]\n",
        "    else: \n",
        "        raise ValueError('train_valid_test need to be a list of 3 positive numbers!')\n",
        "\n",
        "    if sort_split_by_col is not None:\n",
        "        if sort_split_by_col=='index':\n",
        "            df.sort_index(inplace=True) #for ui, datetime info is stored in df.index\n",
        "            df.reset_index(drop=False, inplace=True)\n",
        "        \n",
        "        ls = list(range(df.shape[0])) #range_of(df)  \n",
        "        df.sort_values(by=sort_split_by_col, inplace=True)\n",
        "        ls_train = ls[:int(df.shape[0]*train_valid_test[0])]\n",
        "        train_cut_date = df.iloc[ls_train[-1],:][sort_split_by_col]\n",
        "        train_df = df[df[sort_split_by_col]<=train_cut_date]\n",
        "\n",
        "        ls_test = ls[:int(df.shape[0]*sum(train_valid_test[0:2]))]\n",
        "        test_cut_date = df.iloc[ls_test[-1],:][sort_split_by_col]\n",
        "        test_cut_date = max(test_cut_date, train_cut_date)\n",
        "        test_df = df[df[sort_split_by_col]>test_cut_date]\n",
        "\n",
        "        try: valid_df=df[df[sort_split_by_col]>train_cut_date & df[sort_split_by_col]<=test_cut_date]\n",
        "        except: valid_df = pd.DataFrame()\n",
        "\n",
        "        ls_train = ls[:int(train_df.shape[0])]\n",
        "        ls_test = ls[-int(test_df.shape[0]):]\n",
        "        try: ls_valid = ls[int(train_df.shape[0]):-int(test_df.shape[0])]\n",
        "        except: ls_valid = []\n",
        "        n_train, n_valid, n_serv = train_df.shape[0], valid_df.shape[0], test_df.shape[0]\n",
        "        print('================Double check the indices of train, valid and test are sorted: =================== ')\n",
        "        print(f'train_df={df.iloc[:n_train,:][sort_split_by_col]}/n')\n",
        "        print(f'valid_df={df.iloc[n_train:(n_train+n_valid),:][sort_split_by_col]}/n')\n",
        "        print(f'test_df={df.iloc[(n_train+n_valid):,:][sort_split_by_col]}')\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "        train_df, valid_df, test_df  = df.iloc[:n_train,:], df.iloc[n_train:(n_train+n_valid),:], df.iloc[(n_train+n_valid):,:]#.drop([target], axis=1)\n",
        "        return train_df, valid_df, test_df# X_test, y_test\n",
        "\n",
        "    # downstream dl clf can not accept datetime index, therefore df.reset_index()   \n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    train_df, eval_serv = train_test_split(df, stratify=df[target], test_size = 1 - train_valid_test[0], random_state=random_state)\n",
        "    if train_valid_test[1]>0:\n",
        "        valid_df, test_df = train_test_split(eval_serv, stratify=eval_serv[target], test_size = train_valid_test[1]/(1 - train_valid_test[0]), random_state=random_state)\n",
        "    else:\n",
        "        valid_df, test_df = None, eval_serv\n",
        "    # Serving data emulates the data that would be submitted for predictions, so it should not have the label column.\n",
        "    #y_test = test_df[target]\n",
        "    #X_test = test_df.drop([target], axis=1)\n",
        "\n",
        "    return train_df, valid_df, test_df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHSNS32iiCp2",
        "outputId": "4039e645-4b7a-44c6-b0bc-4c33c8441cb3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((349, 5), (151, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# split df into train_df & test_df NOTE: random split by setting `sort_split_by_col=None`\n",
        "if i==4:\n",
        "  sort_split_by_col='start_datetime'\n",
        "else:\n",
        "  sort_split_by_col=None\n",
        "train_df, _, test_df = split_train_valid_test(df, train_valid_test=[0.7,0, 0.3], target=label_col, random_state=123, sort_split_by_col=sort_split_by_col)\n",
        "train_df.shape, test_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QAyThfFRCfY"
      },
      "source": [
        "# identify cnt_cols, cat_cols, txt_cols, img_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "72QG2VumRX76"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "import requests\n",
        "\n",
        "def check_path(path):\n",
        "    \"\"\"check if path is a valid directory or not\"\"\"\n",
        "    try:\n",
        "      return os.path.exists(os.path.dirname(path))\n",
        "    except:\n",
        "      return False\n",
        "def check_url(path):\n",
        "    \"\"\"check if path is a valid url or not\"\"\"\n",
        "    try: return requests.get(path)\n",
        "    except:\n",
        "      if 'http' in path:\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "\n",
        "def cnt_cat_txt_img_split(df:pd.DataFrame, cnt_card=0.5, excluded_cols = [label_col], txt_card=0.5):\n",
        "    \"\"\"Helper function that returns column names of cnt, cat (furtherly split into int_cat, str_cat), txt variables from given df.\n",
        "    Args: \n",
        "      df\n",
        "      cnt_card (int or float within 0 and 1): cnt cardinarity, e.g. ratio of unique values for cnt column\n",
        "      label (str default None): the target/dependant varible column name\n",
        "      txt_card (int or float within 0 and 1): txt cardinarity, e.g. ratio of unique values for txt column\n",
        "    Return:\n",
        "      txt_cols, cnt_cols, cat_cols, (int_cat_cols, str_cat_cols), img_cols: (list of str)\n",
        "    Example:\n",
        "      txt_cols, cnt_cols, cat_cols, (int_cat_cols, str_cat_cols), img_cols = cnt_cat_txt_split(df, cnt_card=80, label='target', txt_card=0.5)\n",
        "\n",
        "    \"\"\"\n",
        "    # init placeholder for cnt, cat (int_cat, str_cat), txt\n",
        "    cnt_cols, cat_cols, txt_cols, img_cols = [], [], [], []\n",
        "    int_cat_cols, str_cat_cols = [], []\n",
        "\n",
        "    # prep cnt cardinality & txt cardinality\n",
        "    if cnt_card < 1:\n",
        "        print(f'before adjustment...cnt_card={cnt_card}')\n",
        "        cnt_card = int(df.shape[0]*cnt_card)\n",
        "        print(f'before adjustment...cnt_card={cnt_card}')\n",
        "    if txt_card < 1:\n",
        "        print(f'before adjustment...txt_card={txt_card}')\n",
        "        txt_card = int(df.shape[0]*txt_card)\n",
        "        print(f'before adjustment...txt_card={txt_card}')\n",
        "    # exclude target\n",
        "    cols = set(df.columns) - set(excluded_cols)\n",
        "\n",
        "    # separate cnt, cat, txt columns\n",
        "    for col in cols:\n",
        "        if ((pd.api.types.is_integer_dtype(df[col].dtype) and\n",
        "            df[col].unique().shape[0] > cnt_card) or\n",
        "            pd.api.types.is_float_dtype(df[col].dtype)): #add to cnt_cols\n",
        "            cnt_cols.append(col)\n",
        "        elif (pd.api.types.is_string_dtype(df[col].dtype) and\n",
        "            df[col].unique().shape[0] > txt_card):\n",
        "            if all(['.png' in x or '.jpg' in x for x in df[col].sample(10)]): # and (all([check_url(path) for path in df[col].sample(10)]) or all([check_path(path) for path in df[col].sample(10)])): #check 10 samples to see if they are either valid url or valid path \n",
        "              img_cols.append(col)\n",
        "            else: #add to txt_cols\n",
        "              txt_cols.append(col)\n",
        "        else: #add to cat_cols\n",
        "            cat_cols.append(col)\n",
        "            if pd.api.types.is_integer_dtype(df[col].dtype): #separate cat_cols into int_cat_cols and str_cat_cols\n",
        "              int_cat_cols.append(col)\n",
        "            else:\n",
        "              str_cat_cols.append(col)\n",
        "    return sorted(txt_cols), sorted(cnt_cols), sorted(cat_cols), (sorted(int_cat_cols), sorted(str_cat_cols)), sorted(img_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie0IcWKJSiPE",
        "outputId": "d9a0f91c-3076-404e-85ca-a20f6c9f2c52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before adjustment...txt_card=0.1\n",
            "before adjustment...txt_card=50\n",
            "Given label_col=label\n",
            "=======  automatically identify \n",
            " cnt_cols=[]\n",
            " cat_cols=[],\n",
            " img_cols=['image_1_path', 'image_2_path'], \n",
            " txt_cols=['text_1', 'text_2'] \n",
            "======= make sure that is what you expect!\n"
          ]
        }
      ],
      "source": [
        "# determine cnt, cat, txt, img columns and define global variables\n",
        "txt_cols, cnt_cols, cat_cols, (int_cat_cols, str_cat_cols), img_cols= cnt_cat_txt_img_split(df=df, cnt_card=20, excluded_cols = [label_col], txt_card=0.1)\n",
        "print(f'Given label_col={label_col}\\n=======  automatically identify \\n cnt_cols={cnt_cols}\\n cat_cols={cat_cols},\\n img_cols={img_cols}, \\n txt_cols={txt_cols} \\n======= make sure that is what you expect!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ix8Vg7CYl4BD"
      },
      "outputs": [],
      "source": [
        "if i ==4: #manually adjust\n",
        "    cnt_cols=['age', 'cumulative_peer_exit_count',  'is_optional', 'is_organizer', 'length_of_service', 'manager_length_of_service', 'meeting_lapse', 'num_direct_reports', 'start_datetime_Dayofyear', 'time_since_last_promotion', 'time_since_new_manager_start_date', 'time_since_new_org_start_date', 'timestamp']\n",
        "    cat_cols=[ 'job_family', 'start_datetime']\n",
        "    img_cols=[]\n",
        "    txt_cols=['description', 'title']\n",
        "    excluded_cols=['event_id',]\n",
        "    x_cols = cnt_cols+cat_cols+img_cols+txt_cols\n",
        "    i_cols = ['description', 'title', 'start_datetime',  'meeting_lapse',  'start_datetime_Dayofyear',  'timestamp'] # item features\n",
        "    u_cols = list(set(x_cols)-set(i_cols)) # user features\n",
        "\n",
        "    print(f'meeting features: {i_cols}') # to generate item embeddings\n",
        "    print(f'user features: {u_cols}') # to generate user embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXFTvKvuB2sB",
        "outputId": "5f5aa5e7-d55b-4980-fd34-cd10048152c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([], [], ['text_1', 'text_2'], ['image_1_path', 'image_2_path'])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# store column-info in a unchangable variable\n",
        "class CONST(object):\n",
        "    __slots__ = ()\n",
        "    cat_cols = cat_cols\n",
        "    cnt_cols = cnt_cols\n",
        "    txt_cols = txt_cols\n",
        "    img_cols = img_cols\n",
        "    label_col = label_col\n",
        "\n",
        "\n",
        "c = CONST()\n",
        "c.cat_cols, c.cnt_cols, c.txt_cols, c.img_cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3DKq1fFleje"
      },
      "source": [
        "# 1) fastai text classifier\n",
        "\n",
        "The limitation of fastai text classifier is that it only accept 1 txt_col. To deal with this limitation, I have 2 options:\n",
        "\n",
        "- run fastai text classifier through each of txt_cols and then later combine the output through some ensemble learner.\n",
        "\n",
        "- combine all txt_cols into one text col and run fastai text classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD6g_p6Oha8w"
      },
      "source": [
        "## module1: train|reload|inference with fastai_text_classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Wt-hs9SRHr6M"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "#! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n",
        "#from fastai.text.all import *\n",
        "\n",
        "def train_fastai_text_classifier(df:pd.DataFrame, txt_col:str, label_col:str, model_path:str, lr:float=0.005, max_epochs:int=100, emb_size:int=128):\n",
        "    \"\"\"train a fastai text classifier and get its performance metrics\n",
        "    Args:\n",
        "      df:pd.DataFrame the dataframe containing text_col and label_col\n",
        "      txt_col:str e.g. 'hard_skills_name' \n",
        "      label_col:str e.g. 'label'\n",
        "      model_path:str e.g. '/content/drive/My Drive/techskills/model/'\n",
        "      flag_auto_lr:bool=False whether or not use auto search learning rate; if False, use default value 0.005 \n",
        "      max_epochs:int=100\n",
        "      emb_size:int=128\n",
        "    Returns:\n",
        "      lm: the trained language model\n",
        "      clf: the trained fastai text classification model   \n",
        "    \"\"\"\n",
        "\n",
        "    #metrics\n",
        "    f1=FBeta(beta=1, average='weighted')\n",
        "    precision = Precision(average='weighted')\n",
        "    recall = Recall(average='weighted')\n",
        "    metrics=[accuracy, precision, recall, f1]\n",
        "        \n",
        "    # get `dataloader` object for language model\n",
        "    dls_lm = TextDataLoaders.from_df(df[[txt_col, label_col]], is_lm=True, text_col=txt_col, label_col=label_col, valid_pct=0.2, seed=123)\n",
        "\n",
        "    #-----build a language model\n",
        "    # init language model\n",
        "    \"\"\"config = awd_lstm_lm_config.copy()\n",
        "    config['emb_sz'] = emb_size\"\"\"\n",
        "    lm = language_model_learner(dls_lm, AWD_LSTM, drop_mult=0.5, metrics=[accuracy, Perplexity()], wd=0.1).to_fp16()\n",
        "\n",
        "    # auto learning rate for lm\n",
        "    if lr is None:\n",
        "        lr_lm=list(lm.lr_find())[0]\n",
        "        print(f'auto identified learning rate lr_lm={lr_lm}')\n",
        "    else:\n",
        "        lr_lm = lr\n",
        "    # train lm learner\n",
        "    lm.fit_flat_cos(max_epochs, cbs=[EarlyStoppingCallback()], lr=slice(lr_lm/(2.6**4), lr_lm))\n",
        "\n",
        "    # furtherly fine tune lm learner\n",
        "    #lm.fit_one_cycle(5, slice(lr_lm/10,lr_lm * 10))\n",
        "\n",
        "    # Create model_path if it does not exist \n",
        "    import os\n",
        "    if not os.path.exists(model_path):\n",
        "      os.makedirs(model_path)\n",
        "\n",
        "    # save lm and its encoder; The model not including the final layer is called the encoder\n",
        "    lm.save(model_path+txt_col+'_lm')\n",
        "    lm.save_encoder(model_path+txt_col+'_lm_encoder')\n",
        "\n",
        "    #======build a text classifier\n",
        "    # get `dataloader` object for classification model;\n",
        "    dls_clf = TextDataLoaders.from_df(df[[txt_col, label_col]], text_col=txt_col, label_col=label_col, valid_pct=0.2, seed=123, text_vocab=dls_lm.vocab)\n",
        "    clf = text_classifier_learner(dls_clf, AWD_LSTM, drop_mult=0.5, metrics=metrics).to_fp16()\n",
        "    clf.load_encoder(model_path+txt_col+'_lm_encoder')\n",
        "\n",
        "    # auto learning rate\n",
        "    if lr is None:\n",
        "        lr_clf=list(clf.lr_find())[0]\n",
        "        print(f'auto identified learning rate lr_clf={lr_clf}')\n",
        "    else:\n",
        "        lr_clf = lr\n",
        "    # train learner\n",
        "    clf.fit_flat_cos(max_epochs, cbs=[EarlyStoppingCallback()], lr=slice(lr_clf/(2.6**4), lr_clf))\n",
        "\n",
        "    # furtherly fine tune clf learner\n",
        "    #clf.fit_one_cycle(5, slice(lr_clf/(2.6**4), lr_clf))\n",
        "\n",
        "    #unfreeze all except the last 2 layers & retrain\n",
        "    clf.freeze_to(-2)\n",
        "    clf.fit_one_cycle(1, slice(lr_clf/(2.6**4), lr_clf))\n",
        "\n",
        "    #unfreeze all except the last 3 layers & retrain\n",
        "    clf.freeze_to(-3)\n",
        "    clf.fit_one_cycle(1, slice(lr_clf/(2.6**4), lr_clf))\n",
        "\n",
        "    # last, unfreeze the whole model & retrain\n",
        "    clf.unfreeze()\n",
        "    #clf.fit_one_cycle(5, slice(1e-3/(2.6**4),1e-3))\n",
        "    clf.fit_flat_cos(100, cbs=[EarlyStoppingCallback()], lr=slice(lr_clf/(2.6**4), lr_clf))\n",
        "\n",
        "    # save the state of the model, it create a file in `learn.path/models/` named 'baseline_model.pth'\n",
        "    clf.save(model_path+txt_col+'_classifier')\n",
        "    return lm, clf\n",
        "\n",
        "def load_fastai_text_classifier(df:pd.DataFrame, txt_col:str, label_col:str, model_path:str, lr:float=0.005):\n",
        "    \"\"\"train a fastai text classifier and get its performance metrics\n",
        "    Args:\n",
        "      df:pd.DataFrame the dataframe containing text_col and label_col\n",
        "      txt_col:str e.g. 'hard_skills_name' \n",
        "      label_col:str e.g. 'label'\n",
        "      model_path:str e.g. '/content/drive/My Drive/techskills/model/'\n",
        "      lr:float=0.005\n",
        "      #emb_size:int=128\n",
        "      #flag_auto_lr:bool=False whether or not use auto search learning rate; if False, use default value 0.005 \n",
        "    Returns:\n",
        "      lm: the trained language model\n",
        "      clf: the trained fastai text classification model   \n",
        "    \"\"\"\n",
        "\n",
        "    #metrics\n",
        "    f1=FBeta(beta=1, average='weighted')\n",
        "    precision = Precision(average='weighted')\n",
        "    recall = Recall(average='weighted')\n",
        "    metrics=[accuracy, precision, recall, f1]\n",
        "        \n",
        "    # get `dataloader` object for language model\n",
        "    dls_lm = TextDataLoaders.from_df(df[[txt_col, label_col]], is_lm=True, text_col=txt_col, label_col=label_col, valid_pct=0.2, seed=123)\n",
        "\n",
        "    #-----build a language model\n",
        "    # init language model\n",
        "    \n",
        "    lm = language_model_learner(dls_lm, AWD_LSTM, drop_mult=0.5, metrics=[accuracy, Perplexity()], wd=0.1).to_fp16()\n",
        "\n",
        "    # save lm and its encoder; The model not including the final layer is called the encoder\n",
        "    lm.load(model_path+txt_col+'_lm')\n",
        "    lm.load_encoder(model_path+txt_col+'_lm_encoder')\n",
        "\n",
        "    #======build a text classifier\n",
        "    # get `dataloader` object for classification model;\n",
        "    dls_clf = TextDataLoaders.from_df(df[[txt_col, label_col]], text_col=txt_col, label_col=label_col, valid_pct=0.2, seed=123, text_vocab=dls_lm.vocab)\n",
        "    clf = text_classifier_learner(dls_clf, AWD_LSTM, drop_mult=0.5, metrics=metrics).to_fp16()\n",
        "    clf.load_encoder(model_path+txt_col+'_lm_encoder')\n",
        "\n",
        "\n",
        "    # save the state of the model, it create a file in `learn.path/models/` named 'baseline_model.pth'\n",
        "    try: \n",
        "        clf.load(model_path+txt_col+'_classifier')\n",
        "    except: # in case can not load the trained classifier, retrain it\n",
        "        # auto learning rate\n",
        "        if lr is None:\n",
        "            lr_clf=list(clf.lr_find())[0]\n",
        "            print(f'auto identified learning rate lr_clf={lr_clf}')\n",
        "        else:\n",
        "            lr_clf = 0.005\n",
        "        # train learner\n",
        "        clf.fit_flat_cos(100, cbs=[EarlyStoppingCallback()], lr=lr_clf)\n",
        "        \n",
        "        # furtherly fine tune clf learner\n",
        "        #clf.fit_one_cycle(5, slice(lr_clf/10, lr_clf*10))\n",
        "\n",
        "        #unfreeze all except the last 2 layers & retrain\n",
        "        clf.freeze_to(-2)\n",
        "        clf.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n",
        "\n",
        "        #unfreeze all except the last 3 layers & retrain\n",
        "        clf.freeze_to(-3)\n",
        "        clf.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n",
        "\n",
        "        # last, unfreeze the whole model & retrain\n",
        "        clf.unfreeze()\n",
        "        clf.fit_one_cycle(5, slice(1e-3/(2.6**4),1e-3))\n",
        "        # save the state of the model, it create a file in `learn.path/models/` named 'title_raw_classifier.pth'\n",
        "        clf.save(model_path+txt_col+'_classifier')\n",
        "    return lm, clf\n",
        "\n",
        "def fastai_learner_preds(learner, df, label_col, txt_col:str=None):\n",
        "  \"\"\"use a trained fastai learner to make prediction on df[txt_col] \n",
        "  Args:\n",
        "    learner: a trained fastai text learner (model)\n",
        "    test_df:pd.DataFrame e.g. df[[img_col]]\n",
        "    txt_col:str=None e.g. 'Skills_resume', 'title_raw'\n",
        "    ATTENSION: test_df need to have the same features which learner was trained on \n",
        "  Returns:\n",
        "    preds:np.array of shape (num_samples,)\n",
        "    probs:np.array of shape (num_samples, num_classes)\n",
        "  Example:\n",
        "    #make sure the txt_col is renamed as 'text'\n",
        "    df = df[[txt_col]].copy().rename({txt_col:'text'}, axis=1)\n",
        "\n",
        "    #make sure NaN value (of which the dtype is of np numeric) is replaced by '' (dtype is str)\n",
        "    df['text'].fillna('', inplace=True)\n",
        "\n",
        "    preds, probs = fastai_learner_preds(learner=clf0, df)\n",
        "  \n",
        "  \"\"\"\n",
        "  # in case of deal with txt_col, do some preprocessing on test_df[[txt_col]]\n",
        "  if txt_col is not None: \n",
        "    #make sure the txt_col is renamed as 'text'\n",
        "    df = df[[txt_col]].copy().rename({txt_col:'text'}, axis=1)\n",
        "\n",
        "    #make sure NaN value (of which the dtype is of np numeric) is replaced by '' (dtype is str)\n",
        "    df['text'].fillna('', inplace=True)\n",
        "  test_df = df.copy()\n",
        "  if label_col in test_df.columns:\n",
        "      test_df.drop([label_col], axis=1, inplace=True)\n",
        "  dl = learner.dls.test_dl(test_df) #, with_labels=True\n",
        "  probs, _ = learner.get_preds(dl=dl)\n",
        "  preds = probs.numpy().argmax(axis=1)\n",
        "  return preds, probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "A6U6D3zdB8n_",
        "outputId": "95c73a7d-63de-4730-c9c4-b28dc84cf12a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# train txt classifier(s), each is corresponding to every txt_cols\\nlms=[]\\ntxt_clfs=[]\\n\\nfor txt_col in txt_cols:\\n  lm0, txt_clf0 = train_fastai_text_classifier(df, \\n                                            txt_col=txt_col,\\n                                            label_col=label_col,\\n                                            model_path='/content/drive/My Drive/fastai_multimodal/model/',\\n                                            lr=0.005\\n                                            )\\n  lms.append(lm0)\\n  txt_clfs.append(txt_clf0)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "\"\"\"# train txt classifier(s), each is corresponding to every txt_cols\n",
        "lms=[]\n",
        "txt_clfs=[]\n",
        "\n",
        "for txt_col in txt_cols:\n",
        "  lm0, txt_clf0 = train_fastai_text_classifier(df, \n",
        "                                            txt_col=txt_col,\n",
        "                                            label_col=label_col,\n",
        "                                            model_path='/content/drive/My Drive/fastai_multimodal/model/',\n",
        "                                            lr=0.005\n",
        "                                            )\n",
        "  lms.append(lm0)\n",
        "  txt_clfs.append(txt_clf0)\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ko0k1cHYxM7u",
        "outputId": "264a7a90-da27-4642-e2e8-1f44cfeb0585"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# re-load each text classifier trained w. each of txt_cols\\nlms=[]\\ntxt_clfs=[]\\n\\nfor txt_col in txt_cols:\\n  lm0, clf0 = load_fastai_text_classifier(df, \\n                                          txt_col=txt_cols[0],\\n                                          label_col=label_col,\\n                                          model_path='/content/drive/My Drive/fastai_multimodal/model/',\\n                                          lr=0.005\\n                                          )\\n  lms.append(lm0)\\n  txt_clfs.append(txt_clf0)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "\"\"\"# re-load each text classifier trained w. each of txt_cols\n",
        "lms=[]\n",
        "txt_clfs=[]\n",
        "\n",
        "for txt_col in txt_cols:\n",
        "  lm0, clf0 = load_fastai_text_classifier(df, \n",
        "                                          txt_col=txt_cols[0],\n",
        "                                          label_col=label_col,\n",
        "                                          model_path='/content/drive/My Drive/fastai_multimodal/model/',\n",
        "                                          lr=0.005\n",
        "                                          )\n",
        "  lms.append(lm0)\n",
        "  txt_clfs.append(txt_clf0)\"\"\"\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7MSpbgZ1BgY"
      },
      "source": [
        "## module: get_fastai_docs_embs\n",
        "\n",
        "Instead of using out of box embedding methods (tfidf, USE, SBERT), I want to use classifier based embedding method to calculate document embedding.\n",
        "\n",
        "References:\n",
        "\n",
        "- [Getting Document Encodings From ULMFiT (updated for Fastai v2)](https://alanjjian.medium.com/getting-document-encodings-from-ulmfit-updated-for-fastai-v3-7444904011fe)\n",
        "\n",
        "- [Tutorial on SPAM detection using fastai ULMFiT - Part 1: Language Model](https://drive.google.com/drive/u/0/folders/13uo91qC4cUFPepeRCg5XXoBCFqg3Q2Mn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Ny7fszU41tt7"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "def get_fastai_docs_embs(docs:list, learn, lm, df=None, txt_col=None):\n",
        "    \"\"\"use classifier to get document embedding vector (np.array)\n",
        "    Args:\n",
        "      docs:list of str e.g. ['Python (programming language)', 'Data Science', 'git, GitHub, NLP']\n",
        "      learn: e.g. fastai.text.learner.TextLearner\n",
        "      lm: e.g. fastai.text.learner.LMLearner language model to generate numericalizer and tokenizer\n",
        "      df[txt_col] to generate language model's dataloader (dls_lm), numericalizer and tokenizer\n",
        "    Returns:\n",
        "      embs: a num_samples long list of 400D vector\n",
        "\n",
        "    Examples:\n",
        "      get_docs_embs(docs=['Python (programming language)', 'Data Science', 'git, GitHub, NLP'], learn=clf1, lm=lm1, df=None, txt_col=None)\n",
        "      get_docs_embs(docs=['Python (programming language)', 'Data Science', 'git, GitHub, NLP'], learn=clf1, lm=None, df=df, txt_col=txt_cols[1])\n",
        "    \"\"\"\n",
        "    # Utilize DataBlock API to process and load data\n",
        "    if (df is not None) and (txt_col is not None):\n",
        "      dls_lm = DataBlock(blocks=TextBlock.from_df(text_cols=txt_col, is_lm=True),\n",
        "                        get_x=ColReader('text'),\n",
        "                        splitter=RandomSplitter(0.1)).dataloaders(df, bs=128,seq_len=80) \n",
        "      numericalizer = Numericalize(vocab=dls_lm.vocab)\n",
        "      tokenizer = dls_lm.tokenizer\n",
        "    elif lm is not None:\n",
        "      numericalizer = Numericalize(vocab=lm.dls.vocab)\n",
        "      tokenizer = lm.dls.tokenizer\n",
        "    \n",
        "    #preprocess docs\n",
        "    embs = []\n",
        "    for doc in docs:\n",
        "      xb = numericalizer(tokenizer(doc))\n",
        "      xb = xb.reshape((1, xb.size()[0]))\n",
        "      awd_lstm = learn.model[0]\n",
        "      awd_lstm.reset()\n",
        "      with torch.no_grad():\n",
        "          try:\n",
        "            out = awd_lstm.eval()(xb.cuda()) #to deal with 'all-tensors-to-be-on-the-same-device error', add `.cuda(). Ref: https://stackoverflow.com/questions/64929665/pytorch-running-runtimeerror-expected-all-tensors-to-be-on-the-same-device-bu\n",
        "          except:\n",
        "            out = awd_lstm.eval()(xb)  \n",
        "      embs.append(out[0].cpu().max(0).values.detach().numpy().sum(axis=0).reshape(-1,)) #out[0].cpu() is to copy tensor from GPU(cuda) to CPU\n",
        "    return embs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pm4xFT6szgT0",
        "outputId": "ecb5b085-2577-4dc9-e061-06300b55b7b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# load lm and clf based on txt_cols[0]\\nlm, clf = load_fastai_text_classifier(df, \\n                                       txt_col=txt_cols[0],\\n                                       label_col=label_col,\\n                                       model_path='/content/drive/My Drive/fastai_multimodal/model/',\\n                                       lr=0.005\\n                                       )\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "\"\"\"# load lm and clf based on txt_cols[0]\n",
        "lm, clf = load_fastai_text_classifier(df, \n",
        "                                       txt_col=txt_cols[0],\n",
        "                                       label_col=label_col,\n",
        "                                       model_path='/content/drive/My Drive/fastai_multimodal/model/',\n",
        "                                       lr=0.005\n",
        "                                       )\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UvMFM7ON6rLt",
        "outputId": "30aaec43-ee25-4692-e68a-4a436c4d94b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"#dbck: expect the 2 set of vectors producted by the 2 methods are highly correlated \\n# get docs embeddings e1 by method1\\ne1 = get_fastai_docs_embs(docs=df[txt_cols[0]] #['Python (programming language)', 'Data Science', 'git, GitHub, NLP']\\n, learn=clf, lm=lm, df=None, txt_col=None)\\n# get docs embeddings e2 by method2\\ne2 = get_fastai_docs_embs(docs=df[txt_cols[0]] #['Python (programming language)', 'Data Science', 'git, GitHub, NLP']\\n, learn=clf, lm=None, df=df, txt_col=txt_cols[0])\\n\\n#dbck: compute cosine similarity bw e1 and e2, expect the similarity is closed to 1\\nfrom sklearn.metrics.pairwise import cosine_similarity as cs\\n[cs(x1.reshape(1,-1),x2.reshape(1,-1)) for (x1,x2) in zip(e1, e2)]\\npd.DataFrame(e1).shape\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "\"\"\"#dbck: expect the 2 set of vectors producted by the 2 methods are highly correlated \n",
        "# get docs embeddings e1 by method1\n",
        "e1 = get_fastai_docs_embs(docs=df[txt_cols[0]] #['Python (programming language)', 'Data Science', 'git, GitHub, NLP']\n",
        ", learn=clf, lm=lm, df=None, txt_col=None)\n",
        "# get docs embeddings e2 by method2\n",
        "e2 = get_fastai_docs_embs(docs=df[txt_cols[0]] #['Python (programming language)', 'Data Science', 'git, GitHub, NLP']\n",
        ", learn=clf, lm=None, df=df, txt_col=txt_cols[0])\n",
        "\n",
        "#dbck: compute cosine similarity bw e1 and e2, expect the similarity is closed to 1\n",
        "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
        "[cs(x1.reshape(1,-1),x2.reshape(1,-1)) for (x1,x2) in zip(e1, e2)]\n",
        "pd.DataFrame(e1).shape\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7a8CexsXnsz5"
      },
      "outputs": [],
      "source": [
        "#pd.DataFrame(e1, index=df.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FQsAcUpXnIzc"
      },
      "outputs": [],
      "source": [
        "#pd.DataFrame(e2, index=df.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtb7Pz_8fjxl"
      },
      "source": [
        "# 2) fastai image classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "r27UQu3qgBJJ"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "#from fastai.vision.all import *\n",
        "\n",
        "def train_fastai_image_classifier(df:pd.DataFrame, label_col:str, img_col:str, img_path:str, model_path:str, model_name:str, lr:float=0.005, max_epochs:int=100, img_size:int=224, bs:int=64, emb_size:int=128):\n",
        "    \"\"\"train and evaluate a fastai image classifier, where image data is stored under `path`, where df stores the path of each image file\n",
        "    Args:\n",
        "      df:pd.DataFrame, \n",
        "      img_col:str,\n",
        "      img_path:str, the folder where the images are stored e.g.  '/root/.fastai/data/mnist_tiny'\n",
        "      model_path:str, the folder where the image classifier is stored\n",
        "      model_name:str, \n",
        "      lr:float=0.005, \n",
        "      max_epochs:int=100\n",
        "      img_size:int=224,\n",
        "      emb_size:int=128 embedding size \n",
        "      bs:int=64\n",
        "    Returns:\n",
        "      img_learn: a trained image classifier\n",
        "\n",
        "\n",
        "    Example:\n",
        "      path = untar_data(URLs.MNIST_TINY)\n",
        "      df = pd.read_csv(path/'labels.csv')\n",
        "      print(df)\n",
        "      img_col='name'\n",
        "      label_col='label'\n",
        "      img_path = '/root/.fastai/data/mnist_tiny'\n",
        "      img_clf = train_fastai_image_classifier(df=df, img_col=img_col, img_path=img_path, model_path='.', model_name='img_clf', lr=None, max_epochs=100)\n",
        "    \"\"\"\n",
        "    # make sure df[[img_col]] is without img_path in it\n",
        "    import re\n",
        "    def f(row, img_path=img_path):\n",
        "      #make sure img_path ended with '/' e.g. img_path='/root/.keras/datasets/tweet_images/'\n",
        "      if img_path[-1]!='/':\n",
        "          img_path+='/'\n",
        "      return re.sub(img_path, '', row)\n",
        "    \n",
        "    df[img_col] = df[img_col].apply(lambda row: f(row))\n",
        "    print(f'==========dbck df[[img_col]] is without img_path={img_path} in it===========\\n{df[[img_col]].head()}')\n",
        "\n",
        "    # load images fr df into dls\n",
        "    from pathlib import Path \n",
        "    def get_dls(emb_size, bs):\n",
        "        \"\"\" unify the size of input images on a batch, in order to deal with the following Error:\n",
        "        RuntimeError: stack expects each tensor to be equal size, but got [3, 298, 273] at entry 0 and [3, 480, 480] at entry 1\n",
        "        Ref: https://forums.fast.ai/t/what-to-do-if-raw-image-is-very-large-cpu-bottleneck/88432/2\n",
        "        \"\"\"\n",
        "        dls = ImageDataLoaders.from_df(df,\n",
        "                                      path=Path(img_path),\n",
        "                                      fn_col=img_col, #'path'\n",
        "                                      #valid_col='is_val',\n",
        "                                      label_col=label_col,#'target',\n",
        "                                      y_block=CategoryBlock,\n",
        "                                      item_tfms=Resize(emb_size, method=ResizeMethod.Squish),\n",
        "                                      batch_tfms=aug_transforms(size=img_size),\n",
        "                                      bs=bs)\n",
        "\n",
        "        return dls\n",
        "\n",
        "    dls = get_dls(emb_size=emb_size, bs=bs) # replace: dls = ImageDataLoaders.from_df(df=df,fn_col=img_col, label_col=label_col, path=Path(img_path))\n",
        " \n",
        "    f1=FBeta(beta=1, average='weighted')\n",
        "    precision = Precision(average='weighted')\n",
        "    recall = Recall(average='weighted')\n",
        "    metrics=[error_rate, accuracy, precision, recall, f1]\n",
        "    img_learn = cnn_learner(dls, resnet34, metrics=metrics)\n",
        "\n",
        "    # find optimal learner rate lr\n",
        "    if lr is None:\n",
        "      lr=list(img_learn.lr_find())[0]\n",
        "\n",
        "    # fit learner\n",
        "    img_learn.fit_flat_cos(100, cbs=[EarlyStoppingCallback()], lr=lr)\n",
        "\n",
        "    #unfreeze all except the last 2 layers & retrain\n",
        "    img_learn.freeze_to(-2)\n",
        "    img_learn.fit_one_cycle(1, slice(lr/(2.6**4),lr))\n",
        "\n",
        "    #unfreeze all except the last 3 layers & retrain\n",
        "    img_learn.freeze_to(-3)\n",
        "    img_learn.fit_one_cycle(1, slice(lr/(2.6**4),lr))\n",
        "\n",
        "    # last, unfreeze the whole model & retrain until it making no progress in val_lose\n",
        "    img_learn.unfreeze()\n",
        "    #replace: img_learn.fit_one_cycle(epochs, slice(lr/(2.6**4),lr))\n",
        "    img_learn.fit_flat_cos(max_epochs, cbs=[EarlyStoppingCallback()], lr=slice(lr/(2.6**4),lr))\n",
        "\n",
        "    # save the state of the model, it create a file in `learn.path/models/` named 'baseline_model.pth'\n",
        "    img_learn.save(model_path+model_name)\n",
        "    return img_learn\n",
        "\n",
        "#img_clf = train_eval_fastai_image_classifier(df=df, img_col=img_col, img_path=img_path, model_path='.', model_name='img_clf', lr=None, max_epochs=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5Z3ySRp1gBL3",
        "outputId": "78d6b33b-f9e6-405f-8116-a4d72eb8c650"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"#example1\\npath = untar_data(URLs.MNIST_TINY)\\ndf_ = pd.read_csv(path/'labels.csv')\\nprint(df_)\\nimg_col_='name'\\nlabel_col_='label'\\nimg_path_ = '/root/.fastai/data/mnist_tiny'\\nimg_clf, dls = train_fastai_image_classifier(df=df_,label_col=label_col_, img_col=img_col_, img_path=img_path_, model_path='.', model_name='img_clf', lr=None, max_epochs=100)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "\"\"\"#example1\n",
        "path = untar_data(URLs.MNIST_TINY)\n",
        "df_ = pd.read_csv(path/'labels.csv')\n",
        "print(df_)\n",
        "img_col_='name'\n",
        "label_col_='label'\n",
        "img_path_ = '/root/.fastai/data/mnist_tiny'\n",
        "img_clf, dls = train_fastai_image_classifier(df=df_,label_col=label_col_, img_col=img_col_, img_path=img_path_, model_path='.', model_name='img_clf', lr=None, max_epochs=100)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6BU_c5k9J7BP",
        "outputId": "6fbb87f2-b10d-4640-d1b9-7f5c894f94d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"# another example\\nif len(img_cols)>0:\\n  img_clf = train_fastai_image_classifier(df=df, \\n                                label_col=label_col, \\n                                img_col=img_cols[0], \\n                                img_path='/root/.keras/datasets/tweet_images', \\n                                model_path='/content/drive/My Drive/fastai_multimodal/model/',\\n                                model_name=img_cols[0]+'_clf', \\n                                lr=0.005, \\n                                max_epochs=100)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "\"\"\"# another example\n",
        "if len(img_cols)>0:\n",
        "  img_clf = train_fastai_image_classifier(df=df, \n",
        "                                label_col=label_col, \n",
        "                                img_col=img_cols[0], \n",
        "                                img_path='/root/.keras/datasets/tweet_images', \n",
        "                                model_path='/content/drive/My Drive/fastai_multimodal/model/',\n",
        "                                model_name=img_cols[0]+'_clf', \n",
        "                                lr=0.005, \n",
        "                                max_epochs=100)\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhehr57F8rVY"
      },
      "source": [
        "## module: get_fastai_imgs_embs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Ve3_XxOR9KHr"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "def get_fastai_imgs_embs(img_clf, df:pd.DataFrame=None, img_col:str=None):\n",
        "    \"\"\"use classifier to get image embedding vector (np.array)\n",
        "    Args:\n",
        "      img_clf: e.g. fastai.learner.Learner\n",
        "      df[[img_col]] store the path of image files\n",
        "    Returns:\n",
        "      embs: a np.array of shape (num_samples, 512)\n",
        "\n",
        "    Examples: \n",
        "      img_embs = get_fastai_imgs_embs(img_clf=img_clf, df=df.head(5), img_col='name')\n",
        "   \n",
        "    \"\"\"\n",
        "    # define pytorch hook\n",
        "    class SaveFeatures():\n",
        "        features=None\n",
        "        def __init__(self, m): \n",
        "            self.hook = m.register_forward_hook(self.hook_fn)\n",
        "            self.features = None\n",
        "        def hook_fn(self, module, input, output): \n",
        "            out = output.detach().cpu().numpy()\n",
        "            if isinstance(self.features, type(None)):\n",
        "                self.features = out\n",
        "            else:\n",
        "                self.features = np.row_stack((self.features, out))\n",
        "        def remove(self): \n",
        "            self.hook.remove()\n",
        "    # identify the layer from which you want to get embeddings\n",
        "    #print(img_clf.model) \n",
        "    print(f'====== We will get embedding from {img_clf.model[1][4]} =======')\n",
        "    emb_layer = img_clf.model[1][4]\n",
        "    #put hook on the selected emb_layer\n",
        "    sf = SaveFeatures(emb_layer)\n",
        "\n",
        "    # access dls from the trained classifier\n",
        "    test_df = df[[img_col]]\n",
        "    test_dl = img_clf.dls.test_dl(test_df, with_labels=False)\n",
        "    \n",
        "    # run img_clf through test data\n",
        "    preds, _ = img_clf.get_preds(dl=test_dl)\n",
        "    # get the embeddings of test data\n",
        "    embs = sf.features\n",
        "     \n",
        "    return embs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "53dLNe4E9KNQ",
        "outputId": "75d8b7a6-09e7-4793-978b-7ca9a9543742"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#dbck\\nimg_embs = get_fastai_imgs_embs(img_clf=img_clf, df=df.head(), img_col=img_cols[0])\\nimg_embs.shape'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "\"\"\"#dbck\n",
        "img_embs = get_fastai_imgs_embs(img_clf=img_clf, df=df.head(), img_col=img_cols[0])\n",
        "img_embs.shape\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNdIjWi4qYLI"
      },
      "source": [
        "# 3) fastai tabular classifer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "g9utfXgVgIs-"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "\n",
        "#from fastai.tabular.all import *\n",
        "\n",
        "def split_idxs(df, train_size=.9, flag_random_split=True):\n",
        "    \"\"\" split df index into 2 parts: train_idxs and test_idxs \n",
        "    Args:\n",
        "        df: the dataframe of all your data\n",
        "        train_size (float in [0,1], default 0.9)\n",
        "        flag_random_split(bool, default False): do you want random split idxs?\n",
        "    Returns:\n",
        "        (ls_train, ls_test): a 2-tuple of lists for train indices and test indices\n",
        "\n",
        "    Example:\n",
        "        df = pd.DataFrame({'c1':list(range(26)), 'c2':list(string.ascii_lowercase)})\n",
        "        splits = split_idxs(df)\n",
        "        ...\n",
        "        # use splits to build TabularPandas taublar object\n",
        "        to = TabularPandas(df, \n",
        "                   procs=procs,\n",
        "                   cat_names=cat_names,\n",
        "                   cont_names=cont_names,\n",
        "                   y_names=y_names,\n",
        "                   y_block=y_block,\n",
        "                   splits=splits)\n",
        "    \"\"\"\n",
        "    import random\n",
        "    ls = range_of(df)\n",
        "    print(ls)\n",
        "    if flag_random_split:\n",
        "        splits = RandomSplitter()(ls)\n",
        "    else:\n",
        "        ls_train = ls[:int(df.shape[0]*train_size)]\n",
        "        ls_test = ls[int(df.shape[0]*train_size):]\n",
        "        random.shuffle(ls_train)\n",
        "        random.shuffle(ls_test)\n",
        "        splits = (ls_train, ls_test)\n",
        "    return splits\n",
        "\n",
        "def train_fastai_tabular_classifier(df:pd.DataFrame, label_col:str, cnt_cols:list=None, cat_cols:list=None, lr:float=0.005, max_epochs:int=100, model_path:str='/content/drive/My Drive/techskills/model/', model_name:str='tabular_model'):\n",
        "    \"\"\"train an ensembled classifier input with embs_ls, which is a list of embeddings\n",
        "    Args:\n",
        "      df:pd.DataFrame,\n",
        "      label_col:str,\n",
        "      cnt_cols:list of str\n",
        "      cat_cols:list of str\n",
        "      lr:float=0.005 learning rate\n",
        "      max_epochs:int=10 number of epochs to train a tabular learner when unfreeze the whole model\n",
        "      model_path:str='/content/drive/My Drive/techskills/model/', \n",
        "      model_name:str='tabular_model', \n",
        "    Returns:\n",
        "      tab_learner: a trained fastai tabular classifier\n",
        "    \"\"\"\n",
        "    if cnt_cols is None or cat_cols is None:\n",
        "        txt_cols, cnt_cols, cat_cols, (int_cat_cols, str_cat_cols), img_cols = cnt_cat_txt_img_split(df, cnt_card=.5, excluded_cols = [label_col], txt_card=0.5)\n",
        "        print(f'** Given label_col={label_col} ** \\n======= automatically identify\\n cnt_cols={cnt_cols}\\n cat_cols={cat_cols},\\n img_cols={img_cols}, \\n txt_cols={txt_cols} \\n======= make sure that is what you expect; otherwise, manually make changes')\n",
        "\n",
        "    # define variables\n",
        "    y_names = label_col\n",
        "    cat_names = cat_cols\n",
        "    cont_names = cnt_cols\n",
        "    tab_cols = [label_col]+cnt_cols+cat_cols\n",
        "\n",
        "    # build fastai tabular dataloader \n",
        "    procs = [Categorify, FillMissing, Normalize]\n",
        "    splits = split_idxs(df[tab_cols], train_size=.9, flag_random_split=True)\n",
        "    to = TabularPandas(df[tab_cols], \n",
        "                      procs, \n",
        "                      cat_names, \n",
        "                      cont_names,\n",
        "                      y_names=y_names, \n",
        "                      y_block=CategoryBlock(),\n",
        "                      splits=splits)\n",
        "\n",
        "    tab_dls = to.dataloaders(bs=8) \n",
        "\n",
        "    #metrics\n",
        "    f1=FBeta(beta=1, average='weighted')\n",
        "    precision = Precision(average='weighted')\n",
        "    recall = Recall(average='weighted')\n",
        "    metrics=[accuracy, precision, recall, f1]\n",
        "\n",
        "    # tabular learner\n",
        "    tab_learn = tabular_learner(dls=tab_dls, layers=[200,100], metrics=metrics)\n",
        "\n",
        "    # find optimal learner rate lr\n",
        "    if lr is None:\n",
        "      lr=list(tab_learn.lr_find())[0]\n",
        "\n",
        "    # fit learner\n",
        "    tab_learn.fit_flat_cos(max_epochs, cbs=[EarlyStoppingCallback()], lr=lr)\n",
        "\n",
        "    #unfreeze all except the last 2 layers & retrain\n",
        "    tab_learn.freeze_to(-2)\n",
        "    tab_learn.fit_one_cycle(1, slice(lr/(2.6**4),lr))\n",
        "\n",
        "    #unfreeze all except the last 3 layers & retrain\n",
        "    tab_learn.freeze_to(-3)\n",
        "    tab_learn.fit_one_cycle(1, slice(lr/(2.6**4),lr))\n",
        "\n",
        "    # last, unfreeze the whole model & retrain\n",
        "    tab_learn.unfreeze()\n",
        "    #tab_learn.fit_one_cycle(epochs, slice(lr/(2.6**4),lr))\n",
        "    tab_learn.fit_flat_cos(max_epochs, cbs=[EarlyStoppingCallback()], lr=slice(lr/(2.6**4),lr))\n",
        "\n",
        "    # save the state of the model, it create a file in `learn.path/models/` named 'baseline_model.pth'\n",
        "    tab_learn.save(model_path+model_name)\n",
        "    return tab_learn\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moWCSPLmLvhL"
      },
      "source": [
        "### module: get_fastai_tab_embs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "HA8EnMveL2M7"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "def get_fastai_tab_embs(tab_clf, df:pd.DataFrame, cnt_cols:list=None, cat_cols:list=None):\n",
        "    \"\"\"use classifier to get image embedding vector (np.array)\n",
        "    Args:\n",
        "      tab_clf: e.g. fastai.tabular.learner.TabularLearner\n",
        "      df[cnt_cols+cat_cols]\n",
        "    Returns:\n",
        "      embs: a np.array of shape (num_samples, 512)\n",
        "\n",
        "    Examples: \n",
        "      img_embs = get_fastai_imgs_embs(img_clf=img_clf, df=df.head(5), img_col='name')\n",
        "   \n",
        "    \"\"\"\n",
        "    # define pytorch hook\n",
        "    class SaveFeatures():\n",
        "        features=None\n",
        "        def __init__(self, m): \n",
        "            self.hook = m.register_forward_hook(self.hook_fn)\n",
        "            self.features = None\n",
        "        def hook_fn(self, module, input, output): \n",
        "            out = output.detach().cpu().numpy()\n",
        "            if isinstance(self.features, type(None)):\n",
        "                self.features = out\n",
        "            else:\n",
        "                self.features = np.row_stack((self.features, out))\n",
        "        def remove(self): \n",
        "            self.hook.remove()\n",
        "    # identify the layer from which you want to get embeddings\n",
        "    #print(img_clf.model) \n",
        "    print(f'====== We will get embedding from {tab_clf.model.layers[1][0]} =======')\n",
        "    emb_layer = tab_clf.model.layers[1][0]\n",
        "    #put hook on the selected emb_layer\n",
        "    sf = SaveFeatures(emb_layer)\n",
        "\n",
        "    # access dls from the trained classifier\n",
        "    #?? no needed?? test_df = df[cnt_cols+cat_cols]\n",
        "    test_dl = tab_clf.dls.test_dl(df, with_labels=False)\n",
        "    \n",
        "    # run img_clf through test data\n",
        "    preds, _ = tab_clf.get_preds(dl=test_dl)\n",
        "    # get the embeddings of test data\n",
        "    embs = sf.features\n",
        "\n",
        "    return embs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KdSGDOO7gI08",
        "outputId": "c226dc51-40d6-4917-e0e5-2da5855468be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#dbck\\ntab_embs = get_fastai_tab_embs(tab_clf=tab_learn, df=df_.head(5))\\n\\ntab_embs.shape'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "\"\"\"#dbck\n",
        "tab_embs = get_fastai_tab_embs(tab_clf=tab_learn, df=df_.head(5))\n",
        "\n",
        "tab_embs.shape\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nCCqfiH217a"
      },
      "source": [
        "# 4) ensembled models\n",
        "\n",
        "Big idea: you can blend multiple classifiers at different stages:\n",
        "\n",
        "- `early concat`: concatinate cnt, cat, txt, img after data loading and data preprocessing, followed by a learner of choice (e.g. fastai tabular).\n",
        "- `middle concat`: concatinate the embeddings from each of the trained tab (cnt+cat), txt, img models, followed by a learner of choice.\n",
        "- `late concat`: concatinate the probability predictions from each of the trained tab(cnt+cat), txt, img models, followed by a learner of choice.\n",
        "\n",
        "\n",
        "Here I will do experiment on the `middle concat`: blend embedding(txt_col1) and embedding(img_col2) by a fastai tabular learner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPxdDr_h0JCT"
      },
      "source": [
        "## module: train_ensembled_classifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cz6GMIXx7auB"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "def train_ensembled_classifier(embs_ls, lr:float=0.005, max_epochs:int=10, model_path:str='/content/drive/My Drive/fastai_multimodal/model/', model_name:str='tabular_ensemble_enbeddings', n_components:float=1, df=df, label_col=label_col, emb_size=128):\n",
        "    \"\"\"train an ensembled classifier, using fastai tabular \n",
        "    Args:\n",
        "      embs_ls:list of embeddings, each embeddings is a list of 400D-vectors\n",
        "      lr:float=0.005 learning rate\n",
        "      max_epochs:int=10 number of epochs to train a tabular learner when unfreeze the whole model\n",
        "      model_path:str='/content/drive/My Drive/techskills/model/', \n",
        "      model_name:str='tabular_ensemble_enbeddings', \n",
        "      n_components:Union[int, float]=1 if use n_components!=1, use PCA to reduce embs_ls' dimension for the sake of fast computation at the price of accuracy; otherwise, do not use PCA\n",
        "      emb_size:int=128 embedding size\n",
        "    Returns:\n",
        "      tab_learner: a trained fastai tabular classifier\n",
        "    \"\"\"\n",
        "    \n",
        "    # create a new df containing label_col plus all txt_cols' embeddings\n",
        "    # reduce the dimension of df_embs by PCA\n",
        "    if n_components != 1:\n",
        "        from sklearn.decomposition import PCA\n",
        "        pca = PCA(n_components=n_components)\n",
        "        df_embs = pd.concat([df[[label_col]]]+[pd.DataFrame(pca.fit_transform(embs), index=df.index) for embs in embs_ls], axis=1)\n",
        "        df_embs.columns = [label_col]+ list(range(df_embs.shape[1]-1)) #fix the repeated column names problem which comes from pd.concat dfs \n",
        "    else:\n",
        "        df_embs = pd.concat([df[[label_col]]]+[pd.DataFrame(embs, index=df.index) for embs in embs_ls], axis=1)\n",
        "        df_embs.columns = [label_col]+ list(range(df_embs.shape[1]-1))\n",
        "\n",
        "    # define variables\n",
        "    y_names = label_col\n",
        "    cat_names = []\n",
        "    cont_names = list(df_embs.columns)[1:]\n",
        "    tab_cols = [y_names]+cat_names+cont_names\n",
        "\n",
        "    # build fastai tabular dataloader \n",
        "    procs = [Categorify, FillMissing, Normalize]\n",
        "    splits = split_idxs(df_embs[tab_cols], train_size=.9, flag_random_split=True)\n",
        "    to = TabularPandas(df_embs[tab_cols], \n",
        "                      procs, \n",
        "                      cat_names, \n",
        "                      cont_names,\n",
        "                      y_names=y_names, \n",
        "                      y_block=CategoryBlock(),\n",
        "                      splits=splits)\n",
        "\n",
        "    tab_dls = to.dataloaders(bs=8) \n",
        "\n",
        "    #metrics\n",
        "    f1=FBeta(beta=1, average='weighted')\n",
        "    precision = Precision(average='weighted')\n",
        "    recall = Recall(average='weighted')\n",
        "    metrics=[accuracy, precision, recall, f1]\n",
        "\n",
        "    # tabular learner\n",
        "    ensembled_learn = tabular_learner(dls=tab_dls, layers=[2*emb_size,emb_size], metrics=metrics)\n",
        "\n",
        "    # find optimal learner rate lr\n",
        "    if lr is None:\n",
        "      lr=list(ensembled_learn.lr_find())[0]\n",
        "\n",
        "    # fit learner\n",
        "    ensembled_learn.fit_flat_cos(100, cbs=[EarlyStoppingCallback()], lr=lr)\n",
        "\n",
        "    #unfreeze all except the last 2 layers & retrain\n",
        "    ensembled_learn.freeze_to(-2)\n",
        "    ensembled_learn.fit_one_cycle(1, slice(lr/(2.6**4),lr))\n",
        "\n",
        "    #unfreeze all except the last 3 layers & retrain\n",
        "    ensembled_learn.freeze_to(-3)\n",
        "    ensembled_learn.fit_one_cycle(1, slice(lr/(2.6**4),lr))\n",
        "\n",
        "    # last, unfreeze the whole model & retrain\n",
        "    ensembled_learn.unfreeze()\n",
        "    #tab_learn.fit_one_cycle(epochs, slice(lr/(2.6**4),lr))\n",
        "    ensembled_learn.fit_flat_cos(max_epochs, cbs=[EarlyStoppingCallback()], lr=slice(lr/(2.6**4),lr))\n",
        "\n",
        "    # save the state of the model, it create a file in `learn.path/models/` named 'baseline_model.pth'\n",
        "    ensembled_learn.save(model_path+model_name)\n",
        "    return ensembled_learn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fTP_caGRJpz",
        "outputId": "b4ddc933-b5c4-47d6-e6ba-b172b607a20d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ensembled_learn = train_ensembled_classifier(embs_ls)'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"ensembled_learn = train_ensembled_classifier(embs_ls)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er9Eo5Oi8pnz",
        "outputId": "2fc5f2d4-1d04-407e-8038-72580826f1db"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'df_ = pd.concat([pd.DataFrame(embs) for embs in embs_ls], axis=1)\\ndf_.columns = list(range(df_.shape[1]))\\n\\ntest_dl = ensembled_learn.dls.test_dl(df_) #, with_labels=True\\nprobs, _ = ensembled_learn.get_preds(dl=test_dl)\\n\\nprobs'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"df_ = pd.concat([pd.DataFrame(embs) for embs in embs_ls], axis=1)\n",
        "df_.columns = list(range(df_.shape[1]))\n",
        "\n",
        "test_dl = ensembled_learn.dls.test_dl(df_) #, with_labels=True\n",
        "probs, _ = ensembled_learn.get_preds(dl=test_dl)\n",
        "\n",
        "probs\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOYQoy07fq85"
      },
      "source": [
        "# 5) End2End fastai multimodal model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AZRVdmuNKaXI",
        "outputId": "4ca5ad98-be5a-4bf9-d753-8460e7623ce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================= training classifier with \n",
            " cnt_cols=['age', 'cumulative_peer_exit_count', 'is_optional', 'is_organizer', 'length_of_service', 'manager_length_of_service', 'meeting_lapse', 'num_direct_reports', 'start_datetime_Dayofyear', 'time_since_last_promotion', 'time_since_new_manager_start_date', 'time_since_new_org_start_date', 'timestamp'] and \n",
            " cat_cols=['job_family', 'start_datetime']==============================\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.854940</td>\n",
              "      <td>0.721385</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.709463</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.617019</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.762586</td>\n",
              "      <td>0.529770</td>\n",
              "      <td>0.770000</td>\n",
              "      <td>0.769467</td>\n",
              "      <td>0.770000</td>\n",
              "      <td>0.768308</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.662126</td>\n",
              "      <td>0.441568</td>\n",
              "      <td>0.810000</td>\n",
              "      <td>0.805784</td>\n",
              "      <td>0.810000</td>\n",
              "      <td>0.806536</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.584136</td>\n",
              "      <td>0.344992</td>\n",
              "      <td>0.870000</td>\n",
              "      <td>0.873347</td>\n",
              "      <td>0.870000</td>\n",
              "      <td>0.870000</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.562025</td>\n",
              "      <td>0.357581</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.906034</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.897604</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement since epoch 3: early stopping\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.446979</td>\n",
              "      <td>0.271782</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.934538</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.930112</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.406214</td>\n",
              "      <td>0.251744</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.923205</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.919487</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.535088</td>\n",
              "      <td>0.310713</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.889077</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.887062</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.474206</td>\n",
              "      <td>0.218192</td>\n",
              "      <td>0.940000</td>\n",
              "      <td>0.941607</td>\n",
              "      <td>0.940000</td>\n",
              "      <td>0.940091</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.421942</td>\n",
              "      <td>0.267927</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.894859</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.891305</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement since epoch 1: early stopping\n",
            "========================= training classifier with txt_col=description==============================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='105070592' class='' max='105067061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [105070592/105067061 00:02<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>7.998818</td>\n",
              "      <td>7.288180</td>\n",
              "      <td>0.109375</td>\n",
              "      <td>1462.906250</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>7.762439</td>\n",
              "      <td>6.904521</td>\n",
              "      <td>0.111607</td>\n",
              "      <td>996.770935</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>7.537092</td>\n",
              "      <td>6.551679</td>\n",
              "      <td>0.119420</td>\n",
              "      <td>700.419312</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>7.340801</td>\n",
              "      <td>6.216723</td>\n",
              "      <td>0.127232</td>\n",
              "      <td>501.058563</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>7.133634</td>\n",
              "      <td>5.904796</td>\n",
              "      <td>0.164062</td>\n",
              "      <td>366.792267</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>6.946187</td>\n",
              "      <td>5.611001</td>\n",
              "      <td>0.179688</td>\n",
              "      <td>273.417938</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>6.774039</td>\n",
              "      <td>5.339226</td>\n",
              "      <td>0.207589</td>\n",
              "      <td>208.351334</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>6.617428</td>\n",
              "      <td>5.087361</td>\n",
              "      <td>0.239955</td>\n",
              "      <td>161.961929</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>6.449153</td>\n",
              "      <td>4.853444</td>\n",
              "      <td>0.267857</td>\n",
              "      <td>128.181030</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>6.289180</td>\n",
              "      <td>4.636213</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>103.152946</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>6.132423</td>\n",
              "      <td>4.434453</td>\n",
              "      <td>0.304688</td>\n",
              "      <td>84.306038</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>5.977237</td>\n",
              "      <td>4.245704</td>\n",
              "      <td>0.325893</td>\n",
              "      <td>69.804901</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>5.838460</td>\n",
              "      <td>4.071092</td>\n",
              "      <td>0.350446</td>\n",
              "      <td>58.620922</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>5.700673</td>\n",
              "      <td>3.906120</td>\n",
              "      <td>0.376116</td>\n",
              "      <td>49.705696</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>5.583284</td>\n",
              "      <td>3.754301</td>\n",
              "      <td>0.397321</td>\n",
              "      <td>42.704353</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>5.465305</td>\n",
              "      <td>3.610411</td>\n",
              "      <td>0.412946</td>\n",
              "      <td>36.981247</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>5.338759</td>\n",
              "      <td>3.471640</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>32.189503</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>5.214763</td>\n",
              "      <td>3.338697</td>\n",
              "      <td>0.444196</td>\n",
              "      <td>28.182386</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>5.102020</td>\n",
              "      <td>3.210667</td>\n",
              "      <td>0.452009</td>\n",
              "      <td>24.795610</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>4.989580</td>\n",
              "      <td>3.087188</td>\n",
              "      <td>0.475446</td>\n",
              "      <td>21.915365</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>4.889427</td>\n",
              "      <td>2.968233</td>\n",
              "      <td>0.493304</td>\n",
              "      <td>19.457506</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>4.784821</td>\n",
              "      <td>2.854158</td>\n",
              "      <td>0.511161</td>\n",
              "      <td>17.359821</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>4.682526</td>\n",
              "      <td>2.744440</td>\n",
              "      <td>0.524554</td>\n",
              "      <td>15.555894</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>4.579467</td>\n",
              "      <td>2.638175</td>\n",
              "      <td>0.539062</td>\n",
              "      <td>13.987650</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>4.481763</td>\n",
              "      <td>2.537489</td>\n",
              "      <td>0.551339</td>\n",
              "      <td>12.647871</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>4.385592</td>\n",
              "      <td>2.442280</td>\n",
              "      <td>0.561384</td>\n",
              "      <td>11.499225</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>4.292712</td>\n",
              "      <td>2.352700</td>\n",
              "      <td>0.566964</td>\n",
              "      <td>10.513924</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>4.199485</td>\n",
              "      <td>2.267689</td>\n",
              "      <td>0.577009</td>\n",
              "      <td>9.657063</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>4.113497</td>\n",
              "      <td>2.186883</td>\n",
              "      <td>0.582589</td>\n",
              "      <td>8.907403</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>4.028620</td>\n",
              "      <td>2.109678</td>\n",
              "      <td>0.589286</td>\n",
              "      <td>8.245583</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.946882</td>\n",
              "      <td>2.034627</td>\n",
              "      <td>0.600446</td>\n",
              "      <td>7.649400</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>3.864610</td>\n",
              "      <td>1.962248</td>\n",
              "      <td>0.606027</td>\n",
              "      <td>7.115303</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>3.785516</td>\n",
              "      <td>1.892194</td>\n",
              "      <td>0.616071</td>\n",
              "      <td>6.633906</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>3.733329</td>\n",
              "      <td>1.826226</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>6.210406</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>3.658327</td>\n",
              "      <td>1.762120</td>\n",
              "      <td>0.638393</td>\n",
              "      <td>5.824775</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>3.582153</td>\n",
              "      <td>1.700006</td>\n",
              "      <td>0.648438</td>\n",
              "      <td>5.473979</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>3.511336</td>\n",
              "      <td>1.640466</td>\n",
              "      <td>0.660714</td>\n",
              "      <td>5.157570</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>3.441689</td>\n",
              "      <td>1.583749</td>\n",
              "      <td>0.665179</td>\n",
              "      <td>4.873191</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>3.372779</td>\n",
              "      <td>1.529969</td>\n",
              "      <td>0.680804</td>\n",
              "      <td>4.618032</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>3.304832</td>\n",
              "      <td>1.478607</td>\n",
              "      <td>0.699777</td>\n",
              "      <td>4.386832</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.237242</td>\n",
              "      <td>1.430253</td>\n",
              "      <td>0.712054</td>\n",
              "      <td>4.179758</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>3.170966</td>\n",
              "      <td>1.384304</td>\n",
              "      <td>0.722098</td>\n",
              "      <td>3.992046</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>3.109001</td>\n",
              "      <td>1.341642</td>\n",
              "      <td>0.728795</td>\n",
              "      <td>3.825321</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>3.047775</td>\n",
              "      <td>1.301031</td>\n",
              "      <td>0.737723</td>\n",
              "      <td>3.673080</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>2.987185</td>\n",
              "      <td>1.262794</td>\n",
              "      <td>0.746652</td>\n",
              "      <td>3.535286</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>2.927270</td>\n",
              "      <td>1.227252</td>\n",
              "      <td>0.754464</td>\n",
              "      <td>3.411840</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>2.867660</td>\n",
              "      <td>1.193788</td>\n",
              "      <td>0.757813</td>\n",
              "      <td>3.299557</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>2.810281</td>\n",
              "      <td>1.162298</td>\n",
              "      <td>0.755580</td>\n",
              "      <td>3.197273</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>2.756253</td>\n",
              "      <td>1.132412</td>\n",
              "      <td>0.758929</td>\n",
              "      <td>3.103132</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>2.704098</td>\n",
              "      <td>1.103699</td>\n",
              "      <td>0.768973</td>\n",
              "      <td>3.015299</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.652079</td>\n",
              "      <td>1.076788</td>\n",
              "      <td>0.774554</td>\n",
              "      <td>2.935236</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>2.598809</td>\n",
              "      <td>1.050301</td>\n",
              "      <td>0.776786</td>\n",
              "      <td>2.858510</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>2.548553</td>\n",
              "      <td>1.025251</td>\n",
              "      <td>0.781250</td>\n",
              "      <td>2.787796</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>2.499580</td>\n",
              "      <td>1.000753</td>\n",
              "      <td>0.787946</td>\n",
              "      <td>2.720330</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>2.450671</td>\n",
              "      <td>0.976586</td>\n",
              "      <td>0.789063</td>\n",
              "      <td>2.655376</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>2.404595</td>\n",
              "      <td>0.952863</td>\n",
              "      <td>0.791295</td>\n",
              "      <td>2.593123</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>2.359646</td>\n",
              "      <td>0.929646</td>\n",
              "      <td>0.795759</td>\n",
              "      <td>2.533612</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>2.314452</td>\n",
              "      <td>0.906772</td>\n",
              "      <td>0.799107</td>\n",
              "      <td>2.476317</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>2.270547</td>\n",
              "      <td>0.884393</td>\n",
              "      <td>0.803571</td>\n",
              "      <td>2.421514</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>2.227354</td>\n",
              "      <td>0.862626</td>\n",
              "      <td>0.808036</td>\n",
              "      <td>2.369375</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.184833</td>\n",
              "      <td>0.842070</td>\n",
              "      <td>0.813616</td>\n",
              "      <td>2.321167</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>2.147864</td>\n",
              "      <td>0.822353</td>\n",
              "      <td>0.813616</td>\n",
              "      <td>2.275848</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>2.107044</td>\n",
              "      <td>0.803769</td>\n",
              "      <td>0.813616</td>\n",
              "      <td>2.233944</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>2.067451</td>\n",
              "      <td>0.785742</td>\n",
              "      <td>0.818080</td>\n",
              "      <td>2.194034</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>2.027964</td>\n",
              "      <td>0.768459</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>2.156440</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.989743</td>\n",
              "      <td>0.751576</td>\n",
              "      <td>0.823661</td>\n",
              "      <td>2.120339</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>1.955372</td>\n",
              "      <td>0.735712</td>\n",
              "      <td>0.827009</td>\n",
              "      <td>2.086967</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>1.919237</td>\n",
              "      <td>0.720564</td>\n",
              "      <td>0.830357</td>\n",
              "      <td>2.055593</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>1.884936</td>\n",
              "      <td>0.705964</td>\n",
              "      <td>0.832589</td>\n",
              "      <td>2.025798</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>1.849957</td>\n",
              "      <td>0.691766</td>\n",
              "      <td>0.834821</td>\n",
              "      <td>1.997240</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.815579</td>\n",
              "      <td>0.678179</td>\n",
              "      <td>0.837054</td>\n",
              "      <td>1.970286</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>1.783636</td>\n",
              "      <td>0.665540</td>\n",
              "      <td>0.845982</td>\n",
              "      <td>1.945540</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>1.750516</td>\n",
              "      <td>0.653387</td>\n",
              "      <td>0.852679</td>\n",
              "      <td>1.922039</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.720136</td>\n",
              "      <td>0.641996</td>\n",
              "      <td>0.854911</td>\n",
              "      <td>1.900270</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>1.689206</td>\n",
              "      <td>0.631334</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>1.880118</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.660111</td>\n",
              "      <td>0.620463</td>\n",
              "      <td>0.863839</td>\n",
              "      <td>1.859789</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>1.631786</td>\n",
              "      <td>0.610142</td>\n",
              "      <td>0.861607</td>\n",
              "      <td>1.840693</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>1.603501</td>\n",
              "      <td>0.600329</td>\n",
              "      <td>0.861607</td>\n",
              "      <td>1.822719</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>1.576701</td>\n",
              "      <td>0.591045</td>\n",
              "      <td>0.862723</td>\n",
              "      <td>1.805875</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>1.549174</td>\n",
              "      <td>0.582359</td>\n",
              "      <td>0.872768</td>\n",
              "      <td>1.790256</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.522341</td>\n",
              "      <td>0.574552</td>\n",
              "      <td>0.880580</td>\n",
              "      <td>1.776335</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>1.496639</td>\n",
              "      <td>0.566761</td>\n",
              "      <td>0.880580</td>\n",
              "      <td>1.762548</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>1.471033</td>\n",
              "      <td>0.559294</td>\n",
              "      <td>0.880580</td>\n",
              "      <td>1.749436</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>1.445866</td>\n",
              "      <td>0.552231</td>\n",
              "      <td>0.880580</td>\n",
              "      <td>1.737124</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.427531</td>\n",
              "      <td>0.546466</td>\n",
              "      <td>0.880580</td>\n",
              "      <td>1.727139</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>1.403684</td>\n",
              "      <td>0.541139</td>\n",
              "      <td>0.881696</td>\n",
              "      <td>1.717962</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>1.386144</td>\n",
              "      <td>0.536439</td>\n",
              "      <td>0.880580</td>\n",
              "      <td>1.709907</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>1.362707</td>\n",
              "      <td>0.532291</td>\n",
              "      <td>0.881696</td>\n",
              "      <td>1.702828</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>1.340053</td>\n",
              "      <td>0.528612</td>\n",
              "      <td>0.883929</td>\n",
              "      <td>1.696575</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>1.320863</td>\n",
              "      <td>0.525378</td>\n",
              "      <td>0.883929</td>\n",
              "      <td>1.691097</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.298766</td>\n",
              "      <td>0.522699</td>\n",
              "      <td>0.885045</td>\n",
              "      <td>1.686574</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>1.277577</td>\n",
              "      <td>0.520504</td>\n",
              "      <td>0.885045</td>\n",
              "      <td>1.682876</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>1.257894</td>\n",
              "      <td>0.518714</td>\n",
              "      <td>0.885045</td>\n",
              "      <td>1.679866</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>1.237151</td>\n",
              "      <td>0.517354</td>\n",
              "      <td>0.885045</td>\n",
              "      <td>1.677582</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>1.217981</td>\n",
              "      <td>0.516312</td>\n",
              "      <td>0.885045</td>\n",
              "      <td>1.675836</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>1.198785</td>\n",
              "      <td>0.515572</td>\n",
              "      <td>0.885045</td>\n",
              "      <td>1.674595</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>1.179137</td>\n",
              "      <td>0.515102</td>\n",
              "      <td>0.886161</td>\n",
              "      <td>1.673809</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>1.160502</td>\n",
              "      <td>0.514807</td>\n",
              "      <td>0.886161</td>\n",
              "      <td>1.673316</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>1.143445</td>\n",
              "      <td>0.514702</td>\n",
              "      <td>0.886161</td>\n",
              "      <td>1.673140</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>1.130522</td>\n",
              "      <td>0.514655</td>\n",
              "      <td>0.886161</td>\n",
              "      <td>1.673061</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.340637</td>\n",
              "      <td>1.338971</td>\n",
              "      <td>0.324324</td>\n",
              "      <td>0.413490</td>\n",
              "      <td>0.324324</td>\n",
              "      <td>0.335062</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.296805</td>\n",
              "      <td>1.292839</td>\n",
              "      <td>0.459459</td>\n",
              "      <td>0.366637</td>\n",
              "      <td>0.459459</td>\n",
              "      <td>0.376840</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.265078</td>\n",
              "      <td>1.160010</td>\n",
              "      <td>0.337838</td>\n",
              "      <td>0.516564</td>\n",
              "      <td>0.337838</td>\n",
              "      <td>0.332737</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.232622</td>\n",
              "      <td>1.146966</td>\n",
              "      <td>0.310811</td>\n",
              "      <td>0.529663</td>\n",
              "      <td>0.310811</td>\n",
              "      <td>0.332748</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.202296</td>\n",
              "      <td>1.164321</td>\n",
              "      <td>0.243243</td>\n",
              "      <td>0.235811</td>\n",
              "      <td>0.243243</td>\n",
              "      <td>0.203771</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement since epoch 3: early stopping\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.116468</td>\n",
              "      <td>1.166912</td>\n",
              "      <td>0.216216</td>\n",
              "      <td>0.120783</td>\n",
              "      <td>0.216216</td>\n",
              "      <td>0.154054</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.106420</td>\n",
              "      <td>1.137415</td>\n",
              "      <td>0.459459</td>\n",
              "      <td>0.387294</td>\n",
              "      <td>0.459459</td>\n",
              "      <td>0.385070</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.062212</td>\n",
              "      <td>1.207304</td>\n",
              "      <td>0.554054</td>\n",
              "      <td>0.488283</td>\n",
              "      <td>0.554054</td>\n",
              "      <td>0.494878</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.069526</td>\n",
              "      <td>1.125123</td>\n",
              "      <td>0.527027</td>\n",
              "      <td>0.456615</td>\n",
              "      <td>0.527027</td>\n",
              "      <td>0.487739</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.079456</td>\n",
              "      <td>1.199147</td>\n",
              "      <td>0.297297</td>\n",
              "      <td>0.119806</td>\n",
              "      <td>0.297297</td>\n",
              "      <td>0.170788</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement since epoch 1: early stopping\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================= training classifier with txt_col=title==============================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.094016</td>\n",
              "      <td>5.122253</td>\n",
              "      <td>0.279297</td>\n",
              "      <td>167.712784</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.937854</td>\n",
              "      <td>4.852761</td>\n",
              "      <td>0.302734</td>\n",
              "      <td>128.093597</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.877479</td>\n",
              "      <td>4.605003</td>\n",
              "      <td>0.326172</td>\n",
              "      <td>99.983269</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.723520</td>\n",
              "      <td>4.376663</td>\n",
              "      <td>0.335938</td>\n",
              "      <td>79.572075</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>4.554371</td>\n",
              "      <td>4.163024</td>\n",
              "      <td>0.339844</td>\n",
              "      <td>64.265564</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>4.401287</td>\n",
              "      <td>3.958952</td>\n",
              "      <td>0.353516</td>\n",
              "      <td>52.402378</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>4.264030</td>\n",
              "      <td>3.768576</td>\n",
              "      <td>0.365234</td>\n",
              "      <td>43.318321</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>4.251336</td>\n",
              "      <td>3.595887</td>\n",
              "      <td>0.376953</td>\n",
              "      <td>36.448029</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>4.122576</td>\n",
              "      <td>3.432166</td>\n",
              "      <td>0.390625</td>\n",
              "      <td>30.943583</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.993690</td>\n",
              "      <td>3.280746</td>\n",
              "      <td>0.400391</td>\n",
              "      <td>26.595617</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.867503</td>\n",
              "      <td>3.139427</td>\n",
              "      <td>0.421875</td>\n",
              "      <td>23.090626</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>3.751046</td>\n",
              "      <td>3.007988</td>\n",
              "      <td>0.439453</td>\n",
              "      <td>20.246618</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>3.644452</td>\n",
              "      <td>2.884123</td>\n",
              "      <td>0.468750</td>\n",
              "      <td>17.887865</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>3.546126</td>\n",
              "      <td>2.768052</td>\n",
              "      <td>0.482422</td>\n",
              "      <td>15.927582</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>3.457318</td>\n",
              "      <td>2.657456</td>\n",
              "      <td>0.498047</td>\n",
              "      <td>14.259968</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>3.363692</td>\n",
              "      <td>2.551058</td>\n",
              "      <td>0.501953</td>\n",
              "      <td>12.820662</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>3.278697</td>\n",
              "      <td>2.448612</td>\n",
              "      <td>0.505859</td>\n",
              "      <td>11.572273</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>3.193463</td>\n",
              "      <td>2.349347</td>\n",
              "      <td>0.511719</td>\n",
              "      <td>10.478724</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>3.108315</td>\n",
              "      <td>2.253129</td>\n",
              "      <td>0.509766</td>\n",
              "      <td>9.517467</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>3.029889</td>\n",
              "      <td>2.160296</td>\n",
              "      <td>0.525391</td>\n",
              "      <td>8.673702</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.963307</td>\n",
              "      <td>2.072191</td>\n",
              "      <td>0.533203</td>\n",
              "      <td>7.942204</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.896607</td>\n",
              "      <td>1.990334</td>\n",
              "      <td>0.546875</td>\n",
              "      <td>7.317980</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.826372</td>\n",
              "      <td>1.914669</td>\n",
              "      <td>0.560547</td>\n",
              "      <td>6.784691</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.757118</td>\n",
              "      <td>1.845290</td>\n",
              "      <td>0.566406</td>\n",
              "      <td>6.329937</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.688960</td>\n",
              "      <td>1.782242</td>\n",
              "      <td>0.580078</td>\n",
              "      <td>5.943164</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.624864</td>\n",
              "      <td>1.724815</td>\n",
              "      <td>0.580078</td>\n",
              "      <td>5.611485</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.564462</td>\n",
              "      <td>1.672319</td>\n",
              "      <td>0.595703</td>\n",
              "      <td>5.324499</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.506496</td>\n",
              "      <td>1.624188</td>\n",
              "      <td>0.607422</td>\n",
              "      <td>5.074296</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>2.451829</td>\n",
              "      <td>1.580467</td>\n",
              "      <td>0.626953</td>\n",
              "      <td>4.857223</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>2.397285</td>\n",
              "      <td>1.540089</td>\n",
              "      <td>0.646484</td>\n",
              "      <td>4.665004</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.346240</td>\n",
              "      <td>1.504550</td>\n",
              "      <td>0.662109</td>\n",
              "      <td>4.502125</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>2.295016</td>\n",
              "      <td>1.471498</td>\n",
              "      <td>0.666016</td>\n",
              "      <td>4.355756</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>2.245703</td>\n",
              "      <td>1.440992</td>\n",
              "      <td>0.666016</td>\n",
              "      <td>4.224887</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>2.224381</td>\n",
              "      <td>1.411428</td>\n",
              "      <td>0.669922</td>\n",
              "      <td>4.101810</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>2.178234</td>\n",
              "      <td>1.383411</td>\n",
              "      <td>0.669922</td>\n",
              "      <td>3.988482</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.134268</td>\n",
              "      <td>1.357971</td>\n",
              "      <td>0.669922</td>\n",
              "      <td>3.888297</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>2.093053</td>\n",
              "      <td>1.334014</td>\n",
              "      <td>0.677734</td>\n",
              "      <td>3.796252</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>2.051242</td>\n",
              "      <td>1.311167</td>\n",
              "      <td>0.677734</td>\n",
              "      <td>3.710500</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>2.010808</td>\n",
              "      <td>1.289350</td>\n",
              "      <td>0.679688</td>\n",
              "      <td>3.630427</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.973299</td>\n",
              "      <td>1.267604</td>\n",
              "      <td>0.679688</td>\n",
              "      <td>3.552330</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.935883</td>\n",
              "      <td>1.247303</td>\n",
              "      <td>0.683594</td>\n",
              "      <td>3.480943</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.900396</td>\n",
              "      <td>1.227013</td>\n",
              "      <td>0.689453</td>\n",
              "      <td>3.411027</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.868657</td>\n",
              "      <td>1.207763</td>\n",
              "      <td>0.689453</td>\n",
              "      <td>3.345993</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.834959</td>\n",
              "      <td>1.189164</td>\n",
              "      <td>0.691406</td>\n",
              "      <td>3.284334</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.801738</td>\n",
              "      <td>1.171249</td>\n",
              "      <td>0.691406</td>\n",
              "      <td>3.226020</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.768488</td>\n",
              "      <td>1.153995</td>\n",
              "      <td>0.697266</td>\n",
              "      <td>3.170836</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.737133</td>\n",
              "      <td>1.136874</td>\n",
              "      <td>0.697266</td>\n",
              "      <td>3.117008</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.706916</td>\n",
              "      <td>1.120923</td>\n",
              "      <td>0.699219</td>\n",
              "      <td>3.067684</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.680450</td>\n",
              "      <td>1.104775</td>\n",
              "      <td>0.701172</td>\n",
              "      <td>3.018546</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.654002</td>\n",
              "      <td>1.089123</td>\n",
              "      <td>0.699219</td>\n",
              "      <td>2.971668</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.629930</td>\n",
              "      <td>1.073636</td>\n",
              "      <td>0.699219</td>\n",
              "      <td>2.925998</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.603352</td>\n",
              "      <td>1.058869</td>\n",
              "      <td>0.701172</td>\n",
              "      <td>2.883108</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.577119</td>\n",
              "      <td>1.045002</td>\n",
              "      <td>0.701172</td>\n",
              "      <td>2.843404</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.552442</td>\n",
              "      <td>1.031775</td>\n",
              "      <td>0.703125</td>\n",
              "      <td>2.806041</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.527436</td>\n",
              "      <td>1.018724</td>\n",
              "      <td>0.703125</td>\n",
              "      <td>2.769660</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.502983</td>\n",
              "      <td>1.006827</td>\n",
              "      <td>0.708984</td>\n",
              "      <td>2.736903</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.481416</td>\n",
              "      <td>0.995644</td>\n",
              "      <td>0.707031</td>\n",
              "      <td>2.706468</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.458174</td>\n",
              "      <td>0.984746</td>\n",
              "      <td>0.707031</td>\n",
              "      <td>2.677133</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.437140</td>\n",
              "      <td>0.975198</td>\n",
              "      <td>0.714844</td>\n",
              "      <td>2.651691</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.415690</td>\n",
              "      <td>0.966466</td>\n",
              "      <td>0.714844</td>\n",
              "      <td>2.628638</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.394637</td>\n",
              "      <td>0.957700</td>\n",
              "      <td>0.716797</td>\n",
              "      <td>2.605696</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.376829</td>\n",
              "      <td>0.949488</td>\n",
              "      <td>0.716797</td>\n",
              "      <td>2.584385</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>1.357159</td>\n",
              "      <td>0.941486</td>\n",
              "      <td>0.720703</td>\n",
              "      <td>2.563789</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.338174</td>\n",
              "      <td>0.933148</td>\n",
              "      <td>0.734375</td>\n",
              "      <td>2.542500</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>1.319711</td>\n",
              "      <td>0.925043</td>\n",
              "      <td>0.736328</td>\n",
              "      <td>2.521977</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.301917</td>\n",
              "      <td>0.916968</td>\n",
              "      <td>0.740234</td>\n",
              "      <td>2.501693</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>1.286201</td>\n",
              "      <td>0.909108</td>\n",
              "      <td>0.740234</td>\n",
              "      <td>2.482107</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>1.269050</td>\n",
              "      <td>0.901538</td>\n",
              "      <td>0.744141</td>\n",
              "      <td>2.463389</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>1.252016</td>\n",
              "      <td>0.894271</td>\n",
              "      <td>0.744141</td>\n",
              "      <td>2.445551</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>1.235780</td>\n",
              "      <td>0.886988</td>\n",
              "      <td>0.746094</td>\n",
              "      <td>2.427807</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.219234</td>\n",
              "      <td>0.880054</td>\n",
              "      <td>0.746094</td>\n",
              "      <td>2.411029</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>1.204737</td>\n",
              "      <td>0.873315</td>\n",
              "      <td>0.744141</td>\n",
              "      <td>2.394837</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>1.189290</td>\n",
              "      <td>0.866796</td>\n",
              "      <td>0.746094</td>\n",
              "      <td>2.379275</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.176454</td>\n",
              "      <td>0.860843</td>\n",
              "      <td>0.746094</td>\n",
              "      <td>2.365153</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>1.161730</td>\n",
              "      <td>0.855086</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>2.351578</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.147326</td>\n",
              "      <td>0.849721</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>2.338995</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>1.134125</td>\n",
              "      <td>0.844389</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>2.326556</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>1.120609</td>\n",
              "      <td>0.839494</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>2.315196</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>1.107678</td>\n",
              "      <td>0.834921</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>2.304631</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>1.095415</td>\n",
              "      <td>0.830884</td>\n",
              "      <td>0.751953</td>\n",
              "      <td>2.295346</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.082524</td>\n",
              "      <td>0.826747</td>\n",
              "      <td>0.751953</td>\n",
              "      <td>2.285871</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>1.070863</td>\n",
              "      <td>0.822891</td>\n",
              "      <td>0.753906</td>\n",
              "      <td>2.277072</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>1.059066</td>\n",
              "      <td>0.819232</td>\n",
              "      <td>0.751953</td>\n",
              "      <td>2.268757</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>1.047600</td>\n",
              "      <td>0.815857</td>\n",
              "      <td>0.751953</td>\n",
              "      <td>2.261112</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.042582</td>\n",
              "      <td>0.813234</td>\n",
              "      <td>0.751953</td>\n",
              "      <td>2.255189</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>1.031105</td>\n",
              "      <td>0.810887</td>\n",
              "      <td>0.753906</td>\n",
              "      <td>2.249902</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>1.029431</td>\n",
              "      <td>0.808772</td>\n",
              "      <td>0.751953</td>\n",
              "      <td>2.245148</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>1.018544</td>\n",
              "      <td>0.806753</td>\n",
              "      <td>0.751953</td>\n",
              "      <td>2.240622</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>1.007688</td>\n",
              "      <td>0.805209</td>\n",
              "      <td>0.751953</td>\n",
              "      <td>2.237163</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>1.013879</td>\n",
              "      <td>0.803900</td>\n",
              "      <td>0.751953</td>\n",
              "      <td>2.234237</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.002724</td>\n",
              "      <td>0.803190</td>\n",
              "      <td>0.751953</td>\n",
              "      <td>2.232651</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.992131</td>\n",
              "      <td>0.802403</td>\n",
              "      <td>0.751953</td>\n",
              "      <td>2.230895</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.982114</td>\n",
              "      <td>0.801764</td>\n",
              "      <td>0.751953</td>\n",
              "      <td>2.229469</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.972373</td>\n",
              "      <td>0.801425</td>\n",
              "      <td>0.753906</td>\n",
              "      <td>2.228715</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.963007</td>\n",
              "      <td>0.800972</td>\n",
              "      <td>0.753906</td>\n",
              "      <td>2.227704</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.953532</td>\n",
              "      <td>0.800646</td>\n",
              "      <td>0.753906</td>\n",
              "      <td>2.226978</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.944719</td>\n",
              "      <td>0.800572</td>\n",
              "      <td>0.753906</td>\n",
              "      <td>2.226815</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.935629</td>\n",
              "      <td>0.800334</td>\n",
              "      <td>0.753906</td>\n",
              "      <td>2.226284</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.927341</td>\n",
              "      <td>0.800341</td>\n",
              "      <td>0.753906</td>\n",
              "      <td>2.226300</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement since epoch 97: early stopping\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.319238</td>\n",
              "      <td>1.179999</td>\n",
              "      <td>0.324324</td>\n",
              "      <td>0.647098</td>\n",
              "      <td>0.324324</td>\n",
              "      <td>0.279552</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.296849</td>\n",
              "      <td>1.166551</td>\n",
              "      <td>0.283784</td>\n",
              "      <td>0.657131</td>\n",
              "      <td>0.283784</td>\n",
              "      <td>0.251546</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.213418</td>\n",
              "      <td>1.077359</td>\n",
              "      <td>0.283784</td>\n",
              "      <td>0.536103</td>\n",
              "      <td>0.283784</td>\n",
              "      <td>0.250457</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.154161</td>\n",
              "      <td>1.017613</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.553260</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.502091</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.120496</td>\n",
              "      <td>0.983145</td>\n",
              "      <td>0.567568</td>\n",
              "      <td>0.580995</td>\n",
              "      <td>0.567568</td>\n",
              "      <td>0.544710</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.093082</td>\n",
              "      <td>0.957974</td>\n",
              "      <td>0.594595</td>\n",
              "      <td>0.601609</td>\n",
              "      <td>0.594595</td>\n",
              "      <td>0.573842</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.077350</td>\n",
              "      <td>0.946808</td>\n",
              "      <td>0.594595</td>\n",
              "      <td>0.601609</td>\n",
              "      <td>0.594595</td>\n",
              "      <td>0.573842</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.057384</td>\n",
              "      <td>0.941367</td>\n",
              "      <td>0.621622</td>\n",
              "      <td>0.604805</td>\n",
              "      <td>0.621622</td>\n",
              "      <td>0.605370</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.038539</td>\n",
              "      <td>0.946829</td>\n",
              "      <td>0.621622</td>\n",
              "      <td>0.604805</td>\n",
              "      <td>0.621622</td>\n",
              "      <td>0.605370</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement since epoch 7: early stopping\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.935543</td>\n",
              "      <td>0.963233</td>\n",
              "      <td>0.567568</td>\n",
              "      <td>0.588249</td>\n",
              "      <td>0.567568</td>\n",
              "      <td>0.540930</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.941634</td>\n",
              "      <td>1.011578</td>\n",
              "      <td>0.554054</td>\n",
              "      <td>0.547973</td>\n",
              "      <td>0.554054</td>\n",
              "      <td>0.543053</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.961577</td>\n",
              "      <td>1.001945</td>\n",
              "      <td>0.432432</td>\n",
              "      <td>0.504054</td>\n",
              "      <td>0.432432</td>\n",
              "      <td>0.461933</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.929822</td>\n",
              "      <td>1.030023</td>\n",
              "      <td>0.554054</td>\n",
              "      <td>0.547161</td>\n",
              "      <td>0.554054</td>\n",
              "      <td>0.546979</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement since epoch 0: early stopping\n",
            "========================= extracting  tabular embeddings==============================\n",
            "====== We will get embedding from Linear(in_features=200, out_features=100, bias=False) =======\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================= extracting  txt_col=description doc embeddings==============================\n",
            "========================= extracting  txt_col=title doc embeddings==============================\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.916405</td>\n",
              "      <td>1.120879</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.619780</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.599535</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.721671</td>\n",
              "      <td>0.944916</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.787500</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.672091</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.709378</td>\n",
              "      <td>0.643204</td>\n",
              "      <td>0.807692</td>\n",
              "      <td>0.830769</td>\n",
              "      <td>0.807692</td>\n",
              "      <td>0.802564</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.651900</td>\n",
              "      <td>0.758316</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.774725</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.664835</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement since epoch 2: early stopping\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.600256</td>\n",
              "      <td>0.851729</td>\n",
              "      <td>0.730769</td>\n",
              "      <td>0.787330</td>\n",
              "      <td>0.730769</td>\n",
              "      <td>0.710365</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.690266</td>\n",
              "      <td>0.709370</td>\n",
              "      <td>0.807692</td>\n",
              "      <td>0.830769</td>\n",
              "      <td>0.807692</td>\n",
              "      <td>0.802564</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.709442</td>\n",
              "      <td>0.618744</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.669231</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.659341</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.588990</td>\n",
              "      <td>0.519415</td>\n",
              "      <td>0.807692</td>\n",
              "      <td>0.830769</td>\n",
              "      <td>0.807692</td>\n",
              "      <td>0.802564</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.507511</td>\n",
              "      <td>0.846024</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.505495</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.542308</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement since epoch 1: early stopping\n",
            "========================= calculating tab_cols ((['age', 'cumulative_peer_exit_count', 'is_optional', 'is_organizer', 'length_of_service', 'manager_length_of_service', 'meeting_lapse', 'num_direct_reports', 'start_datetime_Dayofyear', 'time_since_last_promotion', 'time_since_new_manager_start_date', 'time_since_new_org_start_date', 'timestamp'], ['job_family', 'start_datetime', 'manager_length_of_service_na'])) probs==============================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.578268</td>\n",
              "      <td>0.414896</td>\n",
              "      <td>0.730769</td>\n",
              "      <td>0.607459</td>\n",
              "      <td>0.730769</td>\n",
              "      <td>0.655641</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.526997</td>\n",
              "      <td>0.228297</td>\n",
              "      <td>0.884615</td>\n",
              "      <td>0.892646</td>\n",
              "      <td>0.884615</td>\n",
              "      <td>0.886917</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.517243</td>\n",
              "      <td>0.244563</td>\n",
              "      <td>0.961538</td>\n",
              "      <td>0.964103</td>\n",
              "      <td>0.961538</td>\n",
              "      <td>0.960453</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement since epoch 1: early stopping\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.394238</td>\n",
              "      <td>0.197607</td>\n",
              "      <td>0.961538</td>\n",
              "      <td>0.964103</td>\n",
              "      <td>0.961538</td>\n",
              "      <td>0.960453</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.366615</td>\n",
              "      <td>0.278249</td>\n",
              "      <td>0.884615</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.884615</td>\n",
              "      <td>0.887749</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.383142</td>\n",
              "      <td>0.230644</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.923844</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.386074</td>\n",
              "      <td>0.250172</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.942308</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.925613</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement since epoch 0: early stopping\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "((array([2, 2, 2, 2, 1, 0, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1,\n",
              "         2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1,\n",
              "         2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 0, 2, 1, 1, 2, 1,\n",
              "         1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2,\n",
              "         1, 1, 1, 1, 2, 1, 1, 2, 2, 0, 1, 2, 0, 0, 0, 2, 2, 0, 2, 2, 2, 0,\n",
              "         0, 1, 2, 0, 2, 2, 0, 1, 0, 2, 2, 1, 2, 2, 0, 0, 2, 2, 2, 2]),\n",
              "  array([2, 2, 2, 2, 1, 0, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1,\n",
              "         2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1,\n",
              "         2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 0, 2, 1, 1, 2, 1,\n",
              "         1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2,\n",
              "         1, 1, 1, 1, 2, 1, 1, 2, 2, 0, 1, 2, 0, 0, 0, 2, 2, 0, 2, 2, 2, 0,\n",
              "         0, 1, 2, 0, 2, 2, 0, 1, 0, 2, 2, 1, 2, 2, 0, 0, 2, 2, 2, 2])),\n",
              " (tensor([[5.8697e-02, 3.7085e-02, 9.0422e-01],\n",
              "          [3.1757e-02, 1.7622e-01, 7.9202e-01],\n",
              "          [5.8697e-02, 3.7085e-02, 9.0422e-01],\n",
              "          [2.3416e-02, 6.3086e-03, 9.7028e-01],\n",
              "          [2.0937e-01, 4.7468e-01, 3.1594e-01],\n",
              "          [5.8053e-01, 5.2601e-02, 3.6687e-01],\n",
              "          [3.7861e-03, 2.9499e-03, 9.9326e-01],\n",
              "          [2.3508e-01, 8.2581e-03, 7.5666e-01],\n",
              "          [5.7352e-01, 4.9064e-02, 3.7741e-01],\n",
              "          [3.2719e-02, 1.9246e-01, 7.7483e-01],\n",
              "          [3.7861e-03, 2.9499e-03, 9.9326e-01],\n",
              "          [2.3291e-01, 8.5493e-03, 7.5854e-01],\n",
              "          [1.2810e-01, 2.5159e-02, 8.4674e-01],\n",
              "          [1.3131e-01, 2.6925e-02, 8.4176e-01],\n",
              "          [2.0937e-01, 4.7468e-01, 3.1594e-01],\n",
              "          [2.3757e-02, 6.5584e-03, 9.6968e-01],\n",
              "          [9.8844e-03, 9.8466e-01, 5.4532e-03],\n",
              "          [3.3837e-06, 9.9979e-01, 2.0297e-04],\n",
              "          [2.5704e-02, 9.6133e-01, 1.2966e-02],\n",
              "          [2.5704e-02, 9.6133e-01, 1.2966e-02],\n",
              "          [2.5704e-02, 9.6133e-01, 1.2966e-02],\n",
              "          [2.5704e-02, 9.6133e-01, 1.2966e-02],\n",
              "          [1.4075e-03, 4.8011e-04, 9.9811e-01],\n",
              "          [5.6073e-02, 8.0880e-01, 1.3513e-01],\n",
              "          [2.0019e-02, 7.3727e-02, 9.0625e-01],\n",
              "          [2.9920e-06, 9.9982e-01, 1.7922e-04],\n",
              "          [5.2104e-02, 3.1191e-01, 6.3598e-01],\n",
              "          [1.9663e-02, 6.9967e-02, 9.1037e-01],\n",
              "          [1.3648e-02, 2.5565e-02, 9.6079e-01],\n",
              "          [2.0459e-02, 7.8230e-02, 9.0131e-01],\n",
              "          [2.0233e-02, 7.5907e-02, 9.0386e-01],\n",
              "          [4.1993e-02, 8.4292e-01, 1.1509e-01],\n",
              "          [4.1993e-02, 8.4292e-01, 1.1509e-01],\n",
              "          [4.1993e-02, 8.4292e-01, 1.1509e-01],\n",
              "          [1.3563e-02, 2.5403e-02, 9.6103e-01],\n",
              "          [1.3483e-02, 2.5262e-02, 9.6125e-01],\n",
              "          [4.1993e-02, 8.4292e-01, 1.1509e-01],\n",
              "          [4.1993e-02, 8.4292e-01, 1.1509e-01],\n",
              "          [4.1993e-02, 8.4292e-01, 1.1509e-01],\n",
              "          [2.0706e-02, 8.0824e-02, 8.9847e-01],\n",
              "          [2.5704e-02, 9.6133e-01, 1.2966e-02],\n",
              "          [1.3860e-02, 2.5986e-02, 9.6015e-01],\n",
              "          [5.5077e-02, 8.1480e-01, 1.3012e-01],\n",
              "          [9.8844e-03, 9.8466e-01, 5.4532e-03],\n",
              "          [3.7484e-03, 7.7553e-04, 9.9548e-01],\n",
              "          [9.8844e-03, 9.8466e-01, 5.4532e-03],\n",
              "          [6.0510e-02, 5.1090e-01, 4.2859e-01],\n",
              "          [5.2310e-02, 3.2061e-01, 6.2708e-01],\n",
              "          [1.3404e-02, 2.5122e-02, 9.6147e-01],\n",
              "          [2.5704e-02, 9.6133e-01, 1.2966e-02],\n",
              "          [9.8844e-03, 9.8466e-01, 5.4532e-03],\n",
              "          [9.8844e-03, 9.8466e-01, 5.4532e-03],\n",
              "          [9.8844e-03, 9.8466e-01, 5.4532e-03],\n",
              "          [1.4483e-03, 5.0079e-04, 9.9805e-01],\n",
              "          [1.9819e-02, 7.1704e-02, 9.0848e-01],\n",
              "          [3.6398e-03, 7.4107e-04, 9.9562e-01],\n",
              "          [6.2726e-02, 5.8611e-03, 9.3141e-01],\n",
              "          [3.5301e-03, 7.0780e-04, 9.9576e-01],\n",
              "          [3.8579e-03, 8.1061e-04, 9.9533e-01],\n",
              "          [4.5896e-02, 5.8181e-01, 3.7229e-01],\n",
              "          [7.5414e-01, 2.3868e-01, 7.1819e-03],\n",
              "          [2.8077e-03, 7.0111e-04, 9.9649e-01],\n",
              "          [4.5896e-02, 5.8181e-01, 3.7229e-01],\n",
              "          [4.5896e-02, 5.8181e-01, 3.7229e-01],\n",
              "          [2.7224e-03, 6.7088e-04, 9.9661e-01],\n",
              "          [4.5896e-02, 5.8181e-01, 3.7229e-01],\n",
              "          [4.5896e-02, 5.8181e-01, 3.7229e-01],\n",
              "          [4.5896e-02, 5.8181e-01, 3.7229e-01],\n",
              "          [6.0510e-02, 5.1090e-01, 4.2859e-01],\n",
              "          [1.3755e-02, 2.5785e-02, 9.6046e-01],\n",
              "          [2.7562e-02, 2.1074e-01, 7.6170e-01],\n",
              "          [2.7562e-02, 2.1074e-01, 7.6170e-01],\n",
              "          [3.8879e-01, 5.9891e-01, 1.2302e-02],\n",
              "          [6.9183e-03, 3.6810e-02, 9.5627e-01],\n",
              "          [1.4562e-05, 9.9993e-01, 5.0686e-05],\n",
              "          [1.9469e-02, 1.5260e-01, 8.2793e-01],\n",
              "          [1.9469e-02, 1.5260e-01, 8.2793e-01],\n",
              "          [4.3048e-03, 1.1808e-03, 9.9451e-01],\n",
              "          [4.3048e-03, 1.1808e-03, 9.9451e-01],\n",
              "          [3.4918e-01, 6.4196e-01, 8.8513e-03],\n",
              "          [8.7383e-03, 4.2594e-02, 9.4867e-01],\n",
              "          [3.3358e-02, 1.2685e-01, 8.3980e-01],\n",
              "          [3.8879e-01, 5.9891e-01, 1.2302e-02],\n",
              "          [1.0147e-02, 3.2221e-02, 9.5763e-01],\n",
              "          [1.4883e-02, 2.4690e-02, 9.6043e-01],\n",
              "          [1.4978e-02, 2.5892e-02, 9.5913e-01],\n",
              "          [6.8761e-03, 3.5891e-02, 9.5723e-01],\n",
              "          [1.0147e-02, 3.2221e-02, 9.5763e-01],\n",
              "          [3.4918e-01, 6.4196e-01, 8.8513e-03],\n",
              "          [2.4348e-01, 6.7469e-01, 8.1826e-02],\n",
              "          [3.8879e-01, 5.9891e-01, 1.2302e-02],\n",
              "          [3.8879e-01, 5.9891e-01, 1.2302e-02],\n",
              "          [1.1022e-02, 1.0335e-01, 8.8563e-01],\n",
              "          [3.8879e-01, 5.9891e-01, 1.2302e-02],\n",
              "          [1.2334e-01, 8.6321e-01, 1.3445e-02],\n",
              "          [1.1022e-02, 1.0335e-01, 8.8563e-01],\n",
              "          [3.3750e-02, 1.2175e-01, 8.4450e-01],\n",
              "          [6.2125e-01, 3.6743e-01, 1.1320e-02],\n",
              "          [3.4918e-01, 6.4196e-01, 8.8513e-03],\n",
              "          [2.4450e-02, 8.3715e-02, 8.9184e-01],\n",
              "          [9.1543e-01, 7.8556e-02, 6.0177e-03],\n",
              "          [9.1543e-01, 7.8556e-02, 6.0177e-03],\n",
              "          [9.1543e-01, 7.8556e-02, 6.0177e-03],\n",
              "          [2.6287e-02, 2.6433e-01, 7.0939e-01],\n",
              "          [5.2074e-02, 3.2456e-02, 9.1547e-01],\n",
              "          [8.2484e-01, 1.6924e-01, 5.9142e-03],\n",
              "          [6.0025e-03, 5.6846e-02, 9.3715e-01],\n",
              "          [5.9411e-03, 5.4882e-02, 9.3918e-01],\n",
              "          [2.6724e-02, 2.5152e-01, 7.2176e-01],\n",
              "          [9.1543e-01, 7.8556e-02, 6.0177e-03],\n",
              "          [8.3449e-01, 1.5960e-01, 5.9129e-03],\n",
              "          [3.4918e-01, 6.4196e-01, 8.8513e-03],\n",
              "          [5.2074e-02, 3.2456e-02, 9.1547e-01],\n",
              "          [9.1543e-01, 7.8556e-02, 6.0177e-03],\n",
              "          [2.4450e-02, 8.3715e-02, 8.9184e-01],\n",
              "          [8.7772e-03, 4.3898e-02, 9.4732e-01],\n",
              "          [7.3555e-01, 2.2533e-01, 3.9123e-02],\n",
              "          [3.4918e-01, 6.4196e-01, 8.8513e-03],\n",
              "          [8.4390e-01, 1.5020e-01, 5.9046e-03],\n",
              "          [1.3485e-02, 8.9988e-02, 8.9653e-01],\n",
              "          [2.2395e-02, 8.5891e-02, 8.9171e-01],\n",
              "          [1.4562e-05, 9.9993e-01, 5.0686e-05],\n",
              "          [1.3523e-02, 9.4762e-02, 8.9172e-01],\n",
              "          [2.2552e-02, 8.2848e-02, 8.9460e-01],\n",
              "          [8.6093e-01, 1.3315e-01, 5.9194e-03],\n",
              "          [8.5263e-01, 1.4147e-01, 5.8970e-03],\n",
              "          [1.7192e-02, 8.5829e-03, 9.7422e-01],\n",
              "          [3.3323e-02, 2.3934e-02, 9.4274e-01],\n",
              "          [1.7748e-02, 8.9931e-03, 9.7326e-01],\n",
              "          [3.2861e-02, 2.3395e-02, 9.4374e-01]]),\n",
              "  tensor([[1.7410e-01, 7.8592e-01, 3.9975e-02],\n",
              "          [8.0323e-02, 9.1817e-01, 1.5079e-03],\n",
              "          [1.7410e-01, 7.8592e-01, 3.9975e-02],\n",
              "          [3.5987e-02, 7.7578e-01, 1.8823e-01],\n",
              "          [9.8888e-02, 8.9904e-01, 2.0742e-03],\n",
              "          [6.2051e-01, 3.7219e-01, 7.3038e-03],\n",
              "          [1.2426e-02, 8.0151e-03, 9.7956e-01],\n",
              "          [8.5186e-01, 1.0431e-01, 4.3827e-02],\n",
              "          [6.4125e-01, 3.5161e-01, 7.1366e-03],\n",
              "          [8.0051e-02, 9.1849e-01, 1.4629e-03],\n",
              "          [1.2426e-02, 8.0151e-03, 9.7956e-01],\n",
              "          [8.4372e-01, 1.0718e-01, 4.9104e-02],\n",
              "          [2.2875e-01, 7.5372e-01, 1.7530e-02],\n",
              "          [2.2594e-01, 7.5778e-01, 1.6271e-02],\n",
              "          [9.8888e-02, 8.9904e-01, 2.0742e-03],\n",
              "          [3.6422e-02, 7.7967e-01, 1.8391e-01],\n",
              "          [8.3527e-02, 9.0809e-01, 8.3785e-03],\n",
              "          [1.0584e-01, 8.9189e-01, 2.2681e-03],\n",
              "          [9.0911e-02, 9.0727e-01, 1.8157e-03],\n",
              "          [9.0911e-02, 9.0727e-01, 1.8157e-03],\n",
              "          [9.0911e-02, 9.0727e-01, 1.8157e-03],\n",
              "          [9.0911e-02, 9.0727e-01, 1.8157e-03],\n",
              "          [1.0490e-02, 5.2684e-03, 9.8424e-01],\n",
              "          [9.4306e-02, 9.0214e-01, 3.5533e-03],\n",
              "          [1.3985e-02, 1.1998e-02, 9.7402e-01],\n",
              "          [1.0025e-01, 8.9833e-01, 1.4196e-03],\n",
              "          [2.5628e-02, 3.5589e-01, 6.1849e-01],\n",
              "          [1.4599e-02, 1.3844e-02, 9.7156e-01],\n",
              "          [1.8230e-02, 1.6185e-02, 9.6559e-01],\n",
              "          [1.3525e-02, 1.0721e-02, 9.7575e-01],\n",
              "          [1.3730e-02, 1.1276e-02, 9.7499e-01],\n",
              "          [3.0623e-02, 7.4261e-01, 2.2677e-01],\n",
              "          [3.0623e-02, 7.4261e-01, 2.2677e-01],\n",
              "          [3.0623e-02, 7.4261e-01, 2.2677e-01],\n",
              "          [1.7908e-02, 1.5175e-02, 9.6692e-01],\n",
              "          [1.7654e-02, 1.4436e-02, 9.6791e-01],\n",
              "          [3.0623e-02, 7.4261e-01, 2.2677e-01],\n",
              "          [3.0623e-02, 7.4261e-01, 2.2677e-01],\n",
              "          [3.0623e-02, 7.4261e-01, 2.2677e-01],\n",
              "          [1.3340e-02, 1.0234e-02, 9.7643e-01],\n",
              "          [9.0911e-02, 9.0727e-01, 1.8157e-03],\n",
              "          [1.9065e-02, 1.8894e-02, 9.6204e-01],\n",
              "          [9.4131e-02, 9.0228e-01, 3.5922e-03],\n",
              "          [8.3527e-02, 9.0809e-01, 8.3785e-03],\n",
              "          [1.0395e-02, 4.9032e-03, 9.8470e-01],\n",
              "          [8.3527e-02, 9.0809e-01, 8.3785e-03],\n",
              "          [2.2903e-02, 1.2569e-01, 8.5141e-01],\n",
              "          [2.5413e-02, 3.4771e-01, 6.2687e-01],\n",
              "          [1.7356e-02, 1.3628e-02, 9.6902e-01],\n",
              "          [9.0911e-02, 9.0727e-01, 1.8157e-03],\n",
              "          [8.3527e-02, 9.0809e-01, 8.3785e-03],\n",
              "          [8.3527e-02, 9.0809e-01, 8.3785e-03],\n",
              "          [8.3527e-02, 9.0809e-01, 8.3785e-03],\n",
              "          [1.0537e-02, 5.3325e-03, 9.8413e-01],\n",
              "          [1.4274e-02, 1.2846e-02, 9.7288e-01],\n",
              "          [1.0375e-02, 4.8847e-03, 9.8474e-01],\n",
              "          [1.0190e-02, 4.8227e-03, 9.8499e-01],\n",
              "          [1.0358e-02, 4.8679e-03, 9.8477e-01],\n",
              "          [1.0415e-02, 4.9234e-03, 9.8466e-01],\n",
              "          [2.1296e-02, 1.5475e-01, 8.2396e-01],\n",
              "          [8.3922e-01, 1.4642e-01, 1.4355e-02],\n",
              "          [1.0162e-02, 4.7812e-03, 9.8506e-01],\n",
              "          [2.1296e-02, 1.5475e-01, 8.2396e-01],\n",
              "          [2.1296e-02, 1.5475e-01, 8.2396e-01],\n",
              "          [1.0152e-02, 4.7696e-03, 9.8508e-01],\n",
              "          [2.1296e-02, 1.5475e-01, 8.2396e-01],\n",
              "          [2.1296e-02, 1.5475e-01, 8.2396e-01],\n",
              "          [2.1296e-02, 1.5475e-01, 8.2396e-01],\n",
              "          [2.2903e-02, 1.2569e-01, 8.5141e-01],\n",
              "          [1.8637e-02, 1.7464e-02, 9.6390e-01],\n",
              "          [3.8651e-02, 2.2554e-01, 7.3581e-01],\n",
              "          [3.8651e-02, 2.2554e-01, 7.3581e-01],\n",
              "          [2.4451e-01, 7.0178e-01, 5.3710e-02],\n",
              "          [1.1480e-02, 6.7165e-03, 9.8180e-01],\n",
              "          [7.0407e-02, 9.2884e-01, 7.4969e-04],\n",
              "          [2.5559e-02, 4.6177e-02, 9.2826e-01],\n",
              "          [2.5559e-02, 4.6177e-02, 9.2826e-01],\n",
              "          [1.0331e-02, 4.8417e-03, 9.8483e-01],\n",
              "          [1.0331e-02, 4.8417e-03, 9.8483e-01],\n",
              "          [6.0170e-01, 3.6563e-01, 3.2668e-02],\n",
              "          [1.1125e-02, 6.0485e-03, 9.8283e-01],\n",
              "          [1.9470e-02, 2.6189e-02, 9.5434e-01],\n",
              "          [2.4451e-01, 7.0178e-01, 5.3710e-02],\n",
              "          [1.0314e-02, 4.9439e-03, 9.8474e-01],\n",
              "          [1.0151e-02, 4.7633e-03, 9.8509e-01],\n",
              "          [1.0166e-02, 4.7813e-03, 9.8505e-01],\n",
              "          [1.1520e-02, 6.7829e-03, 9.8170e-01],\n",
              "          [1.0314e-02, 4.9439e-03, 9.8474e-01],\n",
              "          [6.0170e-01, 3.6563e-01, 3.2668e-02],\n",
              "          [8.5312e-01, 1.4103e-01, 5.8473e-03],\n",
              "          [2.4451e-01, 7.0178e-01, 5.3710e-02],\n",
              "          [2.4451e-01, 7.0178e-01, 5.3710e-02],\n",
              "          [1.6865e-02, 2.1034e-02, 9.6210e-01],\n",
              "          [2.4451e-01, 7.0178e-01, 5.3710e-02],\n",
              "          [8.4182e-01, 1.5457e-01, 3.6113e-03],\n",
              "          [1.6865e-02, 2.1034e-02, 9.6210e-01],\n",
              "          [1.9566e-02, 2.5601e-02, 9.5483e-01],\n",
              "          [8.5670e-01, 1.4008e-01, 3.2244e-03],\n",
              "          [6.0170e-01, 3.6563e-01, 3.2668e-02],\n",
              "          [1.1238e-02, 6.1084e-03, 9.8265e-01],\n",
              "          [9.3496e-01, 6.3324e-02, 1.7135e-03],\n",
              "          [9.3496e-01, 6.3324e-02, 1.7135e-03],\n",
              "          [9.3496e-01, 6.3324e-02, 1.7135e-03],\n",
              "          [2.9414e-02, 5.1714e-01, 4.5345e-01],\n",
              "          [7.4076e-01, 2.2774e-01, 3.1494e-02],\n",
              "          [8.8335e-01, 1.1294e-01, 3.7108e-03],\n",
              "          [1.8428e-02, 5.1097e-02, 9.3047e-01],\n",
              "          [1.8482e-02, 5.2242e-02, 9.2928e-01],\n",
              "          [2.8747e-02, 4.8206e-01, 4.8920e-01],\n",
              "          [9.3496e-01, 6.3324e-02, 1.7135e-03],\n",
              "          [8.8809e-01, 1.0847e-01, 3.4370e-03],\n",
              "          [6.0170e-01, 3.6563e-01, 3.2668e-02],\n",
              "          [7.4076e-01, 2.2774e-01, 3.1494e-02],\n",
              "          [9.3496e-01, 6.3324e-02, 1.7135e-03],\n",
              "          [1.1238e-02, 6.1084e-03, 9.8265e-01],\n",
              "          [1.1139e-02, 6.0679e-03, 9.8279e-01],\n",
              "          [8.6302e-01, 1.3199e-01, 4.9930e-03],\n",
              "          [6.0170e-01, 3.6563e-01, 3.2668e-02],\n",
              "          [8.9270e-01, 1.0412e-01, 3.1871e-03],\n",
              "          [3.1317e-02, 5.3106e-01, 4.3762e-01],\n",
              "          [1.4825e-02, 1.3148e-02, 9.7203e-01],\n",
              "          [7.0407e-02, 9.2884e-01, 7.4969e-04],\n",
              "          [3.1605e-02, 5.4098e-01, 4.2742e-01],\n",
              "          [1.4736e-02, 1.2792e-02, 9.7247e-01],\n",
              "          [9.0088e-01, 9.6320e-02, 2.7958e-03],\n",
              "          [8.9701e-01, 1.0001e-01, 2.9721e-03],\n",
              "          [1.0761e-02, 5.0546e-03, 9.8418e-01],\n",
              "          [1.2855e-02, 8.3781e-03, 9.7877e-01],\n",
              "          [1.0873e-02, 5.1128e-03, 9.8401e-01],\n",
              "          [1.2948e-02, 8.6026e-03, 9.7845e-01]])))"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#export\n",
        "class Fastai_Multimodal_Classifier():\n",
        "  \"\"\"end to end fastai classifier for multimodal data which includes txt_cols, img_cols, cnt_cols, cat_cols\"\"\"\n",
        "  def __init__(self, txt_clfs=None, lms=None, tab_clf=None, img_clfs=None, ensembled_clf_embs=None, ensembled_clf_probs=None, model_path='/content/drive/My Drive/fastai_multimodal/model/' ):\n",
        "    self.txt_clfs = txt_clfs # a list of fastai text classifiers\n",
        "    self.lms = lms # a list of fastai text language models\n",
        "    self.tab_clf = tab_clf # a fastai tabular classifier\n",
        "    self.img_clfs = img_clfs # a list of fastai image classifiers\n",
        "    self.ensembled_clf_embs = ensembled_clf_embs # a ensembled classifier trained on embs_ls (at this point, using fastai tabular)\n",
        "    self.ensembled_clf_probs = ensembled_clf_probs # a ensembled classifier trained on probs_ls (at this point, using fastai tabular)\n",
        "    self.model_path=model_path\n",
        "\n",
        "  \"\"\"def identify_cnt_cat_txt_img(self, df, label_col, cnt_card=0.5, txt_card=0.5):\n",
        "    txt_cols, cnt_cols, cat_cols, (int_cat_cols, str_cat_cols), img_cols = cnt_cat_txt_img_split(df, cnt_card=cnt_card, excluded_cols = [label_col], txt_card=txt_card)\n",
        "    self.txt_cols=txt_cols\n",
        "    self.cnt_cols=cnt_cols\n",
        "    self.cat_cols=cat_cols\n",
        "    self.img_cols=img_cols\"\"\"\n",
        "  \n",
        "  def fit(self, df:pd.DataFrame, label_col:str, txt_cols:list=None, img_cols:list=None, cnt_cols:list=None, cat_cols:list=None, img_path:str=img_path):\n",
        "    \"\"\" In case not provided,\n",
        "    fit multiple fastai text classifiers for each col in txt_cols;\n",
        "    fit multiple fastai img classifiers for each col in img_cols;\n",
        "    fit one fastai tabular classifier for all cnt_cols and cat_cols.\n",
        "    Args:\n",
        "      df:pd.DataFrame, containing both label_col and txt_cols \n",
        "      label_col:str, e.g. 'label'\n",
        "      txt_cols:list, e.g. ['title_raw',\t'hard_skills_name',\t'title_raw+hard_skills_name']\n",
        "      img_cols:list, \n",
        "      cnt_cols:list, \n",
        "      cat_cols:list\n",
        "      img_path:str\n",
        "    Returns:\n",
        "      None (but update self.txt_clfs, self.lms, self.tab_clf)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # in case not provided by user, automatically identify various feature types\n",
        "    if all(x is None for x in  (txt_cols, img_cols, cnt_cols, cat_cols)): \n",
        "        txt_cols, cnt_cols, cat_cols, (int_cat_cols, str_cat_cols), img_cols = cnt_cat_txt_img_split(df, cnt_card=.5, excluded_cols = [label_col], txt_card=0.5)\n",
        "        print(f'** Given label_col={label_col} ** \\n======= automatically identify\\n cnt_cols={cnt_cols}\\n cat_cols={cat_cols},\\n img_cols={img_cols}, \\n txt_cols={txt_cols} \\n======= make sure that is what you expect; otherwise, manually make changes')\n",
        "    else: # store columns info in unchangable container\n",
        "        class CONST(object):\n",
        "            __slots__ = ()\n",
        "            cat_cols = cat_cols\n",
        "            cnt_cols = cnt_cols\n",
        "            txt_cols = txt_cols\n",
        "            img_cols = img_cols\n",
        "            label_col = label_col\n",
        "            df = df\n",
        "        c = CONST()\n",
        "        self.c = c\n",
        "\n",
        "    # convert dtype from object to str & add bin cnt columns to df; also fill missing value\n",
        "    for col in df.columns:\n",
        "      if df[[col]].dtypes[col]==np.dtype('O'):\n",
        "          df[col] = df[col].astype('str')\n",
        "          df.loc[:,col]=df[[col]].fillna('NA')\n",
        "      if col in cat_cols: # force cat_col to be str type, whether the original one is str or int\n",
        "          df[col] = df[col].astype('str')\n",
        "          df.loc[:,col]=df[[col]].fillna('NA')\n",
        "      df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "    \n",
        "    # since tf.one_hot() does not work withs strings label but integer, convert df[label_col] into integer value\n",
        "    label_str2num_map = {x:n for (n,x) in enumerate(sorted(df[label_col].unique()))}\n",
        "    \n",
        "    # store the cols/features type info\n",
        "    self.txt_cols=c.txt_cols\n",
        "    self.cnt_cols=c.cnt_cols\n",
        "    self.cat_cols=c.cat_cols\n",
        "    self.img_cols=c.img_cols\n",
        "    self.label_col=c.label_col\n",
        "    self.df=df\n",
        "    self.img_path=img_path\n",
        "\n",
        "    # in case self.tab_clf = None, make tab_clf from stratch\n",
        "    if self.tab_clf is None and len(c.cnt_cols+c.cat_cols)>0:\n",
        "        print(f'========================= training classifier with \\n cnt_cols={cnt_cols} and \\n cat_cols={cat_cols}==============================')\n",
        "        tab_clf = train_fastai_tabular_classifier(c.df, \n",
        "                                        cnt_cols=c.cnt_cols,\n",
        "                                        cat_cols=c.cat_cols,\n",
        "                                        label_col=c.label_col,\n",
        "                                        model_path=self.model_path,\n",
        "                                        lr=0.005\n",
        "                                        )\n",
        "\n",
        "        self.tab_clf = tab_clf\n",
        "    else:\n",
        "        self.tab_clf = None\n",
        "\n",
        "    # in case self.txt_clfs = None, train txt_clfs from stratch\n",
        "    if self.txt_clfs is None and len(c.txt_cols)>0:\n",
        "      txt_clfs = []\n",
        "      lms = []\n",
        "      for txt_col in c.txt_cols:\n",
        "          print(f'========================= training classifier with txt_col={txt_col}==============================')\n",
        "          lm, txt_clf = train_fastai_text_classifier(self.df, \n",
        "                                        txt_col=txt_col,\n",
        "                                        label_col=self.label_col,\n",
        "                                        model_path=self.model_path,\n",
        "                                        lr=0.005,\n",
        "                                        )\n",
        "          txt_clfs.append(txt_clf)\n",
        "          lms.append(lm)\n",
        "      self.txt_clfs = txt_clfs\n",
        "      self.lms = lms\n",
        "\n",
        "\n",
        "    # in case self.img_clfs = None, train img_clfs from stratch\n",
        "    if self.img_clfs is None and len(c.img_cols)>0:\n",
        "      img_clfs = []\n",
        "\n",
        "      for img_col in c.img_cols:\n",
        "          print(f'========================= training classifier with img_col={img_col}==============================')\n",
        "          img_clf = train_fastai_image_classifier(self.df, \n",
        "                                        img_col=img_col,\n",
        "                                        label_col=c.label_col,\n",
        "                                        model_path=self.model_path,\n",
        "                                        img_path=self.img_path,\n",
        "                                        model_name=img_col+'_clf',\n",
        "                                        lr=0.005\n",
        "                                        )\n",
        "          img_clfs.append(img_clf)\n",
        "\n",
        "      self.img_clfs = img_clfs\n",
        "\n",
        "\n",
        "\n",
        "  def get_preds(self, test_df:pd.DataFrame, flag_load_embs_probs=False):\n",
        "    \"\"\"get predictions for test data df[txt_cols + img_cols + cnt_cols + cat_cols]\n",
        "    Args:\n",
        "      test_df:pd.DataFrame, containing txt_cols + img_cols + cnt_cols + cat_cols\n",
        "      txt_cols:list, img_cols:list, cnt_cols:list, cat_cols:list are features splitted into 4 categories: txt, img, cnt, cat\n",
        "    Returns:\n",
        "      (preds0, preds1), (probs0, probs1), where\n",
        "      - preds0, probs0: the predictions on test data based on txt_clfs, img_clfs, tab_clf embeddings extraction `embs_ls`\n",
        "      - preds1, probs1: the prediction on test data based on txt_clfs, img_clfs, tab_clf output probs_ls\n",
        "    \n",
        "    \"\"\"\n",
        "    df = test_df.copy()\n",
        "    # convert dtype from object to str & add bin cnt columns to df; also fill missing value\n",
        "    for col in df.columns:\n",
        "      if df[[col]].dtypes[col]==np.dtype('O'):\n",
        "          df[col] = df[col].astype('str')\n",
        "          df.loc[:,col]=df[[col]].fillna('NA')\n",
        "      if col in cat_cols: # force cat_col to be str type, whether the original one is str or int\n",
        "          df[col] = df[col].astype('str')\n",
        "          df.loc[:,col]=df[[col]].fillna('NA')\n",
        "      df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "    #################################\n",
        "    ### middle concat: use embs_ls\n",
        "    #################################\n",
        "    # get docs embeddings list\n",
        "    if flag_load_embs_probs:\n",
        "      import pickle\n",
        "      with open(\"embs_ls.pickle\",\"rb\") as f:\n",
        "        embs_ls = pickle.load(f)\n",
        "    else:\n",
        "        # init embs_ls\n",
        "        embs_ls = []\n",
        "\n",
        "        # get tab embeddings list\n",
        "        print(f'========================= extracting  tabular embeddings==============================')\n",
        "        if self.tab_clf is not None and len(self.cnt_cols +self.cat_cols)>0:\n",
        "            embs = get_fastai_tab_embs(tab_clf=self.tab_clf, df=df, cnt_cols=self.cnt_cols, cat_cols=self.cat_cols)\n",
        "            embs_ls.append(embs)\n",
        "\n",
        "        # get docs embeddings list\n",
        "        if len(self.txt_cols)>0:\n",
        "            for (txt_col, txt_clf, lm) in zip(self.txt_cols, self.txt_clfs, self.lms):\n",
        "                print(f'========================= extracting  txt_col={txt_col} doc embeddings==============================')\n",
        "\n",
        "                embs = get_fastai_docs_embs(docs=df[txt_col], learn=txt_clf, lm=lm, df=None, txt_col=None)\n",
        "                embs_ls.append(embs)\n",
        "\n",
        "        # get imgs embeddings list\n",
        "        if len(self.img_cols)>0:\n",
        "            for (img_col, img_clf) in zip(self.img_cols, self.img_clfs):\n",
        "                print(f'========================= extracting  img_col={img_col} img embeddings==============================')\n",
        "                embs = get_fastai_imgs_embs(img_clf=img_clf, df=df, img_col=img_col)\n",
        "                embs_ls.append(embs)\n",
        "        \n",
        "\n",
        "        \n",
        "        # store embs_ls\n",
        "        self.embs_ls = embs_ls\n",
        "        import pickle\n",
        "        with open(\"embs_ls.pickle\",\"wb\") as f:\n",
        "            pickle.dump(embs_ls,f)\n",
        "\n",
        "    # train a ensemble classifier using all embeddings\n",
        "    ensembled_clf_embs = train_ensembled_classifier(embs_ls, df=df, label_col=label_col)  \n",
        "    self.ensembled_clf_embs = ensembled_clf_embs\n",
        "    \n",
        "\n",
        "    #################################\n",
        "    ### late concat: use probs_ls\n",
        "    #################################\n",
        "    # get prediction on labels and probabilities\n",
        "    if flag_load_embs_probs:\n",
        "      import pickle\n",
        "      with open(\"probs_ls.pickle\",\"rb\") as f:\n",
        "        embs_ls = pickle.load(f)\n",
        "    else:\n",
        "        # init probs_ls\n",
        "        probs_ls = []\n",
        "\n",
        "        # probs by tab_clf\n",
        "        if self.tab_clf is not None and len(self.c.cnt_cols + self.c.cat_cols)>0:\n",
        "            print(f'========================= calculating tab_cols ({cnt_cols, cat_cols}) probs==============================')\n",
        "            tab_cols_ = [col for col in self.cnt_cols + self.cat_cols if col.split('_')[-1]!='na'] # tmp: remove added columns look like '*_na'\n",
        "            _, probs = fastai_learner_preds(learner=self.tab_clf, df=df[tab_cols_], label_col=self.label_col)\n",
        "            probs_ls.append(probs.numpy())\n",
        "\n",
        "        # probs by txt_clfs\n",
        "        if len(self.img_cols)>0:\n",
        "            for (txt_clf, txt_col) in zip(self.txt_clfs, self.txt_cols):\n",
        "                print(f'========================= calculating  txt_col={txt_col} probs ==============================')\n",
        "                _, probs = fastai_learner_preds(learner=txt_clf, df=df, label_col=self.label_col, txt_col=txt_col)\n",
        "                probs_ls.append(probs.numpy())\n",
        "\n",
        "        # probs by img_clfs\n",
        "        if len(self.img_cols)>0:\n",
        "            for (img_clf, img_col) in zip(self.img_clfs, self.img_cols):\n",
        "                print(f'========================= calculating  img_col={img_col} probs==============================')\n",
        "\n",
        "                _, probs = fastai_learner_preds(learner=img_clf, df=df[[img_col]],label_col=self.label_col)\n",
        "                probs_ls.append(probs.numpy())\n",
        "\n",
        "        # store probs_ls\n",
        "        self.probs_ls = probs_ls\n",
        "\n",
        "    # train an ensembled classifier using all probs\n",
        "    ensembled_clf_probs = train_ensembled_classifier(probs_ls, df=df, label_col=label_col)  \n",
        "    self.ensembled_clf_probs = ensembled_clf_probs\n",
        "\n",
        "    # in case self.ensembled_clf_embs = None, train ensembled_clfs from stratch\n",
        "    if self.ensembled_clf_embs is None:\n",
        "      print(f'========================= training ensembled classifier with embs_ls ==============================')\n",
        "      ensembled_clf_embs =train_ensembled_classifier(embs_ls, \n",
        "                                               lr=0.005, \n",
        "                                               max_epochs=100, \n",
        "                                               model_path=self.model_path,  \n",
        "                                               model_name='ensembled_model_embs', \n",
        "                                               n_components=1, \n",
        "                                               df=df, \n",
        "                                               label_col=label_col)\n",
        "\n",
        "      self.ensembled_clf_embs = ensembled_clf_embs\n",
        "    \n",
        "    # in case self.ensembled_clf_probs = None, train ensembled_clfs from stratch\n",
        "    if self.ensembled_clf_probs is None:\n",
        "      print(f'========================= training ensembled classifier with probs_ls ==============================')\n",
        "      ensembled_clf_embs =train_ensembled_classifier(probs_ls, \n",
        "                                               lr=0.005, \n",
        "                                               max_epochs=100, \n",
        "                                               model_path=self.model_path,  \n",
        "                                               model_name='ensembled_model_probs', \n",
        "                                               n_components=1, \n",
        "                                               df=df, \n",
        "                                               label_col=label_col)\n",
        "\n",
        "      self.ensembled_clf_embs = ensembled_clf_embs\n",
        "\n",
        "    # ensembled_clf_embs make predictions on embs_ls\n",
        "    df_ = pd.concat([pd.DataFrame(embs) for embs in self.embs_ls], axis=1)\n",
        "    df_.columns = list(range(df_.shape[1]))\n",
        "    test_dl = self.ensembled_clf_embs.dls.test_dl(df_) #, with_labels=True\n",
        "\n",
        "    \n",
        "    probs0, _ = self.ensembled_clf_embs.get_preds(dl=test_dl)\n",
        "    preds0 = probs0.numpy().argmax(axis=1)\n",
        "\n",
        "    # ensembled_clf_probs make predictions on probs_ls\n",
        "    df_ = pd.concat([pd.DataFrame(probs) for probs in self.probs_ls], axis=1)\n",
        "    df_.columns = list(range(df_.shape[1]))\n",
        "\n",
        "    test_dl = self.ensembled_clf_probs.dls.test_dl(df_) #, with_labels=True\n",
        "    probs1, _ = self.ensembled_clf_probs.get_preds(dl=test_dl)\n",
        "    preds1 = probs0.numpy().argmax(axis=1)\n",
        "    \n",
        "    return (preds0, preds1), (probs0, probs1)\n",
        "\n",
        "multimodal_clf = Fastai_Multimodal_Classifier()\n",
        "multimodal_clf.fit(df=train_df, label_col=label_col, cnt_cols=cnt_cols, cat_cols=cat_cols,txt_cols=txt_cols,img_cols=img_cols)\n",
        "(preds0, preds1), (probs0, probs1) = multimodal_clf.get_preds(test_df)\n",
        "(preds0, preds1), (probs0, probs1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QekDbPG0uYf-",
        "outputId": "18f02899-3778-45e1-c811-a2d6619998d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{-1: 0, 0: 1, 1: 2}"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# dls.vocab.o2i  to acess label names from predictions\n",
        "\n",
        "dls = multimodal_clf.tab_clf.dls\n",
        "label_name2num = {k: dls.vocab.o2i[k] for k in list(dls.vocab.o2i)}\n",
        "label_name2num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "keTd3QCDOJiS",
        "outputId": "e95704b9-a888-4799-8bba-e8fa383d0182"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  import sys\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-14f7aec4-cebc-4d42-a2d9-0d4f79cbaf8b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y_true</th>\n",
              "      <th>y_pred</th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>attendee_email</th>\n",
              "      <th>length_of_service</th>\n",
              "      <th>response_status</th>\n",
              "      <th>is_organizer</th>\n",
              "      <th>meeting_lapse</th>\n",
              "      <th>time_since_last_promotion</th>\n",
              "      <th>...</th>\n",
              "      <th>age</th>\n",
              "      <th>job_family</th>\n",
              "      <th>is_optional</th>\n",
              "      <th>start_datetime</th>\n",
              "      <th>num_direct_reports</th>\n",
              "      <th>event_id</th>\n",
              "      <th>manager_length_of_service</th>\n",
              "      <th>start_datetime_Dayofyear</th>\n",
              "      <th>time_since_new_manager_start_date</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>RS Huddle</td>\n",
              "      <td>Hi All-\\nWe would like to meet briefly and discuss the New Year plan to refresh ourselves on who is working on what (ie staff pooled or specific work stream) as well as how work will be distributed throughout winterfest. Please update your WSO and plan to attend. Until then, happy holidays and have a safe New Year!\\n\\nThank you,\\nIMT Team</td>\n",
              "      <td>wellsc12@gene.com</td>\n",
              "      <td>2.51096</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>38.0</td>\n",
              "      <td>Pricing / Reimbursement / Contracting</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2022-01-03 09:00:00</td>\n",
              "      <td>8.0</td>\n",
              "      <td>60695664</td>\n",
              "      <td>21.2160</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.586</td>\n",
              "      <td>1.641229e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>RS Huddle</td>\n",
              "      <td>Hi All-\\nWe would like to meet briefly and discuss the New Year plan to refresh ourselves on who is working on what (ie staff pooled or specific work stream) as well as how work will be distributed throughout winterfest. Please update your WSO and plan to attend. Until then, happy holidays and have a safe New Year!\\n\\nThank you,\\nIMT Team</td>\n",
              "      <td>jtorres3@gene.com</td>\n",
              "      <td>21.31850</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>42.0</td>\n",
              "      <td>Pricing / Reimbursement / Contracting</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2022-01-03 09:00:00</td>\n",
              "      <td>6.0</td>\n",
              "      <td>60729076</td>\n",
              "      <td>18.4797</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.748</td>\n",
              "      <td>1.641229e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>432</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>RS Huddle</td>\n",
              "      <td>Hi All-\\nWe would like to meet briefly and discuss the New Year plan to refresh ourselves on who is working on what (ie staff pooled or specific work stream) as well as how work will be distributed throughout winterfest. Please update your WSO and plan to attend. Until then, happy holidays and have a safe New Year!\\n\\nThank you,\\nIMT Team</td>\n",
              "      <td>jtorres3@gene.com</td>\n",
              "      <td>21.31850</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>42.0</td>\n",
              "      <td>Pricing / Reimbursement / Contracting</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2022-01-03 09:00:00</td>\n",
              "      <td>6.0</td>\n",
              "      <td>60727560</td>\n",
              "      <td>18.4797</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.748</td>\n",
              "      <td>1.641229e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>RS Huddle</td>\n",
              "      <td>Hi All-\\nWe would like to meet briefly and discuss the New Year plan to refresh ourselves on who is working on what (ie staff pooled or specific work stream) as well as how work will be distributed throughout winterfest. Please update your WSO and plan to attend. Until then, happy holidays and have a safe New Year!\\n\\nThank you,\\nIMT Team</td>\n",
              "      <td>jtorres3@gene.com</td>\n",
              "      <td>21.31850</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>42.0</td>\n",
              "      <td>Pricing / Reimbursement / Contracting</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2022-01-03 09:00:00</td>\n",
              "      <td>6.0</td>\n",
              "      <td>60591632</td>\n",
              "      <td>18.4797</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.748</td>\n",
              "      <td>1.641229e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>RS Huddle</td>\n",
              "      <td>Hi All-\\nWe would like to meet briefly and discuss the New Year plan to refresh ourselves on who is working on what (ie staff pooled or specific work stream) as well as how work will be distributed throughout winterfest. Please update your WSO and plan to attend. Until then, happy holidays and have a safe New Year!\\n\\nThank you,\\nIMT Team</td>\n",
              "      <td>jtorres3@gene.com</td>\n",
              "      <td>21.31850</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>42.0</td>\n",
              "      <td>Pricing / Reimbursement / Contracting</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2022-01-03 09:00:00</td>\n",
              "      <td>6.0</td>\n",
              "      <td>60695664</td>\n",
              "      <td>18.4797</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.748</td>\n",
              "      <td>1.641229e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>RS Huddle</td>\n",
              "      <td>Hi All-\\nWe would like to meet briefly and discuss the New Year plan to refresh ourselves on who is working on what (ie staff pooled or specific work stream) as well as how work will be distributed throughout winterfest. Please update your WSO and plan to attend. Until then, happy holidays and have a safe New Year!\\n\\nThank you,\\nIMT Team</td>\n",
              "      <td>jtorres3@gene.com</td>\n",
              "      <td>21.31850</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>42.0</td>\n",
              "      <td>Pricing / Reimbursement / Contracting</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2022-01-03 09:00:00</td>\n",
              "      <td>6.0</td>\n",
              "      <td>60563909</td>\n",
              "      <td>18.4797</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.748</td>\n",
              "      <td>1.641229e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>RS Huddle</td>\n",
              "      <td>Hi All-\\nWe would like to meet briefly and discuss the New Year plan to refresh ourselves on who is working on what (ie staff pooled or specific work stream) as well as how work will be distributed throughout winterfest. Please update your WSO and plan to attend. Until then, happy holidays and have a safe New Year!\\n\\nThank you,\\nIMT Team</td>\n",
              "      <td>jtorres3@gene.com</td>\n",
              "      <td>21.31850</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>42.0</td>\n",
              "      <td>Pricing / Reimbursement / Contracting</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2022-01-03 09:00:00</td>\n",
              "      <td>6.0</td>\n",
              "      <td>60593137</td>\n",
              "      <td>18.4797</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.748</td>\n",
              "      <td>1.641229e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>RS Huddle</td>\n",
              "      <td>Hi All-\\nWe would like to meet briefly and discuss the New Year plan to refresh ourselves on who is working on what (ie staff pooled or specific work stream) as well as how work will be distributed throughout winterfest. Please update your WSO and plan to attend. Until then, happy holidays and have a safe New Year!\\n\\nThank you,\\nIMT Team</td>\n",
              "      <td>wellsc12@gene.com</td>\n",
              "      <td>2.51096</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>38.0</td>\n",
              "      <td>Pricing / Reimbursement / Contracting</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2022-01-03 09:00:00</td>\n",
              "      <td>8.0</td>\n",
              "      <td>60563909</td>\n",
              "      <td>21.2160</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.586</td>\n",
              "      <td>1.641229e+09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-14f7aec4-cebc-4d42-a2d9-0d4f79cbaf8b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-14f7aec4-cebc-4d42-a2d9-0d4f79cbaf8b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-14f7aec4-cebc-4d42-a2d9-0d4f79cbaf8b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     y_true  y_pred       title  \\\n",
              "416       2       1  RS Huddle    \n",
              "429       2       1  RS Huddle    \n",
              "432       2       1  RS Huddle    \n",
              "433       2       1  RS Huddle    \n",
              "435       2       1  RS Huddle    \n",
              "436       2       1  RS Huddle    \n",
              "437       2       1  RS Huddle    \n",
              "438       2       1  RS Huddle    \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                              description  \\\n",
              "416  Hi All-\\nWe would like to meet briefly and discuss the New Year plan to refresh ourselves on who is working on what (ie staff pooled or specific work stream) as well as how work will be distributed throughout winterfest. Please update your WSO and plan to attend. Until then, happy holidays and have a safe New Year!\\n\\nThank you,\\nIMT Team   \n",
              "429  Hi All-\\nWe would like to meet briefly and discuss the New Year plan to refresh ourselves on who is working on what (ie staff pooled or specific work stream) as well as how work will be distributed throughout winterfest. Please update your WSO and plan to attend. Until then, happy holidays and have a safe New Year!\\n\\nThank you,\\nIMT Team   \n",
              "432  Hi All-\\nWe would like to meet briefly and discuss the New Year plan to refresh ourselves on who is working on what (ie staff pooled or specific work stream) as well as how work will be distributed throughout winterfest. Please update your WSO and plan to attend. Until then, happy holidays and have a safe New Year!\\n\\nThank you,\\nIMT Team   \n",
              "433  Hi All-\\nWe would like to meet briefly and discuss the New Year plan to refresh ourselves on who is working on what (ie staff pooled or specific work stream) as well as how work will be distributed throughout winterfest. Please update your WSO and plan to attend. Until then, happy holidays and have a safe New Year!\\n\\nThank you,\\nIMT Team   \n",
              "435  Hi All-\\nWe would like to meet briefly and discuss the New Year plan to refresh ourselves on who is working on what (ie staff pooled or specific work stream) as well as how work will be distributed throughout winterfest. Please update your WSO and plan to attend. Until then, happy holidays and have a safe New Year!\\n\\nThank you,\\nIMT Team   \n",
              "436  Hi All-\\nWe would like to meet briefly and discuss the New Year plan to refresh ourselves on who is working on what (ie staff pooled or specific work stream) as well as how work will be distributed throughout winterfest. Please update your WSO and plan to attend. Until then, happy holidays and have a safe New Year!\\n\\nThank you,\\nIMT Team   \n",
              "437  Hi All-\\nWe would like to meet briefly and discuss the New Year plan to refresh ourselves on who is working on what (ie staff pooled or specific work stream) as well as how work will be distributed throughout winterfest. Please update your WSO and plan to attend. Until then, happy holidays and have a safe New Year!\\n\\nThank you,\\nIMT Team   \n",
              "438  Hi All-\\nWe would like to meet briefly and discuss the New Year plan to refresh ourselves on who is working on what (ie staff pooled or specific work stream) as well as how work will be distributed throughout winterfest. Please update your WSO and plan to attend. Until then, happy holidays and have a safe New Year!\\n\\nThank you,\\nIMT Team   \n",
              "\n",
              "        attendee_email  length_of_service  response_status  is_organizer  \\\n",
              "416  wellsc12@gene.com            2.51096                1           0.0   \n",
              "429  jtorres3@gene.com           21.31850                1           0.0   \n",
              "432  jtorres3@gene.com           21.31850                1           0.0   \n",
              "433  jtorres3@gene.com           21.31850                1           0.0   \n",
              "435  jtorres3@gene.com           21.31850                1           0.0   \n",
              "436  jtorres3@gene.com           21.31850                1           0.0   \n",
              "437  jtorres3@gene.com           21.31850                1           0.0   \n",
              "438  wellsc12@gene.com            2.51096                1           0.0   \n",
              "\n",
              "     meeting_lapse  time_since_last_promotion  ...   age  \\\n",
              "416           0.25                       -1.0  ...  38.0   \n",
              "429           0.25                       -1.0  ...  42.0   \n",
              "432           0.25                       -1.0  ...  42.0   \n",
              "433           0.25                       -1.0  ...  42.0   \n",
              "435           0.25                       -1.0  ...  42.0   \n",
              "436           0.25                       -1.0  ...  42.0   \n",
              "437           0.25                       -1.0  ...  42.0   \n",
              "438           0.25                       -1.0  ...  38.0   \n",
              "\n",
              "                                job_family  is_optional       start_datetime  \\\n",
              "416  Pricing / Reimbursement / Contracting          0.0  2022-01-03 09:00:00   \n",
              "429  Pricing / Reimbursement / Contracting          0.0  2022-01-03 09:00:00   \n",
              "432  Pricing / Reimbursement / Contracting          0.0  2022-01-03 09:00:00   \n",
              "433  Pricing / Reimbursement / Contracting          0.0  2022-01-03 09:00:00   \n",
              "435  Pricing / Reimbursement / Contracting          0.0  2022-01-03 09:00:00   \n",
              "436  Pricing / Reimbursement / Contracting          0.0  2022-01-03 09:00:00   \n",
              "437  Pricing / Reimbursement / Contracting          0.0  2022-01-03 09:00:00   \n",
              "438  Pricing / Reimbursement / Contracting          0.0  2022-01-03 09:00:00   \n",
              "\n",
              "     num_direct_reports  event_id  manager_length_of_service  \\\n",
              "416                 8.0  60695664                    21.2160   \n",
              "429                 6.0  60729076                    18.4797   \n",
              "432                 6.0  60727560                    18.4797   \n",
              "433                 6.0  60591632                    18.4797   \n",
              "435                 6.0  60695664                    18.4797   \n",
              "436                 6.0  60563909                    18.4797   \n",
              "437                 6.0  60593137                    18.4797   \n",
              "438                 8.0  60563909                    21.2160   \n",
              "\n",
              "     start_datetime_Dayofyear  time_since_new_manager_start_date     timestamp  \n",
              "416                       3.0                              1.586  1.641229e+09  \n",
              "429                       3.0                              1.748  1.641229e+09  \n",
              "432                       3.0                              1.748  1.641229e+09  \n",
              "433                       3.0                              1.748  1.641229e+09  \n",
              "435                       3.0                              1.748  1.641229e+09  \n",
              "436                       3.0                              1.748  1.641229e+09  \n",
              "437                       3.0                              1.748  1.641229e+09  \n",
              "438                       3.0                              1.586  1.641229e+09  \n",
              "\n",
              "[8 rows x 22 columns]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#get samples of False Positive (df_fp) and False Negative (df_fn)\n",
        "y_true=y_test; y_pred=preds0; pos_val=2\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_test= test_df[label_col].map(label_name2num)\n",
        "df_ = pd.DataFrame(zip(y_true, y_pred), columns=['y_true', 'y_pred'], index=test_df.index)\n",
        "df__=pd.concat([df_, test_df],axis=1)\n",
        "df_FP = df__[df__['y_true']==pos_val and df__['y_pred']!=df__['y_true']]\n",
        "df_FP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xQ_5dPojKanM",
        "outputId": "868c845c-9f1f-4d79-f2c6-ddde86cb6eac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============ ensembled method using embs (\"middle concat\")============\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============classification report===============\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.58      0.72        24\n",
            "           1       0.70      0.78      0.74        41\n",
            "           2       0.83      0.88      0.85        65\n",
            "\n",
            "    accuracy                           0.79       130\n",
            "   macro avg       0.82      0.75      0.77       130\n",
            "weighted avg       0.80      0.79      0.79       130\n",
            "\n",
            "One-vs-One ROC AUC scores:\n",
            "0.927392 (macro),\n",
            "0.932438 (weighted by prevalence)\n",
            "One-vs-Rest ROC AUC scores:\n",
            "0.936715 (macro),\n",
            "0.935354 (weighted by prevalence)\n",
            "============ ensembled method using probs (\"late concat\")============\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============classification report===============\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      1.00      0.96        24\n",
            "           1       0.90      0.93      0.92        41\n",
            "           2       0.98      0.94      0.96        65\n",
            "\n",
            "    accuracy                           0.95       130\n",
            "   macro avg       0.94      0.96      0.95       130\n",
            "weighted avg       0.95      0.95      0.95       130\n",
            "\n",
            "One-vs-One ROC AUC scores:\n",
            "0.987589 (macro),\n",
            "0.988265 (weighted by prevalence)\n",
            "One-vs-Rest ROC AUC scores:\n",
            "0.987918 (macro),\n",
            "0.988877 (weighted by prevalence)\n"
          ]
        }
      ],
      "source": [
        "#export\n",
        "def get_performance_scores(y_true, y_prob, y_pred=None, label_name2num=None, normalize:str='all', **kwargs): \n",
        "    \"\"\"a wrapper to show common performance scores of a ML model\n",
        "    Args:\n",
        "      y_true, y_prob, y_pred=None, label_name2num=None, **kwargs\n",
        "      normalize : {'true', 'pred', 'all'}, default=None Normalizes confusion matrix over the true (rows), predicted (columns) conditions or all the population. If None, confusion matrix will not be normalized.\n",
        "    Returns:\n",
        "      clf_report, cm_df, macro_roc_auc_ovo, weighted_roc_auc_ovo, macro_roc_auc_ovr, weighted_roc_auc_ovr\n",
        "\n",
        "    \"\"\"\n",
        "    import seaborn as sns  \n",
        "    from sklearn.metrics import roc_auc_score, classification_report\n",
        "    if y_pred is None:\n",
        "      y_pred = y_prob.argmax(axis=1)\n",
        "\n",
        "    # Creating  a confusion matrix,which compares the y_test and y_pred\n",
        "    cm = confusion_matrix(y_test, y_pred, normalize=normalize)\n",
        "\n",
        "    # Creating a dataframe for a array-formatted Confusion matrix,so it will be easy for plotting.\n",
        "    if label_name2num is not None:\n",
        "        label_vals = list(label_name2num.keys())\n",
        "    else:\n",
        "        label_vals = list(range(y_prob.shape[1]))\n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                     index = label_vals,# e.g.  ['SETOSA','VERSICOLR','VIRGINICA']\n",
        "                     columns =label_vals)\n",
        "    \n",
        "    #Plotting the confusion matrix\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm_df,  cmap=\"YlGnBu\", annot=True)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('Actal Values')\n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.show()\n",
        "\n",
        "    # create classification report\n",
        "    clf_report = classification_report(y_true, y_pred, **kwargs)\n",
        "    print(f\"=============classification report===============\\n\")\n",
        "    print(clf_report)\n",
        "    # make various kinds of roc_auc\n",
        "    macro_roc_auc_ovo = roc_auc_score(y_true, y_prob, multi_class=\"ovo\", average=\"macro\")\n",
        "\n",
        "    weighted_roc_auc_ovo = roc_auc_score(\n",
        "        y_test, y_prob, multi_class=\"ovo\", average=\"weighted\"\n",
        "    )\n",
        "\n",
        "    macro_roc_auc_ovr = roc_auc_score(y_true, y_prob, multi_class=\"ovr\", average=\"macro\")\n",
        "\n",
        "    weighted_roc_auc_ovr = roc_auc_score(\n",
        "        y_true, y_prob, multi_class=\"ovr\", average=\"weighted\"\n",
        "    )\n",
        "    print(\n",
        "        \"One-vs-One ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n",
        "        \"(weighted by prevalence)\".format(macro_roc_auc_ovo, weighted_roc_auc_ovo)\n",
        "    )\n",
        "    print(\n",
        "        \"One-vs-Rest ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n",
        "        \"(weighted by prevalence)\".format(macro_roc_auc_ovr, weighted_roc_auc_ovr)\n",
        "    )\n",
        "    return clf_report, cm_df, macro_roc_auc_ovo, weighted_roc_auc_ovo, macro_roc_auc_ovr, weighted_roc_auc_ovr\n",
        "\n",
        "print('============ ensembled method using embs (\"middle concat\")============\\n')\n",
        "_ = get_performance_scores(y_true=y_test, y_prob=probs0, label_name2num= label_name2num)\n",
        "print('============ ensembled method using probs (\"late concat\")============\\n')\n",
        "_ = get_performance_scores(y_true=y_test, y_prob=probs1, label_name2num= label_name2num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRnpTrjxgSsi"
      },
      "source": [
        "# 6) Bigdata ML\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_5ctor-gdNr"
      },
      "source": [
        "ref:https://gdmarmerola.github.io/big-data-ml-training/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XTmy6sukJqK"
      },
      "source": [
        "## bigdata ML solu1: ensemble learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpjcG-pzgt-s",
        "outputId": "af05988c-eedd-4d61-9ca0-19546130a09b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.7/dist-packages (2.12.0)\n",
            "Collecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 10.1 MB/s \n",
            "\u001b[?25hCollecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (1.21.5)\n",
            "Requirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (0.11.2)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->dask[dataframe]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->dask[dataframe]) (2018.9)\n",
            "Collecting locket\n",
            "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.0->dask[dataframe]) (1.15.0)\n",
            "Installing collected packages: locket, partd, fsspec\n",
            "Successfully installed fsspec-2022.3.0 locket-0.2.1 partd-1.2.0\n",
            "Requirement already satisfied: fsspec>=0.3.3 in /usr/local/lib/python3.7/dist-packages (2022.3.0)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install \"dask[dataframe]\"\n",
        "!pip install 'fsspec>=0.3.3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb9rRfkCKUO-"
      },
      "outputs": [],
      "source": [
        "# libs to help us track memory via sampling\n",
        "import numpy as np\n",
        "import tracemalloc\n",
        "from time import sleep\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sampling time in seconds\n",
        "SAMPLING_TIME = 0.001\n",
        "\n",
        "class MemoryMonitor:\n",
        "    def __init__(self, close=True):\n",
        "        \n",
        "        # start tracemalloc and sets\n",
        "        # measurement atribute to True\n",
        "        tracemalloc.start()\n",
        "        self.keep_measuring = True\n",
        "        self.close = close\n",
        "        \n",
        "    def measure_usage(self):\n",
        "        \n",
        "        \"\"\"\n",
        "        Takes measurements of used memory on\n",
        "        regular intevals determined by the \n",
        "        global SAMPLING_TIME constant\n",
        "        \"\"\"\n",
        "        \n",
        "        # list to store memory usage samples\n",
        "        usage_list = []\n",
        "        \n",
        "        # keeps going until someone changes this parameter to false\n",
        "        while self.keep_measuring:\n",
        "            \n",
        "            # takes a sample, stores it in the usage_list and sleeps\n",
        "            current, peak = tracemalloc.get_traced_memory()\n",
        "            usage_list.append(current/1e6)\n",
        "            sleep(SAMPLING_TIME)\n",
        "            \n",
        "        # stop tracemalloc and returns list\n",
        "        if self.close:\n",
        "            tracemalloc.stop()\n",
        "        return usage_list\n",
        "\n",
        "# imports executor\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from functools import wraps\n",
        "\n",
        "def plot_memory_use(history, fn_name, open_figure=True, offset=0, **kwargs):\n",
        "    \n",
        "    \"\"\"Function to plot memory use from a history collected\n",
        "        by the MemoryMonitor class\n",
        "    \"\"\"\n",
        "\n",
        "    # getting times from counts and sampling time\n",
        "    times = (offset + np.arange(len(history))) * SAMPLING_TIME\n",
        "    \n",
        "    # opening figure and plotting\n",
        "    if open_figure:\n",
        "        plt.figure(figsize=(10,3), dpi=120)\n",
        "    plt.plot(times, history, 'k--', linewidth=1)\n",
        "    plt.fill_between(times, history, alpha=0.5, **kwargs)\n",
        "    \n",
        "    # axes titles\n",
        "    plt.ylabel('Memory usage [MB]')\n",
        "    plt.xlabel('Time [seconds]')\n",
        "    plt.title(f'{fn_name} memory usage over time')\n",
        "    \n",
        "    # legend\n",
        "    plt.legend();\n",
        "\n",
        "def track_memory_use(plot=True, close=True, return_history=False):\n",
        "    \n",
        "    def meta_wrapper(fn):\n",
        "    \n",
        "        \"\"\"\n",
        "        This function is meant to be used as a decorator\n",
        "        that informs wrapped function memory usage\n",
        "        \"\"\"\n",
        "        \n",
        "        # decorator so we can retrieve original fn\n",
        "        @wraps(fn)\n",
        "        def wrapper(*args, **kwargs):\n",
        "\n",
        "            \"\"\"\n",
        "            Starts wrapped function and holds a process \n",
        "            to sample memory usage while executing it\n",
        "            \"\"\"\n",
        "\n",
        "            # context manager for executor\n",
        "            with ThreadPoolExecutor() as executor:\n",
        "\n",
        "                # start memory monitor\n",
        "                monitor = MemoryMonitor(close=close)\n",
        "                mem_thread = executor.submit(monitor.measure_usage)\n",
        "\n",
        "                # start wrapped function and get its result\n",
        "                try:\n",
        "                    fn_thread = executor.submit(fn, *args, **kwargs)\n",
        "                    fn_result = fn_thread.result()\n",
        "\n",
        "                # when wrapped function ends, stop measuring\n",
        "                finally:\n",
        "                    monitor.keep_measuring = False\n",
        "                    history = mem_thread.result()\n",
        "\n",
        "                # inform results via prints and plot\n",
        "                print(f'Current memory usage: {history[-1]:2f}')\n",
        "                print(f'Peak memory usage: {max(history):2f}')\n",
        "                if plot:\n",
        "                    plot_memory_use(history, fn.__name__)\n",
        "            if return_history:\n",
        "                return fn_result, history\n",
        "            else:\n",
        "                return fn_result\n",
        "\n",
        "        return wrapper\n",
        "    \n",
        "    return meta_wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pc-CVstCg7xe"
      },
      "outputs": [],
      "source": [
        "# to visualize memory use ref:https://gdmarmerola.github.io/big-data-ml-training/\n",
        "# track_memory_use will be used as a decorator\n",
        "#from nbdev_colab.track_memory import track_memory_use, plot_memory_use\n",
        "import dask.dataframe as dd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "4YAMnxPKgbC5",
        "outputId": "3ed14610-a793-4a14-f784-9bea3a4613e6"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-27c24b2de952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m# executing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_history_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdask_read_sample_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10e6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_history_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdask_read_test_and_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-124-a56e43bd92af>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0mfn_thread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                     \u001b[0mfn_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;31m# when wrapped function ends, stop measuring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    426\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-127-27c24b2de952>\u001b[0m in \u001b[0;36mdask_read_sample_and_fit_model\u001b[0;34m(blocksize, sample_size, n_models)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# reading train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdask_read_and_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# fitting model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute '__wrapped__'"
          ]
        }
      ],
      "source": [
        "class EnsembleWrapper:\n",
        "    \"\"\"\n",
        "    create an ensembled model of a list of models.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_list):\n",
        "        self.model_list = model_list\n",
        "    def predict_proba(self, X):\n",
        "        probs_list = [mdl.predict_proba(X) for mdl in self.model_list]\n",
        "        return np.array(probs_list).mean(axis=0)\n",
        "\n",
        "# all label values\n",
        "label_vals = list(df[label_col].value_counts().keys())\n",
        "\n",
        "def dask_read_and_sample(blocksize:int=10**6, sample_size:int=1e3, label_vals=label_vals):\n",
        "    \n",
        "    # reading train data\n",
        "    print(f'================== dask reads train_df.csv===================')\n",
        "\n",
        "    # reading train & test with dask data_file = 'train_df.csv'\n",
        "    train_dd = dd.read_csv(data_path+'train_df.csv', \n",
        "                       engine='python',\n",
        "                       encoding='utf-8', # 'utf-8-sig', #\n",
        "                       error_bad_lines=False,\n",
        "                       blocksize=blocksize,\n",
        "                       assume_missing=True,\n",
        "                       )    \n",
        "    print(f'================== dask reads test_df.csv===================')\n",
        "\n",
        "    test_dd = dd.read_csv(data_path+'test_df.csv', \n",
        "                       engine='python',\n",
        "                       encoding='utf-8', # 'utf-8-sig', #\n",
        "                       error_bad_lines=False,\n",
        "                       blocksize=blocksize,\n",
        "                       assume_missing=True\n",
        "                       )\n",
        "    # let us stratify to get the same number of rows for frauds and non-frauds\n",
        "    sample_ls = [train_dd.query(f'{label_col} == {label_val}').sample(frac=sample_size) for label_val in label_vals]\n",
        "    #sample_positive = train_dd.query(f'{label_col} == 1').sample(frac=sample_size)\n",
        "    #sample_negative = train_dd.query(f'{label_col} == 0').sample(frac=sample_size)\n",
        "    \n",
        "    # concatenate the dataframe\n",
        "    df_sampled = dd.concat(sample_ls)\n",
        "    \n",
        "    return df_sampled.compute(scheduler='synchronous')\n",
        "\n",
        "\n",
        "@track_memory_use(close=True, return_history=True)\n",
        "def dask_read_test_and_score(model, blocksize):\n",
        "    \n",
        "    # reading test\n",
        "    df_test = dd.read_csv(data_path+'test_df.csv', blocksize=blocksize)\n",
        "\n",
        "    # splitting design matrix and target\n",
        "    X_test = df_test.drop(label_col, axis=1)\n",
        "    y_test = df_test[label_col].persist(scheduler='synchronous')\n",
        "    \n",
        "    # scoring and printing result\n",
        "    y_prob = X_test.map_partitions(model.predict_proba).compute(scheduler='synchronous')\n",
        "\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    macro_roc_auc_ovo = roc_auc_score(y_test, y_prob, multi_class=\"ovo\", average=\"macro\")\n",
        "    \n",
        "    weighted_roc_auc_ovo = roc_auc_score(\n",
        "        y_test, y_prob, multi_class=\"ovo\", average=\"weighted\"\n",
        "    )\n",
        "    \n",
        "    macro_roc_auc_ovr = roc_auc_score(y_test, y_prob, multi_class=\"ovr\", average=\"macro\")\n",
        "    \n",
        "    weighted_roc_auc_ovr = roc_auc_score(\n",
        "        y_test, y_prob, multi_class=\"ovr\", average=\"weighted\"\n",
        "    )\n",
        "    print(\n",
        "        \"One-vs-One ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n",
        "        \"(weighted by prevalence)\".format(macro_roc_auc_ovo, weighted_roc_auc_ovo)\n",
        "    )\n",
        "    print(\n",
        "        \"One-vs-Rest ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n",
        "        \"(weighted by prevalence)\".format(macro_roc_auc_ovr, weighted_roc_auc_ovr)\n",
        "    )\n",
        "    \n",
        "#example: _, mem_history_3 = dask_read_test_and_score(model, blocksize=5e6)\n",
        "\n",
        "\n",
        "# using a function so we can track memory usage\n",
        "@track_memory_use(close=False, return_history=True)\n",
        "def dask_read_sample_and_fit_model(blocksize, sample_size, n_models):\n",
        "    \n",
        "    # init model list\n",
        "    model_list = []\n",
        "    \n",
        "    # loop for each model\n",
        "    for _ in range(n_models):\n",
        "    \n",
        "        # reading train data\n",
        "        df_train = dask_read_and_sample.__wrapped__(blocksize, sample_size)\n",
        "\n",
        "        # fitting model\n",
        "        model_list.append(fit_model.__wrapped__(df_train))\n",
        "    \n",
        "    return EnsembleWrapper(model_list)\n",
        "    \n",
        "# executing\n",
        "model, mem_history_1 = dask_read_sample_and_fit_model(blocksize=10e6, sample_size=0.05, n_models=10)\n",
        "_, mem_history_2 = dask_read_test_and_score(model, blocksize=5e6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QaAIqWQgbF7"
      },
      "outputs": [],
      "source": [
        "\n",
        "multimodal_clf = Fastai_Multimodal_Classifier()\n",
        "multimodal_clf.fit(df=train_df, label_col=label_col, cnt_cols=cnt_cols, cat_cols=cat_cols,txt_cols=txt_cols,img_cols=img_cols)\n",
        "(preds0, preds1), (probs0, probs1) = multimodal_clf.get_preds(test_df)\n",
        "(preds0, preds1), (probs0, probs1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwUpsh_BgbIf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ_DDqpyj8pG"
      },
      "source": [
        "## bigdata ML solu2: incremental learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "xD57GAh5LJY0",
        "outputId": "94c25760-866b-42ca-cc79-d50d998b7d20"
      },
      "outputs": [
        {
          "ename": "ParserError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-edd3d504cc6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mKerasWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_history_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdask_read_and_incrementally_fit_keras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-124-a56e43bd92af>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0mfn_thread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                     \u001b[0mfn_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;31m# when wrapped function ends, stop measuring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-129-edd3d504cc6e>\u001b[0m in \u001b[0;36mdask_read_and_incrementally_fit_keras\u001b[0;34m(blocksize)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# reading df with dask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'train_df.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# creating keras model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/dask/dataframe/io/csv.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(urlpath, blocksize, collection, lineterminator, compression, sample, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0minclude_path_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_path_column\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m         )\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/dask/dataframe/io/csv.py\u001b[0m in \u001b[0;36mread_pandas\u001b[0;34m(reader, urlpath, blocksize, collection, lineterminator, compression, sample, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;31m# Use sample to infer dtypes and check for presence of include_path_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m     \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minclude_path_column\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minclude_path_column\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 642"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "\n",
        "class KerasWrapper:\n",
        "    def __init__(self, model, feat_mean, feat_std):\n",
        "        self.model = model\n",
        "        self.feat_mean = feat_mean\n",
        "        self.feat_std = feat_std\n",
        "        \n",
        "    def predict_proba(self, X):\n",
        "        \n",
        "        preds = self.model.predict((X - self.feat_mean)/self.feat_std)\n",
        "        return np.c_[preds, preds]\n",
        "        \n",
        "# using a function so we can track memory usage\n",
        "@track_memory_use(close=False, return_history=True)\n",
        "def dask_read_and_incrementally_fit_keras(blocksize):\n",
        "    print(f'================== dask reads train_df.csv===================')\n",
        "\n",
        "    # reading train & test with dask data_file = 'train_df.csv'\n",
        "    train_dd = dd.read_csv(data_path+'train_df.csv', \n",
        "                       engine='python',\n",
        "                       encoding='utf-8', # 'utf-8-sig', #\n",
        "                       error_bad_lines=False,\n",
        "                       blocksize=blocksize,\n",
        "                       assume_missing=True,\n",
        "                       )    \n",
        "    print(f'================== dask reads test_df.csv===================')\n",
        "\n",
        "    test_dd = dd.read_csv(data_path+'test_df.csv', \n",
        "                       engine='python',\n",
        "                       encoding='utf-8', # 'utf-8-sig', #\n",
        "                       error_bad_lines=False,\n",
        "                       blocksize=blocksize,\n",
        "                       assume_missing=True\n",
        "                       )\n",
        "    # reading df with dask***********************************\n",
        "    df_train = dd.read_csv(data_path+'train_df.csv', blocksize=blocksize)\n",
        "    \n",
        "    # creating keras model\n",
        "    model = Sequential([Dense(16, activation='relu'),\n",
        "                        Dropout(0.25),\n",
        "                        Dense(16, activation='relu'),\n",
        "                        Dropout(0.25),\n",
        "                        Dense(16, activation='relu'),\n",
        "                        Dropout(0.25),\n",
        "                        Dense(1, activation='sigmoid')])\n",
        "    model.compile(loss='binary_crossentropy')\n",
        "    \n",
        "    # getting mean and std for dataset to normalize features\n",
        "    feat_mean = df_train.drop(label_col, axis=1).mean().compute(scheduler='synchronous')\n",
        "    feat_std = df_train.drop(label_col, axis=1).std().compute(scheduler='synchronous')\n",
        "    \n",
        "    # loop for number of partitions\n",
        "    for i in range(df_train.npartitions):\n",
        "        \n",
        "        # getting one partition\n",
        "        part = df_train.get_partition(i).compute(scheduler='synchronous')\n",
        "        \n",
        "        # splitting\n",
        "        X_part = (part.drop('isFraud', axis=1) - feat_mean)/feat_std\n",
        "        y_part = part['isFraud']\n",
        "        \n",
        "        # running partial fit\n",
        "        model.fit(X_part, y_part, batch_size=512)\n",
        "    \n",
        "    return KerasWrapper(model, feat_mean, feat_std)\n",
        "\n",
        "model, mem_history_1 = dask_read_and_incrementally_fit_keras(blocksize=5e6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QM5jtHrXLJhv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT5ncNvjLJli"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvEe2TMVLJo0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RdkJjaODdXu"
      },
      "source": [
        "# 7) Extra & Experimental\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbN0hc3yCFlT"
      },
      "source": [
        "## experiment: create ensemble classifier using individual classifiers' various output\n",
        "\n",
        "I can extract various info from each individual classifier, including\n",
        "\n",
        "- probs (highest level features)\n",
        "- embeddings (intermediate level features)\n",
        "- encoding of txt_cols, img_cols, cnt_cols, cat_cols (lowest level features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gehuY4i1t08",
        "outputId": "9eb133a5-c6a7-4f3a-986d-167be50ce10c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# load lm and clf based on 'skills' column\n",
        "lm0, clf0 = load_fastai_text_classifier(df, \n",
        "                                       txt_col=txt_cols[0],\n",
        "                                       label_col=label_col,\n",
        "                                       model_path='/content/drive/My Drive/fastai_multimodal/model/',\n",
        "                                       flag_auto_lr=False\n",
        "                                       )\n",
        "\n",
        "# load lm and clf based on 'title' column\n",
        "lm1, clf1 = load_fastai_text_classifier(df, \n",
        "                                       txt_col=txt_cols[1],\n",
        "                                       label_col=label_col,\n",
        "                                       model_path='/content/drive/My Drive/fastai_multimodal/model/',\n",
        "                                       flag_auto_lr=False\n",
        "                                       )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ja6rAxtaZ_3G"
      },
      "outputs": [],
      "source": [
        "# get df[txt_cols[0]] embeddings_df e1 \n",
        "e1 = get_fastai_docs_embs(docs=df[txt_cols[0]], learn=clf0, lm=lm0, df=None, txt_col=None)\n",
        "# get df[txt_cols[1]] embeddings_df e2\n",
        "e2 = get_fastai_docs_embs(docs=df[txt_cols[1]], learn=clf1, lm=lm1, df=None, txt_col=None)\n",
        "\n",
        "#construct a embedings ls\n",
        "embs_ls = [e1, e2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUeOUpIL_Dlf"
      },
      "outputs": [],
      "source": [
        "embs_ls =[e1, e2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AvFwFUEoxF6"
      },
      "outputs": [],
      "source": [
        "#pickle embs_ls\n",
        "import pickle\n",
        "with open(\"embs_ls.pickle\",\"wb\") as f:\n",
        "  pickle.dump(embs_ls,f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcYefhm13QMy",
        "outputId": "ee764bf4-b1f4-4aef-a4e3-22200b9cf194"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# get prediction on labels and probabilities\n",
        "preds0, prods0 = fastai_learner_preds(learner=clf0, test_df=df, label_col=label_col, txt_col=txt_cols[0])\n",
        "preds1, prods1 = fastai_learner_preds(learner=clf1, test_df=df, label_col=label_col, txt_col=txt_cols[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6-lC45d7ama"
      },
      "outputs": [],
      "source": [
        "#pickle probs_ls\n",
        "probs_ls = [prods0.numpy(), prods1.numpy()]\n",
        "with open(\"probs_ls.pickle\",\"wb\") as f:\n",
        "  pickle.dump(probs_ls,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIjBmklLkCve"
      },
      "source": [
        "## module: Visualize doc similarity\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diZW7gjylL3r"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\"\"\"\n",
        "def normalize(data):\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "## permutation test\n",
        "def perm_test(x1, x2):\n",
        "  \"\"\"return the p-value of similarity bw x1 and x2\"\"\"\n",
        "  import math, random\n",
        "  from scipy import stats\n",
        "  similarity = lambda x1, x2: sum(xj*xk for xj,xk in zip(x1, x2))/math.sqrt(sum(xj**2 for xj in x1)*sum(xk**2 for xk in x2))\n",
        "\n",
        "  lx, sr = len(x1), []\n",
        "  for j in range(10000):\n",
        "      mj = random.sample(x1, lx)\n",
        "      sr.append(similarity(mj, x2))\n",
        "  shape, loc, scale = stats.weibull_min.fit(sr)\n",
        "  ## -log10(p)\n",
        "  ej = ((sr-loc)/scale)**shape*math.log10(math.exp(1.))\n",
        "  p = 10**(-ej)\n",
        "  return p\n",
        "\n",
        "def plot_similarity(labels, features, rotation, flag_norm_corr=True):\n",
        "  norm_features = [tf.linalg.normalize(t)[0].numpy() for t in features]\n",
        "  corr = np.inner(norm_features, norm_features)\n",
        "  if flag_norm_corr:\n",
        "    corr = normalize(corr)\n",
        "\n",
        "  sns.set(font_scale=1.2)\n",
        "  g = sns.heatmap(\n",
        "      corr,\n",
        "      xticklabels=labels,\n",
        "      yticklabels=labels,\n",
        "      vmin=0,\n",
        "      vmax=1,\n",
        "      cmap=\"YlOrRd\")\n",
        "  g.set_xticklabels(labels, rotation=rotation)\n",
        "  g.set_title(\"Semantic Textual Similarity\")\n",
        "\n",
        "def plot_docs_sim(label_doc_dic, doc_embeddings, flag_norm_corr=True):\n",
        "  \"\"\"get embedding vectors for docs and plot their similarity\n",
        "  Args:\n",
        "    label_doc_dic:dict with label:doc pairs\n",
        "    doc_embeddings: a list of embedding vectors\n",
        "    flag_norm_corr: whether or not normalized corr e.g. max(X)->1, min(X)->0, x-> x/(max(X)-min(X))\n",
        "  Return:\n",
        "    None\n",
        "  \"\"\"\n",
        "  \n",
        "  if isinstance(doc_embeddings, scipy.sparse.csr.csr_matrix):\n",
        "      doc_embeddings = doc_embeddings.toarray()\n",
        "\n",
        "  plot_similarity(list(label_doc_dic.keys()), doc_embeddings, 90, flag_norm_corr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhEcff2nkTpc"
      },
      "outputs": [],
      "source": [
        "# load validation data\n",
        "df = pd.read_csv(data_path+'JD_skills_similarity_validation.csv')\n",
        "# extract title from job requisition\n",
        "df['Title'] = [re.split('-|,', ' '.join(x.split(' ')[1:]))[0] for x in df['Job Requisition']]\n",
        "# concate multiple text cols\n",
        "df['Title_Skills'] = df[['Title','Skills']].agg(', '.join, axis=1)\n",
        "# construct a dictionary with key=label, value=skills\n",
        "label_doc_dic = dict(zip(df['Code'],df['Title_Skills']))\n",
        "label_doc_dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRwF7XDWkTtT"
      },
      "outputs": [],
      "source": [
        "doc_embeddings = get_fastai_docs_embs(docs=list(label_doc_dic.values()), learn=clf, lm=lm, df=None, txt_col=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0HVezj8kTwI"
      },
      "outputs": [],
      "source": [
        "plot_docs_sim(label_doc_dic, doc_embeddings, flag_norm_corr=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7vUtT-IKa67"
      },
      "outputs": [],
      "source": [
        "x1 = list(doc_embeddings[0])\n",
        "x2 = list(doc_embeddings[1])\n",
        "perm_test(x1, x2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaOgn9xLBGPw"
      },
      "source": [
        "## Experiment2: Error analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "LL5hcGs-fdTW",
        "outputId": "4e1cf2ae-c12d-4b01-e6f8-9cc22de2a811"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-13c8842e57de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# classifier performance by confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minterp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationInterpretation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultimodal_clf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0minterp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/interpret.py\u001b[0m in \u001b[0;36mfrom_learner\u001b[0;34m(cls, learn, ds_idx, dl, act)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;34m\"Construct interpretation object from a learner\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         _,_,losses = learn.get_preds(dl=dl, with_input=False, with_loss=True, with_decoded=False,\n\u001b[1;32m     41\u001b[0m                                      with_preds=False, with_targs=False, act=act)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Fastai_Multimodal_Classifier' object has no attribute 'dls'"
          ]
        }
      ],
      "source": [
        "# classifier performance by confusion matrix\n",
        "interp = ClassificationInterpretation.from_learner(multimodal_clf)\n",
        "interp.plot_confusion_matrix(figsize=(8,8), dpi=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "5__fNigTPx-T",
        "outputId": "7fbcdff4-906a-4960-c67c-c4a6a38e5107"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        26\n",
            "           1       0.32      0.17      0.22       222\n",
            "           2       0.33      0.39      0.36       253\n",
            "           3       0.40      0.15      0.22       239\n",
            "           4       0.37      0.68      0.48       260\n",
            "\n",
            "    accuracy                           0.35      1000\n",
            "   macro avg       0.28      0.28      0.26      1000\n",
            "weighted avg       0.35      0.35      0.32      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "interp.print_classification_report()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "rG0dyJA1g3LT",
        "outputId": "07f93ddd-5756-4dad-8c84-e2d3c35541f1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(array([2, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 3, 1, 1, 2, 2, 4, 3, 2, 4, 2, 1,\n",
              "        2, 2, 2, 2, 4, 2, 3, 2, 4, 3, 4, 4, 4, 2, 1, 4, 3, 2, 4, 2, 4, 2,\n",
              "        2, 2, 3, 4, 2, 4, 4, 4, 2, 3, 4, 4, 4, 1, 2, 3, 2, 4, 2, 2, 4, 4,\n",
              "        3, 4, 4, 4, 4, 4, 4, 2, 1, 2, 1, 4, 4, 4, 4, 2, 1, 4, 4, 4, 4, 4,\n",
              "        2, 4, 4, 4, 4, 4, 4, 2, 4, 2, 2, 2, 3, 1, 4, 4, 4, 4, 1, 2, 4, 1,\n",
              "        4, 4, 2, 4, 3, 2, 3, 3, 2, 4, 3, 4, 3, 4, 3, 0, 2, 4, 4, 1, 4, 2,\n",
              "        3, 4, 4, 2, 2, 1, 4, 4, 4, 4, 1, 1, 4, 1, 4, 3, 4, 1, 4, 4, 4, 2,\n",
              "        4, 4, 2, 3, 3, 4, 2, 4, 2, 4, 4, 4, 4, 4, 2, 4, 2, 4, 4, 4, 1, 2,\n",
              "        1, 4, 2, 1, 2, 2, 3, 3, 0, 4, 1, 4, 2, 4, 4, 2, 4, 1, 4, 4, 2, 1,\n",
              "        4, 3, 4, 2, 2, 1, 2, 2, 1, 1, 4, 2, 3, 2, 2, 0, 4, 3, 2, 4, 2, 2,\n",
              "        1, 3, 2, 1, 3, 2, 1, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 2, 2, 2, 1, 4,\n",
              "        2, 4, 2, 3, 4, 3, 3, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4,\n",
              "        4, 4, 4, 4, 2, 2, 4, 4, 4, 3, 3, 4, 4, 1, 3, 3, 2, 2, 4, 3, 4, 4,\n",
              "        4, 2, 4, 4, 4, 3, 3, 1, 4, 3, 4, 2, 2, 2, 4, 4, 0, 4, 4, 4, 4, 2,\n",
              "        4, 1, 3, 2, 2, 1, 4, 2, 1, 4, 3, 4, 4, 4, 4, 3, 4, 4, 2, 4, 3, 2,\n",
              "        4, 2, 2, 4, 4, 1, 4, 2, 4, 2, 4, 4, 4, 4, 2, 4, 1, 1, 2, 2, 0, 2,\n",
              "        2, 4, 4, 4, 4, 0, 4, 4, 4, 4, 1, 4, 3, 4, 2, 4, 3, 2, 4, 2, 2, 1,\n",
              "        2, 1, 4, 1, 4, 4, 3, 1, 4, 2, 2, 2, 4, 1, 1, 1, 3, 1, 4, 1, 4, 4,\n",
              "        3, 4, 4, 4, 1, 4, 4, 2, 2, 4, 2, 4, 2, 4, 2, 4, 0, 4, 4, 3, 4, 1,\n",
              "        4, 4, 3, 1, 4, 4, 3, 4, 2, 4, 2, 4, 3, 1, 2, 4, 2, 2, 1, 4, 3, 4,\n",
              "        2, 4, 2, 4, 2, 1, 2, 2, 4, 2, 4, 1, 4, 4, 1, 4, 4, 2, 4, 4, 4, 4,\n",
              "        4, 4, 4, 4, 4, 4, 1, 1, 1, 4, 1, 4, 4, 4, 4, 4, 2, 2, 1, 2, 4, 4,\n",
              "        2, 4, 1, 4, 4, 3, 2, 4, 4, 3, 2, 4, 4, 0, 3, 4, 4, 2, 1, 0, 2, 1,\n",
              "        4, 4, 2, 4, 2, 4, 2, 1, 4, 2, 4, 4, 4, 4, 3, 0, 4, 4, 4, 2, 2, 3,\n",
              "        2, 1, 2, 0, 4, 4, 4, 2, 2, 2, 2, 2, 1, 4, 2, 2, 2, 2, 4, 2, 4, 4,\n",
              "        1, 0, 2, 2, 4, 4, 2, 2, 2, 2, 3, 2, 1, 0, 2, 1, 4, 1, 2, 2, 4, 4,\n",
              "        1, 4, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 1, 4, 1, 2, 3, 3, 3, 4, 4, 4,\n",
              "        2, 4, 3, 2, 1, 4, 1, 4, 3, 4, 4, 2, 4, 4, 2, 2, 4, 4, 4, 1, 4, 4,\n",
              "        4, 2, 2, 4, 4, 2, 1, 4, 4, 4, 4, 4, 3, 4, 0, 2, 4, 2, 2, 4, 2, 0,\n",
              "        2, 4, 2, 4, 3, 4, 2, 4, 2, 4, 2, 4, 4, 4, 4, 2, 2, 2, 4, 4, 4, 2,\n",
              "        4, 2, 1, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 2, 3, 2, 4, 4, 4, 4, 2,\n",
              "        4, 4, 4, 2, 2, 4, 2, 3, 1, 2, 4, 2, 3, 4, 4, 4, 4, 4, 2, 2, 4, 4,\n",
              "        3, 4, 4, 1, 4, 2, 4, 1, 3, 4, 4, 1, 4, 4, 4, 4, 2, 4, 4, 2, 4, 3,\n",
              "        4, 1, 4, 3, 2, 4, 4, 2, 1, 4, 2, 1, 4, 2, 2, 4, 4, 4, 2, 2, 4, 2,\n",
              "        1, 4, 4, 4, 1, 4, 2, 3, 4, 2, 4, 4, 4, 2, 4, 4, 3, 4, 2, 4, 4, 4,\n",
              "        4, 2, 1, 2, 4, 2, 0, 4, 4, 4, 2, 2, 1, 2, 2, 1, 2, 2, 4, 2, 4, 4,\n",
              "        2, 2, 4, 4, 4, 2, 4, 2, 4, 2, 4, 2, 4, 3, 2, 4, 2, 2, 2, 2, 4, 2,\n",
              "        3, 1, 4, 1, 4, 4, 4, 2, 3, 1, 4, 4, 2, 2, 1, 2, 2, 4, 1, 3, 2, 1,\n",
              "        4, 4, 2, 2, 4, 1, 4, 0, 4, 4, 1, 4, 4, 4, 1, 2, 4, 0, 2, 2, 4, 4,\n",
              "        1, 4, 4, 2, 4, 2, 2, 4, 4, 4, 2, 1, 4, 2, 2, 3, 4, 4, 4, 4, 4, 2,\n",
              "        4, 4, 4, 4, 4, 4, 4, 2, 3, 0, 2, 3, 1, 1, 4, 2, 4, 4, 4, 4, 4, 4,\n",
              "        4, 2, 2, 4, 2, 4, 4, 4, 1, 2, 1, 4, 4, 3, 3, 4, 2, 3, 4, 2, 4, 3,\n",
              "        2, 4, 4, 2, 1, 2, 2, 4, 4, 2, 4, 2, 1, 2, 4, 4, 2, 1, 4, 3, 2, 4,\n",
              "        4, 4, 2, 3, 2, 3, 3, 2, 4, 3, 4, 4, 4, 4, 2, 2, 2, 4, 2, 0, 4, 3,\n",
              "        4, 2, 2, 2, 2, 2, 4, 4, 4, 2, 4, 3, 2, 4, 4, 2, 4, 4, 4, 1, 4, 4,\n",
              "        2, 2, 4, 4, 2, 4, 1, 2, 4, 2]),\n",
              " TensorText([[2.6377e-03, 1.0557e-01, 3.5229e-01, 1.9200e-01, 3.4750e-01],\n",
              "         [1.6237e-01, 2.3570e-01, 1.5390e-01, 4.6321e-02, 4.0172e-01],\n",
              "         [2.1127e-03, 2.3576e-01, 1.0836e-01, 1.9921e-01, 4.5455e-01],\n",
              "         ...,\n",
              "         [7.2892e-03, 2.0223e-01, 5.3397e-01, 1.9132e-01, 6.5190e-02],\n",
              "         [5.1529e-04, 5.6832e-02, 1.5343e-01, 7.9755e-02, 7.0946e-01],\n",
              "         [4.3410e-02, 1.7131e-01, 3.0719e-01, 1.8852e-01, 2.8957e-01]]))"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# how to get prediction/inference on validation data? https://forums.fast.ai/t/unable-to-get-predictions-on-validation-dataset-v2/79171/2\n",
        "\n",
        "# get the idxs of validation data\n",
        "valid_idxs = clf0.dls.valid.get_idxs()\n",
        "\n",
        "# get predictions on validation data\n",
        "preds_valid, probs_valid = fastai_learner_preds(learner=clf0, test_df=df.iloc[valid_idxs,], label_col=label_col, txt_col=txt_cols[-1])\n",
        "preds_valid, probs_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "id": "9eHSxRLdYTL5",
        "outputId": "d763f81e-ba42-4338-e021-6f3e470ae3f1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c14a81ab-44e1-4c48-95bf-7420a61b47c7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Description</th>\n",
              "      <th>AdoptionSpeed</th>\n",
              "      <th>pred</th>\n",
              "      <th>prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>We just rescued this baby girl from Cyberjaya. Will take her to the vet for check-up, deworming and vaccination and also will sponsor for future spaying cost. Currently being fostered but cannot foster for too long. Please call if interested. Thanks</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0.352287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>Mok is very active and like to play. I hope can find loving and caring owner to take care of Mok. The adoption fee will be used to buy food for other cat in my home</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0.401718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>970</th>\n",
              "      <td>Smite was saved from the pound. He was about to be euthanized three weeks ago. He was diagnosed with distemper. A common disease due to the condition in the pound. Doctors told that puppies only has a 20% survival rate. However, Smite is a lucky boy, he pulled through. He is active, happy and ever cheerful now. Do give this wonderful boy a second chance in life. There would be certain terms and conditions imposed to the potential adopter.</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0.454553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>I picked her up at Sg.Buloh roadside. When I rescue her, she have lack of vitamin B12 which cause her leg can't walk properly. I've sent her to vet &amp; x-ray, been taking care of her for almost a month. Now she is fully recover which she is quite active on running here and there. Hope can find her a great home. Btw, she is toilet trained. =) FREE for adoption. Please help to adopt her...thank you! Call me or SMS me at: (TEH)</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.339245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>352</th>\n",
              "      <td>please adopt this little furkid. She needs a home urgently or else the house community will call the local authority. She deserve another chance to live.</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0.912060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>983</th>\n",
              "      <td>I am a very well-behaved cat.</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0.943942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>Only for serious adopter please do sms or directly contact with me for more details, thanks!</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.416027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>A real cute girl up for adoption. She was rescued in front of ISKL. She was playing with her brothers and sisters. Please open your hearts out to her. Please call immediately. A must view.</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.533969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>672</th>\n",
              "      <td>Musang was born at my house. When she was 4-mth old, she suffered serious injury as a result of an attack. It really scared the whole household. Luckily, after adequately attended and medication, she recovered to her oldself. What a relief... But soon I'll need to move to a new residence, I can't bring her along. I'm looking for somebody who would adopt her and love her. I'll miss her, for sure...</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0.709465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>780</th>\n",
              "      <td>Shadow is a charming black knight with shinning black armor, sometimes he could be a cute black dragon Toothless from the movie ''How To Train Your Dragon\".:D He has big bright eyes that can make your heart melt by saying :\" AWW \". He loves to be around human , and loves to play. Unfortunately , me and my Family are moving, and there's too many amount of cats , so we have to giveaway some. We are looking for a loving and caring home for Shadow, adopt him to brighten up you everyday life ! Please contact ( Miss Joanne ) for more info ! Thanks :D</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0.307192</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c14a81ab-44e1-4c48-95bf-7420a61b47c7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c14a81ab-44e1-4c48-95bf-7420a61b47c7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c14a81ab-44e1-4c48-95bf-7420a61b47c7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Description  \\\n",
              "150                                                                                                                                                                                                                                                                                                               We just rescued this baby girl from Cyberjaya. Will take her to the vet for check-up, deworming and vaccination and also will sponsor for future spaying cost. Currently being fostered but cannot foster for too long. Please call if interested. Thanks   \n",
              "211                                                                                                                                                                                                                                                                                                                                                                                                    Mok is very active and like to play. I hope can find loving and caring owner to take care of Mok. The adoption fee will be used to buy food for other cat in my home   \n",
              "970                                                                                                              Smite was saved from the pound. He was about to be euthanized three weeks ago. He was diagnosed with distemper. A common disease due to the condition in the pound. Doctors told that puppies only has a 20% survival rate. However, Smite is a lucky boy, he pulled through. He is active, happy and ever cheerful now. Do give this wonderful boy a second chance in life. There would be certain terms and conditions imposed to the potential adopter.   \n",
              "110                                                                                                                              I picked her up at Sg.Buloh roadside. When I rescue her, she have lack of vitamin B12 which cause her leg can't walk properly. I've sent her to vet & x-ray, been taking care of her for almost a month. Now she is fully recover which she is quite active on running here and there. Hope can find her a great home. Btw, she is toilet trained. =) FREE for adoption. Please help to adopt her...thank you! Call me or SMS me at: (TEH)   \n",
              "352                                                                                                                                                                                                                                                                                                                                                                                                               please adopt this little furkid. She needs a home urgently or else the house community will call the local authority. She deserve another chance to live.   \n",
              "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ...   \n",
              "983                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           I am a very well-behaved cat.   \n",
              "379                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Only for serious adopter please do sms or directly contact with me for more details, thanks!   \n",
              "441                                                                                                                                                                                                                                                                                                                                                                            A real cute girl up for adoption. She was rescued in front of ISKL. She was playing with her brothers and sisters. Please open your hearts out to her. Please call immediately. A must view.   \n",
              "672                                                                                                                                                        Musang was born at my house. When she was 4-mth old, she suffered serious injury as a result of an attack. It really scared the whole household. Luckily, after adequately attended and medication, she recovered to her oldself. What a relief... But soon I'll need to move to a new residence, I can't bring her along. I'm looking for somebody who would adopt her and love her. I'll miss her, for sure...   \n",
              "780  Shadow is a charming black knight with shinning black armor, sometimes he could be a cute black dragon Toothless from the movie ''How To Train Your Dragon\".:D He has big bright eyes that can make your heart melt by saying :\" AWW \". He loves to be around human , and loves to play. Unfortunately , me and my Family are moving, and there's too many amount of cats , so we have to giveaway some. We are looking for a loving and caring home for Shadow, adopt him to brighten up you everyday life ! Please contact ( Miss Joanne ) for more info ! Thanks :D   \n",
              "\n",
              "     AdoptionSpeed  pred      prob  \n",
              "150              4     2  0.352287  \n",
              "211              1     4  0.401718  \n",
              "970              1     4  0.454553  \n",
              "110              2     2  0.339245  \n",
              "352              4     4  0.912060  \n",
              "..             ...   ...       ...  \n",
              "983              4     4  0.943942  \n",
              "379              4     1  0.416027  \n",
              "441              2     2  0.533969  \n",
              "672              2     4  0.709465  \n",
              "780              4     2  0.307192  \n",
              "\n",
              "[1000 rows x 4 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_res = df.iloc[valid_idxs,][[txt_cols[-1], label_col]].copy()\n",
        "df_res['pred'] = preds_valid\n",
        "df_res['prob'] = probs_valid.numpy().max(axis=1)\n",
        "df_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "qYSk4CneBPpB",
        "outputId": "0637f883-fccc-42f0-a769-462fe58669df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  \n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-19999259693f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# false positive samples subset idxs in valid_idxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pred'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt_cols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_fp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3462\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3464\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3466\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['label'] not in index\""
          ]
        }
      ],
      "source": [
        "# false positive samples subset idxs in valid_idxs\n",
        "df_fp = df_res[df_res[label_col] == 0][df_res['pred'] == 1][['label', 'pred', 'prob', txt_cols[-1]]]\n",
        "df_fp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "yoLQ0KoIBPr-",
        "outputId": "ebe026a5-4d29-4cf9-eecd-47e1c7838c38"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-bff921da-e3a2-4753-9b0e-46095fbf13e0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pred</th>\n",
              "      <th>prob</th>\n",
              "      <th>text_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bff921da-e3a2-4753-9b0e-46095fbf13e0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bff921da-e3a2-4753-9b0e-46095fbf13e0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bff921da-e3a2-4753-9b0e-46095fbf13e0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [label, pred, prob, text_2]\n",
              "Index: []"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# false negative samples\n",
        "df_fn = df_res[df_res[label_col] == 1][df_res['pred'] == 0][['label', 'pred','prob', txt_cols[-1]]]\n",
        "df_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "597Jc_E01i0_",
        "outputId": "9cd9e657-fa3c-468a-892f-a6dcacb5e8ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([], [])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(set(df_fn.index ) & set(valid_idxs)), list(set(df_fp.index ) & set(valid_idxs)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW4IEkxIBS-_"
      },
      "source": [
        "## module: Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzlNAlthBYEd"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "def get_fastai_classifier_error_analysis(learner, df:pd.DataFrame, label_col:str, txt_col:str=None):\n",
        "    \"\"\"get the error analysis of a fastai text classifier on its validation dataset\n",
        "    Args:\n",
        "      learner:a trained fastai text classifier e.g. clf2\n",
        "      df:pd.DataFrame the whole dataframe learner was trained on. learner has the info on how to split df into train and valid\n",
        "      label_col:str e.g. 'label'\n",
        "      txt_col:str e.g. 'title_raw+hard_skills_name'\n",
        "    Ref: https://forums.fast.ai/t/unable-to-get-predictions-on-validation-dataset-v2/79171\n",
        "    \"\"\"\n",
        "    # show classifier performance by confusion matrix\n",
        "    interp = ClassificationInterpretation.from_learner(learner)\n",
        "    interp.plot_confusion_matrix(figsize=(8,8), dpi=60)\n",
        "\n",
        "    #show classification report\n",
        "    interp.print_classification_report()\n",
        "\n",
        "    # get the idxs of validation data\n",
        "    valid_idxs = learner.dls.valid.get_idxs()\n",
        "\n",
        "    # get predictions on validation data\n",
        "    preds_valid, probs_valid = fastai_learner_preds(learner=learner, test_df=df.iloc[valid_idxs,], label_col=label_col, txt_col=txt_col)\n",
        "\n",
        "    # prep df for result\n",
        "    df_res = df.iloc[valid_idxs,][[txt_col, label_col]].copy()\n",
        "    df_res['prediction'] = preds_valid\n",
        "    df_res['probability'] = probs_valid.numpy().max(axis=1)\n",
        "\n",
        "\n",
        "    # false positive samples subset idxs in valid_idxs\n",
        "    df_fp = df_res[df_res[label_col] == 0][df_res['prediction'] == 1][['label', 'prediction', 'probability', txt_col]]\n",
        "\n",
        "    # false negative samples\n",
        "    df_fn = df_res[df_res[label_col] == 1][df_res['prediction'] == 0][['label', 'prediction','probability', txt_col]]\n",
        "    return df_fp, df_fn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "TZhpZHk_BYHp",
        "outputId": "7ec334bf-42a6-413b-f27f-f98785546d82"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-8de32cace172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get false_positives and false_negatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_fp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fastai_classifier_error_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtxt_cols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_fp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'clf0' is not defined"
          ]
        }
      ],
      "source": [
        "# get false_positives and false_negatives \n",
        "df_fp, df_fn = get_fastai_classifier_error_analysis(learner=clf0, df=df, label_col=label_col, txt_col=txt_cols[-1])\n",
        "\n",
        "df_fp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tamlFbdyGGjZ"
      },
      "outputs": [],
      "source": [
        "df_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FVEAyUUPAtO"
      },
      "source": [
        "### experiment\n",
        "I need to convert tab_learn1, tab_learn2 as  End-to-End classifiers which take raw input and output preds, probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2h2Mcp6xEKC"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(\"embs_ls.pickle\",\"wb\") as f:\n",
        "  pickle.dump(embs_ls,f)\n",
        "with open(\"embs_ls.pickle\",\"rb\") as f:\n",
        "  embs_ls = pickle.load(f)\n",
        "\n",
        "with open(\"probs_ls.pickle\",\"wb\") as f:\n",
        "  pickle.dump(probs_ls,f)\n",
        "with open(\"probs_ls.pickle\", \"rb\") as f:\n",
        "  probs_ls = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW0DABTBDgCG"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "def split_idxs(df, train_size=.9, flag_random_split=True):\n",
        "    \"\"\" split df index into 2 parts: train_idxs and test_idxs \n",
        "    Args:\n",
        "        df: the dataframe of all your data\n",
        "        train_size (float in [0,1], default 0.9)\n",
        "        flag_random_split(bool, default False): do you want random split idxs?\n",
        "    Returns:\n",
        "        (ls_train, ls_test): a 2-tuple of lists for train indices and test indices\n",
        "\n",
        "    Example:\n",
        "        df = pd.DataFrame({'c1':list(range(26)), 'c2':list(string.ascii_lowercase)})\n",
        "        splits = split_idxs(df)\n",
        "        ...\n",
        "        # use splits to build TabularPandas taublar object\n",
        "        to = TabularPandas(df, \n",
        "                   procs=procs,\n",
        "                   cat_names=cat_names,\n",
        "                   cont_names=cont_names,\n",
        "                   y_names=y_names,\n",
        "                   y_block=y_block,\n",
        "                   splits=splits)\n",
        "    \"\"\"\n",
        "    import random\n",
        "    ls = range_of(df)\n",
        "    print(ls)\n",
        "    if flag_random_split:\n",
        "        splits = RandomSplitter()(ls)\n",
        "    else:\n",
        "        ls_train = ls[:int(df.shape[0]*train_size)]\n",
        "        ls_test = ls[int(df.shape[0]*train_size):]\n",
        "        random.shuffle(ls_train)\n",
        "        random.shuffle(ls_test)\n",
        "        splits = (ls_train, ls_test)\n",
        "    return splits\n",
        "def train_ensembled_classifier_by_fastai_tabular(clfs, lms, df, label_col, txt_cols):\n",
        "    \"\"\"based on fastai tabular learner, train 2 ensembled text classifiers, including\n",
        "    1) doc_embs\n",
        "    2) probs\n",
        "    #TBD 3) ensembled of the above 2\n",
        "    Args:\n",
        "      clfs:list of fastai text learners e.g. clfs=[clf0, clf1, clf2]\n",
        "      lms: list of fastai text language models e.g. lms=[lm0, lm1, lm2]\n",
        "      df:pd.DataFrame containing txt_cols\n",
        "      label_col:str e.g. 'label'\n",
        "      txt_cols: list of text columns the clfs learned from e.g.txt_cols = ['title_raw', 'hard_skills_name', 'title_raw+hard_skills_name']\n",
        "\n",
        "    Returns: a list of fastai tabular classifiers with \n",
        "    - tab_clf_embs is the classifier trained with txt_col embeddings\n",
        "    - tab_clf_probs is the classifier trained with probability predictions by clfs\n",
        "      [tab_clf_embs, tab_clf_probs] \n",
        "    \n",
        "    \"\"\"\n",
        "    # make sure txt_cols in df is string\n",
        "    for col in txt_cols:\n",
        "      df[col].fillna('', inplace=True) # replace NaN by ''\n",
        "      df[col] = df[col].astype(str) # convert dtype from object to str\n",
        "\n",
        "    # get docs embeddings list\n",
        "    try:\n",
        "      import pickle\n",
        "      with open(\"embs_ls.pickle\",\"rb\") as f:\n",
        "        embs_ls = pickle.load(f)\n",
        "    except:\n",
        "      embs_ls = []\n",
        "      for (txt_col, clf, lm) in zip(txt_cols, clfs, lms):\n",
        "          embs = get_fastai_docs_embs(docs=df[txt_col], learn=clf, lm=lm, df=None, txt_col=None)\n",
        "      embs_ls.append(embs)\n",
        "\n",
        "      #pickle embs_ls\n",
        "      import pickle\n",
        "      with open(\"embs_ls.pickle\",\"wb\") as f:\n",
        "        pickle.dump(embs_ls,f)\n",
        "\n",
        "\n",
        "    # train a ensemble classifier using doc embedding\n",
        "    tab_clf_embs = train_fastai_tabular_classifier_fr_embs_ls(embs_ls, df=df, label_col=label_col)  \n",
        "\n",
        "    # get prediction on labels and probabilities\n",
        "    try:\n",
        "      with open(\"probs_ls.pickle\", \"rb\") as f:\n",
        "        probs_ls = pickle.load(f)\n",
        "      #probs_ls = probs_ls #load_pickled_objs(object_names=['probs_ls'])\n",
        "    except:\n",
        "      probs_ls = []\n",
        "      for (clf, txt_col) in zip(clfs, txt_cols):\n",
        "          _, probs = fastai_learner_preds(learner=clf, df=df, txt_col=txt_col)\n",
        "          probs_ls.append(probs.numpy())\n",
        "\n",
        "      #pickle probs_ls\n",
        "      with open(\"probs_ls.pickle\",\"wb\") as f:\n",
        "        pickle.dump(probs_ls,f)\n",
        "\n",
        "    # train a ensemble classifier using probs prediction\n",
        "    tab_clf_probs = train_fastai_tabular_classifier_fr_embs_ls(probs_ls, df=df, label_col=label_col) \n",
        "\n",
        "    return [tab_clf_embs, tab_clf_probs] \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfQDrkxikl-h"
      },
      "outputs": [],
      "source": [
        "#dbck\n",
        "tab_learn0, tab_learn1 = train_ensembled_classifier_by_fastai_tabular(clfs=[clf0, clf1], lms=[lm0, lm1], txt_cols=txt_cols[:2], df=df, label_col=label_col) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbG6mA5JhCPH"
      },
      "source": [
        "## module: text classifier for multiple text columns\n",
        "\n",
        "The end2end ensemble classifier combines multiple txt_clfs and tab_clfs:\n",
        "- inputs: df[txt_cols]\n",
        "- outputs: preds, probs\n",
        "\n",
        "df[txt_col]=>`txt_clfs`=>emb_ls|probs_ls=>`tab_clfs`=>preds, probs\n",
        "\n",
        "\n",
        "**Why it matters**\n",
        "\n",
        "- ensemble of multiple txt classifiers combine signal from multiple txt_cols, therefore possibly improve classification performance\n",
        "\n",
        "- this will be a template for fastai multimodal classification\n",
        "  - tab_clf =>tab_embs|probs\n",
        "  - img_clf =>img_embs|probs\n",
        "  - txt_clf =>txt_embs|probs\n",
        "Then end2end ensemble classifier can combine all 3 together.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1iorTEFgVN8"
      },
      "outputs": [],
      "source": [
        "class End2End_Fastai_Texts_Classifier():\n",
        "  \"\"\"end to end fastai classifier for multiple txt_cols\"\"\"\n",
        "  def __init__(self,txt_clfs=None, lms=None, tab_clfs=None):\n",
        "    self.txt_clfs = txt_clfs # a list of fastai text classifiers\n",
        "    self.lms = lms # a list of fastai text language models\n",
        "    self.tab_clfs = tab_clfs # a list of fastai tabular classifiers\n",
        "  \n",
        "  \n",
        "  def fit(self, df:pd.DataFrame, label_col:str, txt_cols:list):\n",
        "    \"\"\"fit multiple fastai text classifiers for each col in txt_cols\n",
        "    Args:\n",
        "      df:pd.DataFrame, containing both label_col and txt_cols \n",
        "      label_col:str, e.g. 'label'\n",
        "      txt_cols:list, e.g. ['title_raw',\t'hard_skills_name',\t'title_raw+hard_skills_name']\n",
        "    Returns:\n",
        "      None (but update self.txt_clfs, self.lms, self.tab_clfs)\n",
        "    \"\"\"\n",
        "    # in case self.txt_clfs = None, make txt_clfs from stratch\n",
        "    if self.txt_clfs is None:\n",
        "      txt_clfs = []\n",
        "      lms = []\n",
        "      for txt_col in txt_cols:\n",
        "          lm, txt_clf = train_fastai_text_classifier(df, \n",
        "                                        txt_col=txt_col,\n",
        "                                        label_col=label_col,\n",
        "                                        model_path='/content/drive/My Drive/techskills/model/',\n",
        "                                        flag_auto_lr=False\n",
        "                                        )\n",
        "          txt_clfs.append(txt_clf)\n",
        "          lms.append(lm)\n",
        "      self.txt_clfs = txt_clfs\n",
        "      self.lms = lms\n",
        "\n",
        "    # in case self.tab_clfs = None, make tab_clfs from stratch\n",
        "    if self.tab_clfs is None:\n",
        "      tab_clfs = train_ensembled_classifier_by_fastai_tabular(self.txt_clfs, self.lms, df, label_col, txt_cols)\n",
        "      self.tab_clfs = tab_clfs\n",
        "\n",
        "  def get_preds(self, df:pd.DataFrame, txt_cols:list):\n",
        "    \"\"\"get predictions for test data df[txt_cols]\n",
        "    Args:\n",
        "      df:pd.DataFrame, txt_cols:list\n",
        "    Returns:\n",
        "      (preds0, preds1), (probs0, probs1), where\n",
        "      - preds0, probs0: the predictions on test data based on txt_clfs embeddings extraction embs_ls\n",
        "      - preds1, probs1: the prediction on test data based on txt_clfs output probs_ls\n",
        "    \n",
        "    \"\"\"\n",
        "    # make sure txt_cols in df is string\n",
        "    for col in txt_cols:\n",
        "      df[col].fillna('', inplace=True) # replace NaN by ''\n",
        "      df[col] = df[col].astype(str) # convert dtype from object to str\n",
        "\n",
        "    # get docs embeddings list\n",
        "    embs_ls = []\n",
        "    for (txt_col, clf, lm) in zip(txt_cols, self.txt_clfs, self.lms):\n",
        "        embs = get_fastai_docs_embs(docs=df[txt_col], learn=clf, lm=lm, df=None, txt_col=None)\n",
        "        embs_ls.append(embs)\n",
        "    \n",
        "    # get prediction on labels and probabilities\n",
        "    probs_ls = []\n",
        "    for (clf, txt_col) in zip(self.txt_clfs, txt_cols):\n",
        "        _, probs = fastai_learner_preds(learner=clf, df=df, txt_col=txt_col)\n",
        "        probs_ls.append(probs.numpy())\n",
        "\n",
        "    # tab_clfs[0] make predictions on embs_ls\n",
        "    df_ = pd.concat([pd.DataFrame(embs) for embs in embs_ls], axis=1)\n",
        "    df_.columns = list(range(df_.shape[1]))\n",
        "    test_dl = self.tab_clfs[0].dls.test_dl(df_) #, with_labels=True\n",
        "    probs0, _ = self.tab_clfs[0].get_preds(dl=test_dl)\n",
        "    preds0 = probs0.numpy().argmax(axis=1)\n",
        "\n",
        "    # tab_clfs[1] make predictions on probs_ls\n",
        "    df_ = pd.concat([pd.DataFrame(embs) for probs in probs_ls], axis=1)\n",
        "    df_.columns = list(range(df_.shape[1]))\n",
        "\n",
        "    test_dl = self.tab_clfs[1].dls.test_dl(df_) #, with_labels=True\n",
        "    probs1, _ = self.tab_clfs[1].get_preds(dl=test_dl)\n",
        "    preds1 = probs0.numpy().argmax(axis=1)\n",
        "    \n",
        "    return (preds0, preds1), (probs0, probs1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "MBJAB7XSHvTO",
        "outputId": "9ab8cd6f-fd63-4ed1-e51e-635d54929fb6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "((array([1, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
              "  array([1, 0, 0, 0, 0, 0, 1, 0, 0, 0])),\n",
              " (tensor([[2.3049e-02, 9.7695e-01],\n",
              "          [9.4323e-01, 5.6766e-02],\n",
              "          [9.9966e-01, 3.3622e-04],\n",
              "          [8.7000e-01, 1.3000e-01],\n",
              "          [9.9992e-01, 7.5751e-05],\n",
              "          [9.9978e-01, 2.2153e-04],\n",
              "          [1.5772e-02, 9.8423e-01],\n",
              "          [9.7450e-01, 2.5504e-02],\n",
              "          [9.9691e-01, 3.0925e-03],\n",
              "          [9.9855e-01, 1.4542e-03]]), tensor([[1.0000e+00, 0.0000e+00],\n",
              "          [1.0000e+00, 0.0000e+00],\n",
              "          [1.0000e+00, 4.9072e-07],\n",
              "          [1.0000e+00, 0.0000e+00],\n",
              "          [1.0000e+00, 0.0000e+00],\n",
              "          [1.0000e+00, 0.0000e+00],\n",
              "          [1.0000e+00, 0.0000e+00],\n",
              "          [1.0000e+00, 0.0000e+00],\n",
              "          [1.0000e+00, 0.0000e+00],\n",
              "          [1.0000e+00, 0.0000e+00]])))"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#dbck    \n",
        "n2n_clf = End2End_Fastai_Texts_Classifier(txt_clfs=[clf0, clf1], lms=[lm0, lm1], tab_clfs=[tab_learn0, tab_learn1])\n",
        "df_tmp = df.head(10).copy()\n",
        "n2n_clf.get_preds(df=df_tmp, txt_cols=txt_cols[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEPPOWOYgC3C"
      },
      "source": [
        "### experiment: extract fastai tabular embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB6bqkFQSh6i",
        "outputId": "3ec5a52d-4c79-484f-f4e0-bdc72d0ab49d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   age          workclass  fnlwgt     education  education-num  \\\n",
            "0   49            Private  101320    Assoc-acdm           12.0   \n",
            "1   44            Private  236746       Masters           14.0   \n",
            "2   38            Private   96185       HS-grad            NaN   \n",
            "3   38       Self-emp-inc  112847   Prof-school           15.0   \n",
            "4   42   Self-emp-not-inc   82297       7th-8th            NaN   \n",
            "\n",
            "        marital-status        occupation    relationship                 race  \\\n",
            "0   Married-civ-spouse               NaN            Wife                White   \n",
            "1             Divorced   Exec-managerial   Not-in-family                White   \n",
            "2             Divorced               NaN       Unmarried                Black   \n",
            "3   Married-civ-spouse    Prof-specialty         Husband   Asian-Pac-Islander   \n",
            "4   Married-civ-spouse     Other-service            Wife                Black   \n",
            "\n",
            "       sex  capital-gain  capital-loss  hours-per-week  native-country salary  \n",
            "0   Female             0          1902              40   United-States  >=50k  \n",
            "1     Male         10520             0              45   United-States  >=50k  \n",
            "2   Female             0             0              32   United-States   <50k  \n",
            "3     Male             0             0              40   United-States  >=50k  \n",
            "4   Female             0             0              50   United-States   <50k  \n",
            "before adjustment...cnt_card=0.5\n",
            "before adjustment...cnt_card=16280\n",
            "before adjustment...txt_card=0.5\n",
            "before adjustment...txt_card=16280\n",
            "===== automatically identify\n",
            " cnt_cols=['education-num', 'fnlwgt']\n",
            " cat_cols=['age', 'capital-gain', 'capital-loss', 'education', 'hours-per-week', 'marital-status', 'native-country', 'occupation', 'race', 'relationship', 'sex', 'workclass'],\n",
            " img_cols=[], \n",
            " txt_cols=[] ======\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319, 3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329, 3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355, 3356, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541, 3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552, 3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596, 3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607, 3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618, 3619, 3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629, 3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3640, 3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649, 3650, 3651, 3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673, 3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689, 3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701, 3702, 3703, 3704, 3705, 3706, 3707, 3708, 3709, 3710, 3711, 3712, 3713, 3714, 3715, 3716, 3717, 3718, 3719, 3720, 3721, 3722, 3723, 3724, 3725, 3726, 3727, 3728, 3729, 3730, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 3776, 3777, 3778, 3779, 3780, 3781, 3782, 3783, 3784, 3785, 3786, 3787, 3788, 3789, 3790, 3791, 3792, 3793, 3794, 3795, 3796, 3797, 3798, 3799, 3800, 3801, 3802, 3803, 3804, 3805, 3806, 3807, 3808, 3809, 3810, 3811, 3812, 3813, 3814, 3815, 3816, 3817, 3818, 3819, 3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3833, 3834, 3835, 3836, 3837, 3838, 3839, 3840, 3841, 3842, 3843, 3844, 3845, 3846, 3847, 3848, 3849, 3850, 3851, 3852, 3853, 3854, 3855, 3856, 3857, 3858, 3859, 3860, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879, 3880, 3881, 3882, 3883, 3884, 3885, 3886, 3887, 3888, 3889, 3890, 3891, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899, 3900, 3901, 3902, 3903, 3904, 3905, 3906, 3907, 3908, 3909, 3910, 3911, 3912, 3913, 3914, 3915, 3916, 3917, 3918, 3919, 3920, 3921, 3922, 3923, 3924, 3925, 3926, 3927, 3928, 3929, 3930, 3931, 3932, 3933, 3934, 3935, 3936, 3937, 3938, 3939, 3940, 3941, 3942, 3943, 3944, 3945, 3946, 3947, 3948, 3949, 3950, 3951, 3952, 3953, 3954, 3955, 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 3964, 3965, 3966, 3967, 3968, 3969, 3970, 3971, 3972, 3973, 3974, 3975, 3976, 3977, 3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 3988, 3989, 3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999, 4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019, 4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031, 4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044, 4045, 4046, 4047, 4048, 4049, 4050, 4051, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4059, 4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067, 4068, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077, 4078, 4079, 4080, 4081, 4082, 4083, 4084, 4085, 4086, 4087, 4088, 4089, 4090, 4091, 4092, 4093, 4094, 4095, 4096, 4097, 4098, 4099, 4100, 4101, 4102, 4103, 4104, 4105, 4106, 4107, 4108, 4109, 4110, 4111, 4112, 4113, 4114, 4115, 4116, 4117, 4118, 4119, 4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4130, 4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139, 4140, 4141, 4142, 4143, 4144, 4145, 4146, 4147, 4148, 4149, 4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157, 4158, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166, 4167, 4168, 4169, 4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179, 4180, 4181, 4182, 4183, 4184, 4185, 4186, 4187, 4188, 4189, 4190, 4191, 4192, 4193, 4194, 4195, 4196, 4197, 4198, 4199, 4200, 4201, 4202, 4203, 4204, 4205, 4206, 4207, 4208, 4209, 4210, 4211, 4212, 4213, 4214, 4215, 4216, 4217, 4218, 4219, 4220, 4221, 4222, 4223, 4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4239, 4240, 4241, 4242, 4243, 4244, 4245, 4246, 4247, 4248, 4249, 4250, 4251, 4252, 4253, 4254, 4255, 4256, 4257, 4258, 4259, 4260, 4261, 4262, 4263, 4264, 4265, 4266, 4267, 4268, 4269, 4270, 4271, 4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281, 4282, 4283, 4284, 4285, 4286, 4287, 4288, 4289, 4290, 4291, 4292, 4293, 4294, 4295, 4296, 4297, 4298, 4299, 4300, 4301, 4302, 4303, 4304, 4305, 4306, 4307, 4308, 4309, 4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319, 4320, 4321, 4322, 4323, 4324, 4325, 4326, 4327, 4328, 4329, 4330, 4331, 4332, 4333, 4334, 4335, 4336, 4337, 4338, 4339, 4340, 4341, 4342, 4343, 4344, 4345, 4346, 4347, 4348, 4349, 4350, 4351, 4352, 4353, 4354, 4355, 4356, 4357, 4358, 4359, 4360, 4361, 4362, 4363, 4364, 4365, 4366, 4367, 4368, 4369, 4370, 4371, 4372, 4373, 4374, 4375, 4376, 4377, 4378, 4379, 4380, 4381, 4382, 4383, 4384, 4385, 4386, 4387, 4388, 4389, 4390, 4391, 4392, 4393, 4394, 4395, 4396, 4397, 4398, 4399, 4400, 4401, 4402, 4403, 4404, 4405, 4406, 4407, 4408, 4409, 4410, 4411, 4412, 4413, 4414, 4415, 4416, 4417, 4418, 4419, 4420, 4421, 4422, 4423, 4424, 4425, 4426, 4427, 4428, 4429, 4430, 4431, 4432, 4433, 4434, 4435, 4436, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444, 4445, 4446, 4447, 4448, 4449, 4450, 4451, 4452, 4453, 4454, 4455, 4456, 4457, 4458, 4459, 4460, 4461, 4462, 4463, 4464, 4465, 4466, 4467, 4468, 4469, 4470, 4471, 4472, 4473, 4474, 4475, 4476, 4477, 4478, 4479, 4480, 4481, 4482, 4483, 4484, 4485, 4486, 4487, 4488, 4489, 4490, 4491, 4492, 4493, 4494, 4495, 4496, 4497, 4498, 4499, 4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509, 4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4521, 4522, 4523, 4524, 4525, 4526, 4527, 4528, 4529, 4530, 4531, 4532, 4533, 4534, 4535, 4536, 4537, 4538, 4539, 4540, 4541, 4542, 4543, 4544, 4545, 4546, 4547, 4548, 4549, 4550, 4551, 4552, 4553, 4554, 4555, 4556, 4557, 4558, 4559, 4560, 4561, 4562, 4563, 4564, 4565, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4574, 4575, 4576, 4577, 4578, 4579, 4580, 4581, 4582, 4583, 4584, 4585, 4586, 4587, 4588, 4589, 4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598, 4599, 4600, 4601, 4602, 4603, 4604, 4605, 4606, 4607, 4608, 4609, 4610, 4611, 4612, 4613, 4614, 4615, 4616, 4617, 4618, 4619, 4620, 4621, 4622, 4623, 4624, 4625, 4626, 4627, 4628, 4629, 4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4640, 4641, 4642, 4643, 4644, 4645, 4646, 4647, 4648, 4649, 4650, 4651, 4652, 4653, 4654, 4655, 4656, 4657, 4658, 4659, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669, 4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4678, 4679, 4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689, 4690, 4691, 4692, 4693, 4694, 4695, 4696, 4697, 4698, 4699, 4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4708, 4709, 4710, 4711, 4712, 4713, 4714, 4715, 4716, 4717, 4718, 4719, 4720, 4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4729, 4730, 4731, 4732, 4733, 4734, 4735, 4736, 4737, 4738, 4739, 4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4756, 4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769, 4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4809, 4810, 4811, 4812, 4813, 4814, 4815, 4816, 4817, 4818, 4819, 4820, 4821, 4822, 4823, 4824, 4825, 4826, 4827, 4828, 4829, 4830, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838, 4839, 4840, 4841, 4842, 4843, 4844, 4845, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866, 4867, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4878, 4879, 4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887, 4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 4896, 4897, 4898, 4899, 4900, 4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956, 4957, 4958, 4959, 4960, 4961, 4962, 4963, 4964, 4965, 4966, 4967, 4968, 4969, 4970, 4971, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979, 4980, 4981, 4982, 4983, 4984, 4985, 4986, 4987, 4988, 4989, 4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999, 5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019, 5020, 5021, 5022, 5023, 5024, 5025, 5026, 5027, 5028, 5029, 5030, 5031, 5032, 5033, 5034, 5035, 5036, 5037, 5038, 5039, 5040, 5041, 5042, 5043, 5044, 5045, 5046, 5047, 5048, 5049, 5050, 5051, 5052, 5053, 5054, 5055, 5056, 5057, 5058, 5059, 5060, 5061, 5062, 5063, 5064, 5065, 5066, 5067, 5068, 5069, 5070, 5071, 5072, 5073, 5074, 5075, 5076, 5077, 5078, 5079, 5080, 5081, 5082, 5083, 5084, 5085, 5086, 5087, 5088, 5089, 5090, 5091, 5092, 5093, 5094, 5095, 5096, 5097, 5098, 5099, 5100, 5101, 5102, 5103, 5104, 5105, 5106, 5107, 5108, 5109, 5110, 5111, 5112, 5113, 5114, 5115, 5116, 5117, 5118, 5119, 5120, 5121, 5122, 5123, 5124, 5125, 5126, 5127, 5128, 5129, 5130, 5131, 5132, 5133, 5134, 5135, 5136, 5137, 5138, 5139, 5140, 5141, 5142, 5143, 5144, 5145, 5146, 5147, 5148, 5149, 5150, 5151, 5152, 5153, 5154, 5155, 5156, 5157, 5158, 5159, 5160, 5161, 5162, 5163, 5164, 5165, 5166, 5167, 5168, 5169, 5170, 5171, 5172, 5173, 5174, 5175, 5176, 5177, 5178, 5179, 5180, 5181, 5182, 5183, 5184, 5185, 5186, 5187, 5188, 5189, 5190, 5191, 5192, 5193, 5194, 5195, 5196, 5197, 5198, 5199, 5200, 5201, 5202, 5203, 5204, 5205, 5206, 5207, 5208, 5209, 5210, 5211, 5212, 5213, 5214, 5215, 5216, 5217, 5218, 5219, 5220, 5221, 5222, 5223, 5224, 5225, 5226, 5227, 5228, 5229, 5230, 5231, 5232, 5233, 5234, 5235, 5236, 5237, 5238, 5239, 5240, 5241, 5242, 5243, 5244, 5245, 5246, 5247, 5248, 5249, 5250, 5251, 5252, 5253, 5254, 5255, 5256, 5257, 5258, 5259, 5260, 5261, 5262, 5263, 5264, 5265, 5266, 5267, 5268, 5269, 5270, 5271, 5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281, 5282, 5283, 5284, 5285, 5286, 5287, 5288, 5289, 5290, 5291, 5292, 5293, 5294, 5295, 5296, 5297, 5298, 5299, 5300, 5301, 5302, 5303, 5304, 5305, 5306, 5307, 5308, 5309, 5310, 5311, 5312, 5313, 5314, 5315, 5316, 5317, 5318, 5319, 5320, 5321, 5322, 5323, 5324, 5325, 5326, 5327, 5328, 5329, 5330, 5331, 5332, 5333, 5334, 5335, 5336, 5337, 5338, 5339, 5340, 5341, 5342, 5343, 5344, 5345, 5346, 5347, 5348, 5349, 5350, 5351, 5352, 5353, 5354, 5355, 5356, 5357, 5358, 5359, 5360, 5361, 5362, 5363, 5364, 5365, 5366, 5367, 5368, 5369, 5370, 5371, 5372, 5373, 5374, 5375, 5376, 5377, 5378, 5379, 5380, 5381, 5382, 5383, 5384, 5385, 5386, 5387, 5388, 5389, 5390, 5391, 5392, 5393, 5394, 5395, 5396, 5397, 5398, 5399, 5400, 5401, 5402, 5403, 5404, 5405, 5406, 5407, 5408, 5409, 5410, 5411, 5412, 5413, 5414, 5415, 5416, 5417, 5418, 5419, 5420, 5421, 5422, 5423, 5424, 5425, 5426, 5427, 5428, 5429, 5430, 5431, 5432, 5433, 5434, 5435, 5436, 5437, 5438, 5439, 5440, 5441, 5442, 5443, 5444, 5445, 5446, 5447, 5448, 5449, 5450, 5451, 5452, 5453, 5454, 5455, 5456, 5457, 5458, 5459, 5460, 5461, 5462, 5463, 5464, 5465, 5466, 5467, 5468, 5469, 5470, 5471, 5472, 5473, 5474, 5475, 5476, 5477, 5478, 5479, 5480, 5481, 5482, 5483, 5484, 5485, 5486, 5487, 5488, 5489, 5490, 5491, 5492, 5493, 5494, 5495, 5496, 5497, 5498, 5499, 5500, 5501, 5502, 5503, 5504, 5505, 5506, 5507, 5508, 5509, 5510, 5511, 5512, 5513, 5514, 5515, 5516, 5517, 5518, 5519, 5520, 5521, 5522, 5523, 5524, 5525, 5526, 5527, 5528, 5529, 5530, 5531, 5532, 5533, 5534, 5535, 5536, 5537, 5538, 5539, 5540, 5541, 5542, 5543, 5544, 5545, 5546, 5547, 5548, 5549, 5550, 5551, 5552, 5553, 5554, 5555, 5556, 5557, 5558, 5559, 5560, 5561, 5562, 5563, 5564, 5565, 5566, 5567, 5568, 5569, 5570, 5571, 5572, 5573, 5574, 5575, 5576, 5577, 5578, 5579, 5580, 5581, 5582, 5583, 5584, 5585, 5586, 5587, 5588, 5589, 5590, 5591, 5592, 5593, 5594, 5595, 5596, 5597, 5598, 5599, 5600, 5601, 5602, 5603, 5604, 5605, 5606, 5607, 5608, 5609, 5610, 5611, 5612, 5613, 5614, 5615, 5616, 5617, 5618, 5619, 5620, 5621, 5622, 5623, 5624, 5625, 5626, 5627, 5628, 5629, 5630, 5631, 5632, 5633, 5634, 5635, 5636, 5637, 5638, 5639, 5640, 5641, 5642, 5643, 5644, 5645, 5646, 5647, 5648, 5649, 5650, 5651, 5652, 5653, 5654, 5655, 5656, 5657, 5658, 5659, 5660, 5661, 5662, 5663, 5664, 5665, 5666, 5667, 5668, 5669, 5670, 5671, 5672, 5673, 5674, 5675, 5676, 5677, 5678, 5679, 5680, 5681, 5682, 5683, 5684, 5685, 5686, 5687, 5688, 5689, 5690, 5691, 5692, 5693, 5694, 5695, 5696, 5697, 5698, 5699, 5700, 5701, 5702, 5703, 5704, 5705, 5706, 5707, 5708, 5709, 5710, 5711, 5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719, 5720, 5721, 5722, 5723, 5724, 5725, 5726, 5727, 5728, 5729, 5730, 5731, 5732, 5733, 5734, 5735, 5736, 5737, 5738, 5739, 5740, 5741, 5742, 5743, 5744, 5745, 5746, 5747, 5748, 5749, 5750, 5751, 5752, 5753, 5754, 5755, 5756, 5757, 5758, 5759, 5760, 5761, 5762, 5763, 5764, 5765, 5766, 5767, 5768, 5769, 5770, 5771, 5772, 5773, 5774, 5775, 5776, 5777, 5778, 5779, 5780, 5781, 5782, 5783, 5784, 5785, 5786, 5787, 5788, 5789, 5790, 5791, 5792, 5793, 5794, 5795, 5796, 5797, 5798, 5799, 5800, 5801, 5802, 5803, 5804, 5805, 5806, 5807, 5808, 5809, 5810, 5811, 5812, 5813, 5814, 5815, 5816, 5817, 5818, 5819, 5820, 5821, 5822, 5823, 5824, 5825, 5826, 5827, 5828, 5829, 5830, 5831, 5832, 5833, 5834, 5835, 5836, 5837, 5838, 5839, 5840, 5841, 5842, 5843, 5844, 5845, 5846, 5847, 5848, 5849, 5850, 5851, 5852, 5853, 5854, 5855, 5856, 5857, 5858, 5859, 5860, 5861, 5862, 5863, 5864, 5865, 5866, 5867, 5868, 5869, 5870, 5871, 5872, 5873, 5874, 5875, 5876, 5877, 5878, 5879, 5880, 5881, 5882, 5883, 5884, 5885, 5886, 5887, 5888, 5889, 5890, 5891, 5892, 5893, 5894, 5895, 5896, 5897, 5898, 5899, 5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907, 5908, 5909, 5910, 5911, 5912, 5913, 5914, 5915, 5916, 5917, 5918, 5919, 5920, 5921, 5922, 5923, 5924, 5925, 5926, 5927, 5928, 5929, 5930, 5931, 5932, 5933, 5934, 5935, 5936, 5937, 5938, 5939, 5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951, 5952, 5953, 5954, 5955, 5956, 5957, 5958, 5959, 5960, 5961, 5962, 5963, 5964, 5965, 5966, 5967, 5968, 5969, 5970, 5971, 5972, 5973, 5974, 5975, 5976, 5977, 5978, 5979, 5980, 5981, 5982, 5983, 5984, 5985, 5986, 5987, 5988, 5989, 5990, 5991, 5992, 5993, 5994, 5995, 5996, 5997, 5998, 5999, 6000, 6001, 6002, 6003, 6004, 6005, 6006, 6007, 6008, 6009, 6010, 6011, 6012, 6013, 6014, 6015, 6016, 6017, 6018, 6019, 6020, 6021, 6022, 6023, 6024, 6025, 6026, 6027, 6028, 6029, 6030, 6031, 6032, 6033, 6034, 6035, 6036, 6037, 6038, 6039, 6040, 6041, 6042, 6043, 6044, 6045, 6046, 6047, 6048, 6049, 6050, 6051, 6052, 6053, 6054, 6055, 6056, 6057, 6058, 6059, 6060, 6061, 6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069, 6070, 6071, 6072, 6073, 6074, 6075, 6076, 6077, 6078, 6079, 6080, 6081, 6082, 6083, 6084, 6085, 6086, 6087, 6088, 6089, 6090, 6091, 6092, 6093, 6094, 6095, 6096, 6097, 6098, 6099, 6100, 6101, 6102, 6103, 6104, 6105, 6106, 6107, 6108, 6109, 6110, 6111, 6112, 6113, 6114, 6115, 6116, 6117, 6118, 6119, 6120, 6121, 6122, 6123, 6124, 6125, 6126, 6127, 6128, 6129, 6130, 6131, 6132, 6133, 6134, 6135, 6136, 6137, 6138, 6139, 6140, 6141, 6142, 6143, 6144, 6145, 6146, 6147, 6148, 6149, 6150, 6151, 6152, 6153, 6154, 6155, 6156, 6157, 6158, 6159, 6160, 6161, 6162, 6163, 6164, 6165, 6166, 6167, 6168, 6169, 6170, 6171, 6172, 6173, 6174, 6175, 6176, 6177, 6178, 6179, 6180, 6181, 6182, 6183, 6184, 6185, 6186, 6187, 6188, 6189, 6190, 6191, 6192, 6193, 6194, 6195, 6196, 6197, 6198, 6199, 6200, 6201, 6202, 6203, 6204, 6205, 6206, 6207, 6208, 6209, 6210, 6211, 6212, 6213, 6214, 6215, 6216, 6217, 6218, 6219, 6220, 6221, 6222, 6223, 6224, 6225, 6226, 6227, 6228, 6229, 6230, 6231, 6232, 6233, 6234, 6235, 6236, 6237, 6238, 6239, 6240, 6241, 6242, 6243, 6244, 6245, 6246, 6247, 6248, 6249, 6250, 6251, 6252, 6253, 6254, 6255, 6256, 6257, 6258, 6259, 6260, 6261, 6262, 6263, 6264, 6265, 6266, 6267, 6268, 6269, 6270, 6271, 6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281, 6282, 6283, 6284, 6285, 6286, 6287, 6288, 6289, 6290, 6291, 6292, 6293, 6294, 6295, 6296, 6297, 6298, 6299, 6300, 6301, 6302, 6303, 6304, 6305, 6306, 6307, 6308, 6309, 6310, 6311, 6312, 6313, 6314, 6315, 6316, 6317, 6318, 6319, 6320, 6321, 6322, 6323, 6324, 6325, 6326, 6327, 6328, 6329, 6330, 6331, 6332, 6333, 6334, 6335, 6336, 6337, 6338, 6339, 6340, 6341, 6342, 6343, 6344, 6345, 6346, 6347, 6348, 6349, 6350, 6351, 6352, 6353, 6354, 6355, 6356, 6357, 6358, 6359, 6360, 6361, 6362, 6363, 6364, 6365, 6366, 6367, 6368, 6369, 6370, 6371, 6372, 6373, 6374, 6375, 6376, 6377, 6378, 6379, 6380, 6381, 6382, 6383, 6384, 6385, 6386, 6387, 6388, 6389, 6390, 6391, 6392, 6393, 6394, 6395, 6396, 6397, 6398, 6399, 6400, 6401, 6402, 6403, 6404, 6405, 6406, 6407, 6408, 6409, 6410, 6411, 6412, 6413, 6414, 6415, 6416, 6417, 6418, 6419, 6420, 6421, 6422, 6423, 6424, 6425, 6426, 6427, 6428, 6429, 6430, 6431, 6432, 6433, 6434, 6435, 6436, 6437, 6438, 6439, 6440, 6441, 6442, 6443, 6444, 6445, 6446, 6447, 6448, 6449, 6450, 6451, 6452, 6453, 6454, 6455, 6456, 6457, 6458, 6459, 6460, 6461, 6462, 6463, 6464, 6465, 6466, 6467, 6468, 6469, 6470, 6471, 6472, 6473, 6474, 6475, 6476, 6477, 6478, 6479, 6480, 6481, 6482, 6483, 6484, 6485, 6486, 6487, 6488, 6489, 6490, 6491, 6492, 6493, 6494, 6495, 6496, 6497, 6498, 6499, 6500, 6501, 6502, 6503, 6504, 6505, 6506, 6507, 6508, 6509, 6510, 6511, 6512, 6513, 6514, 6515, 6516, 6517, 6518, 6519, 6520, 6521, 6522, 6523, 6524, 6525, 6526, 6527, 6528, 6529, 6530, 6531, 6532, 6533, 6534, 6535, 6536, 6537, 6538, 6539, 6540, 6541, 6542, 6543, 6544, 6545, 6546, 6547, 6548, 6549, 6550, 6551, 6552, 6553, 6554, 6555, 6556, 6557, 6558, 6559, 6560, 6561, 6562, 6563, 6564, 6565, 6566, 6567, 6568, 6569, 6570, 6571, 6572, 6573, 6574, 6575, 6576, 6577, 6578, 6579, 6580, 6581, 6582, 6583, 6584, 6585, 6586, 6587, 6588, 6589, 6590, 6591, 6592, 6593, 6594, 6595, 6596, 6597, 6598, 6599, 6600, 6601, 6602, 6603, 6604, 6605, 6606, 6607, 6608, 6609, 6610, 6611, 6612, 6613, 6614, 6615, 6616, 6617, 6618, 6619, 6620, 6621, 6622, 6623, 6624, 6625, 6626, 6627, 6628, 6629, 6630, 6631, 6632, 6633, 6634, 6635, 6636, 6637, 6638, 6639, 6640, 6641, 6642, 6643, 6644, 6645, 6646, 6647, 6648, 6649, 6650, 6651, 6652, 6653, 6654, 6655, 6656, 6657, 6658, 6659, 6660, 6661, 6662, 6663, 6664, 6665, 6666, 6667, 6668, 6669, 6670, 6671, 6672, 6673, 6674, 6675, 6676, 6677, 6678, 6679, 6680, 6681, 6682, 6683, 6684, 6685, 6686, 6687, 6688, 6689, 6690, 6691, 6692, 6693, 6694, 6695, 6696, 6697, 6698, 6699, 6700, 6701, 6702, 6703, 6704, 6705, 6706, 6707, 6708, 6709, 6710, 6711, 6712, 6713, 6714, 6715, 6716, 6717, 6718, 6719, 6720, 6721, 6722, 6723, 6724, 6725, 6726, 6727, 6728, 6729, 6730, 6731, 6732, 6733, 6734, 6735, 6736, 6737, 6738, 6739, 6740, 6741, 6742, 6743, 6744, 6745, 6746, 6747, 6748, 6749, 6750, 6751, 6752, 6753, 6754, 6755, 6756, 6757, 6758, 6759, 6760, 6761, 6762, 6763, 6764, 6765, 6766, 6767, 6768, 6769, 6770, 6771, 6772, 6773, 6774, 6775, 6776, 6777, 6778, 6779, 6780, 6781, 6782, 6783, 6784, 6785, 6786, 6787, 6788, 6789, 6790, 6791, 6792, 6793, 6794, 6795, 6796, 6797, 6798, 6799, 6800, 6801, 6802, 6803, 6804, 6805, 6806, 6807, 6808, 6809, 6810, 6811, 6812, 6813, 6814, 6815, 6816, 6817, 6818, 6819, 6820, 6821, 6822, 6823, 6824, 6825, 6826, 6827, 6828, 6829, 6830, 6831, 6832, 6833, 6834, 6835, 6836, 6837, 6838, 6839, 6840, 6841, 6842, 6843, 6844, 6845, 6846, 6847, 6848, 6849, 6850, 6851, 6852, 6853, 6854, 6855, 6856, 6857, 6858, 6859, 6860, 6861, 6862, 6863, 6864, 6865, 6866, 6867, 6868, 6869, 6870, 6871, 6872, 6873, 6874, 6875, 6876, 6877, 6878, 6879, 6880, 6881, 6882, 6883, 6884, 6885, 6886, 6887, 6888, 6889, 6890, 6891, 6892, 6893, 6894, 6895, 6896, 6897, 6898, 6899, 6900, 6901, 6902, 6903, 6904, 6905, 6906, 6907, 6908, 6909, 6910, 6911, 6912, 6913, 6914, 6915, 6916, 6917, 6918, 6919, 6920, 6921, 6922, 6923, 6924, 6925, 6926, 6927, 6928, 6929, 6930, 6931, 6932, 6933, 6934, 6935, 6936, 6937, 6938, 6939, 6940, 6941, 6942, 6943, 6944, 6945, 6946, 6947, 6948, 6949, 6950, 6951, 6952, 6953, 6954, 6955, 6956, 6957, 6958, 6959, 6960, 6961, 6962, 6963, 6964, 6965, 6966, 6967, 6968, 6969, 6970, 6971, 6972, 6973, 6974, 6975, 6976, 6977, 6978, 6979, 6980, 6981, 6982, 6983, 6984, 6985, 6986, 6987, 6988, 6989, 6990, 6991, 6992, 6993, 6994, 6995, 6996, 6997, 6998, 6999, 7000, 7001, 7002, 7003, 7004, 7005, 7006, 7007, 7008, 7009, 7010, 7011, 7012, 7013, 7014, 7015, 7016, 7017, 7018, 7019, 7020, 7021, 7022, 7023, 7024, 7025, 7026, 7027, 7028, 7029, 7030, 7031, 7032, 7033, 7034, 7035, 7036, 7037, 7038, 7039, 7040, 7041, 7042, 7043, 7044, 7045, 7046, 7047, 7048, 7049, 7050, 7051, 7052, 7053, 7054, 7055, 7056, 7057, 7058, 7059, 7060, 7061, 7062, 7063, 7064, 7065, 7066, 7067, 7068, 7069, 7070, 7071, 7072, 7073, 7074, 7075, 7076, 7077, 7078, 7079, 7080, 7081, 7082, 7083, 7084, 7085, 7086, 7087, 7088, 7089, 7090, 7091, 7092, 7093, 7094, 7095, 7096, 7097, 7098, 7099, 7100, 7101, 7102, 7103, 7104, 7105, 7106, 7107, 7108, 7109, 7110, 7111, 7112, 7113, 7114, 7115, 7116, 7117, 7118, 7119, 7120, 7121, 7122, 7123, 7124, 7125, 7126, 7127, 7128, 7129, 7130, 7131, 7132, 7133, 7134, 7135, 7136, 7137, 7138, 7139, 7140, 7141, 7142, 7143, 7144, 7145, 7146, 7147, 7148, 7149, 7150, 7151, 7152, 7153, 7154, 7155, 7156, 7157, 7158, 7159, 7160, 7161, 7162, 7163, 7164, 7165, 7166, 7167, 7168, 7169, 7170, 7171, 7172, 7173, 7174, 7175, 7176, 7177, 7178, 7179, 7180, 7181, 7182, 7183, 7184, 7185, 7186, 7187, 7188, 7189, 7190, 7191, 7192, 7193, 7194, 7195, 7196, 7197, 7198, 7199, 7200, 7201, 7202, 7203, 7204, 7205, 7206, 7207, 7208, 7209, 7210, 7211, 7212, 7213, 7214, 7215, 7216, 7217, 7218, 7219, 7220, 7221, 7222, 7223, 7224, 7225, 7226, 7227, 7228, 7229, 7230, 7231, 7232, 7233, 7234, 7235, 7236, 7237, 7238, 7239, 7240, 7241, 7242, 7243, 7244, 7245, 7246, 7247, 7248, 7249, 7250, 7251, 7252, 7253, 7254, 7255, 7256, 7257, 7258, 7259, 7260, 7261, 7262, 7263, 7264, 7265, 7266, 7267, 7268, 7269, 7270, 7271, 7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281, 7282, 7283, 7284, 7285, 7286, 7287, 7288, 7289, 7290, 7291, 7292, 7293, 7294, 7295, 7296, 7297, 7298, 7299, 7300, 7301, 7302, 7303, 7304, 7305, 7306, 7307, 7308, 7309, 7310, 7311, 7312, 7313, 7314, 7315, 7316, 7317, 7318, 7319, 7320, 7321, 7322, 7323, 7324, 7325, 7326, 7327, 7328, 7329, 7330, 7331, 7332, 7333, 7334, 7335, 7336, 7337, 7338, 7339, 7340, 7341, 7342, 7343, 7344, 7345, 7346, 7347, 7348, 7349, 7350, 7351, 7352, 7353, 7354, 7355, 7356, 7357, 7358, 7359, 7360, 7361, 7362, 7363, 7364, 7365, 7366, 7367, 7368, 7369, 7370, 7371, 7372, 7373, 7374, 7375, 7376, 7377, 7378, 7379, 7380, 7381, 7382, 7383, 7384, 7385, 7386, 7387, 7388, 7389, 7390, 7391, 7392, 7393, 7394, 7395, 7396, 7397, 7398, 7399, 7400, 7401, 7402, 7403, 7404, 7405, 7406, 7407, 7408, 7409, 7410, 7411, 7412, 7413, 7414, 7415, 7416, 7417, 7418, 7419, 7420, 7421, 7422, 7423, 7424, 7425, 7426, 7427, 7428, 7429, 7430, 7431, 7432, 7433, 7434, 7435, 7436, 7437, 7438, 7439, 7440, 7441, 7442, 7443, 7444, 7445, 7446, 7447, 7448, 7449, 7450, 7451, 7452, 7453, 7454, 7455, 7456, 7457, 7458, 7459, 7460, 7461, 7462, 7463, 7464, 7465, 7466, 7467, 7468, 7469, 7470, 7471, 7472, 7473, 7474, 7475, 7476, 7477, 7478, 7479, 7480, 7481, 7482, 7483, 7484, 7485, 7486, 7487, 7488, 7489, 7490, 7491, 7492, 7493, 7494, 7495, 7496, 7497, 7498, 7499, 7500, 7501, 7502, 7503, 7504, 7505, 7506, 7507, 7508, 7509, 7510, 7511, 7512, 7513, 7514, 7515, 7516, 7517, 7518, 7519, 7520, 7521, 7522, 7523, 7524, 7525, 7526, 7527, 7528, 7529, 7530, 7531, 7532, 7533, 7534, 7535, 7536, 7537, 7538, 7539, 7540, 7541, 7542, 7543, 7544, 7545, 7546, 7547, 7548, 7549, 7550, 7551, 7552, 7553, 7554, 7555, 7556, 7557, 7558, 7559, 7560, 7561, 7562, 7563, 7564, 7565, 7566, 7567, 7568, 7569, 7570, 7571, 7572, 7573, 7574, 7575, 7576, 7577, 7578, 7579, 7580, 7581, 7582, 7583, 7584, 7585, 7586, 7587, 7588, 7589, 7590, 7591, 7592, 7593, 7594, 7595, 7596, 7597, 7598, 7599, 7600, 7601, 7602, 7603, 7604, 7605, 7606, 7607, 7608, 7609, 7610, 7611, 7612, 7613, 7614, 7615, 7616, 7617, 7618, 7619, 7620, 7621, 7622, 7623, 7624, 7625, 7626, 7627, 7628, 7629, 7630, 7631, 7632, 7633, 7634, 7635, 7636, 7637, 7638, 7639, 7640, 7641, 7642, 7643, 7644, 7645, 7646, 7647, 7648, 7649, 7650, 7651, 7652, 7653, 7654, 7655, 7656, 7657, 7658, 7659, 7660, 7661, 7662, 7663, 7664, 7665, 7666, 7667, 7668, 7669, 7670, 7671, 7672, 7673, 7674, 7675, 7676, 7677, 7678, 7679, 7680, 7681, 7682, 7683, 7684, 7685, 7686, 7687, 7688, 7689, 7690, 7691, 7692, 7693, 7694, 7695, 7696, 7697, 7698, 7699, 7700, 7701, 7702, 7703, 7704, 7705, 7706, 7707, 7708, 7709, 7710, 7711, 7712, 7713, 7714, 7715, 7716, 7717, 7718, 7719, 7720, 7721, 7722, 7723, 7724, 7725, 7726, 7727, 7728, 7729, 7730, 7731, 7732, 7733, 7734, 7735, 7736, 7737, 7738, 7739, 7740, 7741, 7742, 7743, 7744, 7745, 7746, 7747, 7748, 7749, 7750, 7751, 7752, 7753, 7754, 7755, 7756, 7757, 7758, 7759, 7760, 7761, 7762, 7763, 7764, 7765, 7766, 7767, 7768, 7769, 7770, 7771, 7772, 7773, 7774, 7775, 7776, 7777, 7778, 7779, 7780, 7781, 7782, 7783, 7784, 7785, 7786, 7787, 7788, 7789, 7790, 7791, 7792, 7793, 7794, 7795, 7796, 7797, 7798, 7799, 7800, 7801, 7802, 7803, 7804, 7805, 7806, 7807, 7808, 7809, 7810, 7811, 7812, 7813, 7814, 7815, 7816, 7817, 7818, 7819, 7820, 7821, 7822, 7823, 7824, 7825, 7826, 7827, 7828, 7829, 7830, 7831, 7832, 7833, 7834, 7835, 7836, 7837, 7838, 7839, 7840, 7841, 7842, 7843, 7844, 7845, 7846, 7847, 7848, 7849, 7850, 7851, 7852, 7853, 7854, 7855, 7856, 7857, 7858, 7859, 7860, 7861, 7862, 7863, 7864, 7865, 7866, 7867, 7868, 7869, 7870, 7871, 7872, 7873, 7874, 7875, 7876, 7877, 7878, 7879, 7880, 7881, 7882, 7883, 7884, 7885, 7886, 7887, 7888, 7889, 7890, 7891, 7892, 7893, 7894, 7895, 7896, 7897, 7898, 7899, 7900, 7901, 7902, 7903, 7904, 7905, 7906, 7907, 7908, 7909, 7910, 7911, 7912, 7913, 7914, 7915, 7916, 7917, 7918, 7919, 7920, 7921, 7922, 7923, 7924, 7925, 7926, 7927, 7928, 7929, 7930, 7931, 7932, 7933, 7934, 7935, 7936, 7937, 7938, 7939, 7940, 7941, 7942, 7943, 7944, 7945, 7946, 7947, 7948, 7949, 7950, 7951, 7952, 7953, 7954, 7955, 7956, 7957, 7958, 7959, 7960, 7961, 7962, 7963, 7964, 7965, 7966, 7967, 7968, 7969, 7970, 7971, 7972, 7973, 7974, 7975, 7976, 7977, 7978, 7979, 7980, 7981, 7982, 7983, 7984, 7985, 7986, 7987, 7988, 7989, 7990, 7991, 7992, 7993, 7994, 7995, 7996, 7997, 7998, 7999, 8000, 8001, 8002, 8003, 8004, 8005, 8006, 8007, 8008, 8009, 8010, 8011, 8012, 8013, 8014, 8015, 8016, 8017, 8018, 8019, 8020, 8021, 8022, 8023, 8024, 8025, 8026, 8027, 8028, 8029, 8030, 8031, 8032, 8033, 8034, 8035, 8036, 8037, 8038, 8039, 8040, 8041, 8042, 8043, 8044, 8045, 8046, 8047, 8048, 8049, 8050, 8051, 8052, 8053, 8054, 8055, 8056, 8057, 8058, 8059, 8060, 8061, 8062, 8063, 8064, 8065, 8066, 8067, 8068, 8069, 8070, 8071, 8072, 8073, 8074, 8075, 8076, 8077, 8078, 8079, 8080, 8081, 8082, 8083, 8084, 8085, 8086, 8087, 8088, 8089, 8090, 8091, 8092, 8093, 8094, 8095, 8096, 8097, 8098, 8099, 8100, 8101, 8102, 8103, 8104, 8105, 8106, 8107, 8108, 8109, 8110, 8111, 8112, 8113, 8114, 8115, 8116, 8117, 8118, 8119, 8120, 8121, 8122, 8123, 8124, 8125, 8126, 8127, 8128, 8129, 8130, 8131, 8132, 8133, 8134, 8135, 8136, 8137, 8138, 8139, 8140, 8141, 8142, 8143, 8144, 8145, 8146, 8147, 8148, 8149, 8150, 8151, 8152, 8153, 8154, 8155, 8156, 8157, 8158, 8159, 8160, 8161, 8162, 8163, 8164, 8165, 8166, 8167, 8168, 8169, 8170, 8171, 8172, 8173, 8174, 8175, 8176, 8177, 8178, 8179, 8180, 8181, 8182, 8183, 8184, 8185, 8186, 8187, 8188, 8189, 8190, 8191, 8192, 8193, 8194, 8195, 8196, 8197, 8198, 8199, 8200, 8201, 8202, 8203, 8204, 8205, 8206, 8207, 8208, 8209, 8210, 8211, 8212, 8213, 8214, 8215, 8216, 8217, 8218, 8219, 8220, 8221, 8222, 8223, 8224, 8225, 8226, 8227, 8228, 8229, 8230, 8231, 8232, 8233, 8234, 8235, 8236, 8237, 8238, 8239, 8240, 8241, 8242, 8243, 8244, 8245, 8246, 8247, 8248, 8249, 8250, 8251, 8252, 8253, 8254, 8255, 8256, 8257, 8258, 8259, 8260, 8261, 8262, 8263, 8264, 8265, 8266, 8267, 8268, 8269, 8270, 8271, 8272, 8273, 8274, 8275, 8276, 8277, 8278, 8279, 8280, 8281, 8282, 8283, 8284, 8285, 8286, 8287, 8288, 8289, 8290, 8291, 8292, 8293, 8294, 8295, 8296, 8297, 8298, 8299, 8300, 8301, 8302, 8303, 8304, 8305, 8306, 8307, 8308, 8309, 8310, 8311, 8312, 8313, 8314, 8315, 8316, 8317, 8318, 8319, 8320, 8321, 8322, 8323, 8324, 8325, 8326, 8327, 8328, 8329, 8330, 8331, 8332, 8333, 8334, 8335, 8336, 8337, 8338, 8339, 8340, 8341, 8342, 8343, 8344, 8345, 8346, 8347, 8348, 8349, 8350, 8351, 8352, 8353, 8354, 8355, 8356, 8357, 8358, 8359, 8360, 8361, 8362, 8363, 8364, 8365, 8366, 8367, 8368, 8369, 8370, 8371, 8372, 8373, 8374, 8375, 8376, 8377, 8378, 8379, 8380, 8381, 8382, 8383, 8384, 8385, 8386, 8387, 8388, 8389, 8390, 8391, 8392, 8393, 8394, 8395, 8396, 8397, 8398, 8399, 8400, 8401, 8402, 8403, 8404, 8405, 8406, 8407, 8408, 8409, 8410, 8411, 8412, 8413, 8414, 8415, 8416, 8417, 8418, 8419, 8420, 8421, 8422, 8423, 8424, 8425, 8426, 8427, 8428, 8429, 8430, 8431, 8432, 8433, 8434, 8435, 8436, 8437, 8438, 8439, 8440, 8441, 8442, 8443, 8444, 8445, 8446, 8447, 8448, 8449, 8450, 8451, 8452, 8453, 8454, 8455, 8456, 8457, 8458, 8459, 8460, 8461, 8462, 8463, 8464, 8465, 8466, 8467, 8468, 8469, 8470, 8471, 8472, 8473, 8474, 8475, 8476, 8477, 8478, 8479, 8480, 8481, 8482, 8483, 8484, 8485, 8486, 8487, 8488, 8489, 8490, 8491, 8492, 8493, 8494, 8495, 8496, 8497, 8498, 8499, 8500, 8501, 8502, 8503, 8504, 8505, 8506, 8507, 8508, 8509, 8510, 8511, 8512, 8513, 8514, 8515, 8516, 8517, 8518, 8519, 8520, 8521, 8522, 8523, 8524, 8525, 8526, 8527, 8528, 8529, 8530, 8531, 8532, 8533, 8534, 8535, 8536, 8537, 8538, 8539, 8540, 8541, 8542, 8543, 8544, 8545, 8546, 8547, 8548, 8549, 8550, 8551, 8552, 8553, 8554, 8555, 8556, 8557, 8558, 8559, 8560, 8561, 8562, 8563, 8564, 8565, 8566, 8567, 8568, 8569, 8570, 8571, 8572, 8573, 8574, 8575, 8576, 8577, 8578, 8579, 8580, 8581, 8582, 8583, 8584, 8585, 8586, 8587, 8588, 8589, 8590, 8591, 8592, 8593, 8594, 8595, 8596, 8597, 8598, 8599, 8600, 8601, 8602, 8603, 8604, 8605, 8606, 8607, 8608, 8609, 8610, 8611, 8612, 8613, 8614, 8615, 8616, 8617, 8618, 8619, 8620, 8621, 8622, 8623, 8624, 8625, 8626, 8627, 8628, 8629, 8630, 8631, 8632, 8633, 8634, 8635, 8636, 8637, 8638, 8639, 8640, 8641, 8642, 8643, 8644, 8645, 8646, 8647, 8648, 8649, 8650, 8651, 8652, 8653, 8654, 8655, 8656, 8657, 8658, 8659, 8660, 8661, 8662, 8663, 8664, 8665, 8666, 8667, 8668, 8669, 8670, 8671, 8672, 8673, 8674, 8675, 8676, 8677, 8678, 8679, 8680, 8681, 8682, 8683, 8684, 8685, 8686, 8687, 8688, 8689, 8690, 8691, 8692, 8693, 8694, 8695, 8696, 8697, 8698, 8699, 8700, 8701, 8702, 8703, 8704, 8705, 8706, 8707, 8708, 8709, 8710, 8711, 8712, 8713, 8714, 8715, 8716, 8717, 8718, 8719, 8720, 8721, 8722, 8723, 8724, 8725, 8726, 8727, 8728, 8729, 8730, 8731, 8732, 8733, 8734, 8735, 8736, 8737, 8738, 8739, 8740, 8741, 8742, 8743, 8744, 8745, 8746, 8747, 8748, 8749, 8750, 8751, 8752, 8753, 8754, 8755, 8756, 8757, 8758, 8759, 8760, 8761, 8762, 8763, 8764, 8765, 8766, 8767, 8768, 8769, 8770, 8771, 8772, 8773, 8774, 8775, 8776, 8777, 8778, 8779, 8780, 8781, 8782, 8783, 8784, 8785, 8786, 8787, 8788, 8789, 8790, 8791, 8792, 8793, 8794, 8795, 8796, 8797, 8798, 8799, 8800, 8801, 8802, 8803, 8804, 8805, 8806, 8807, 8808, 8809, 8810, 8811, 8812, 8813, 8814, 8815, 8816, 8817, 8818, 8819, 8820, 8821, 8822, 8823, 8824, 8825, 8826, 8827, 8828, 8829, 8830, 8831, 8832, 8833, 8834, 8835, 8836, 8837, 8838, 8839, 8840, 8841, 8842, 8843, 8844, 8845, 8846, 8847, 8848, 8849, 8850, 8851, 8852, 8853, 8854, 8855, 8856, 8857, 8858, 8859, 8860, 8861, 8862, 8863, 8864, 8865, 8866, 8867, 8868, 8869, 8870, 8871, 8872, 8873, 8874, 8875, 8876, 8877, 8878, 8879, 8880, 8881, 8882, 8883, 8884, 8885, 8886, 8887, 8888, 8889, 8890, 8891, 8892, 8893, 8894, 8895, 8896, 8897, 8898, 8899, 8900, 8901, 8902, 8903, 8904, 8905, 8906, 8907, 8908, 8909, 8910, 8911, 8912, 8913, 8914, 8915, 8916, 8917, 8918, 8919, 8920, 8921, 8922, 8923, 8924, 8925, 8926, 8927, 8928, 8929, 8930, 8931, 8932, 8933, 8934, 8935, 8936, 8937, 8938, 8939, 8940, 8941, 8942, 8943, 8944, 8945, 8946, 8947, 8948, 8949, 8950, 8951, 8952, 8953, 8954, 8955, 8956, 8957, 8958, 8959, 8960, 8961, 8962, 8963, 8964, 8965, 8966, 8967, 8968, 8969, 8970, 8971, 8972, 8973, 8974, 8975, 8976, 8977, 8978, 8979, 8980, 8981, 8982, 8983, 8984, 8985, 8986, 8987, 8988, 8989, 8990, 8991, 8992, 8993, 8994, 8995, 8996, 8997, 8998, 8999, 9000, 9001, 9002, 9003, 9004, 9005, 9006, 9007, 9008, 9009, 9010, 9011, 9012, 9013, 9014, 9015, 9016, 9017, 9018, 9019, 9020, 9021, 9022, 9023, 9024, 9025, 9026, 9027, 9028, 9029, 9030, 9031, 9032, 9033, 9034, 9035, 9036, 9037, 9038, 9039, 9040, 9041, 9042, 9043, 9044, 9045, 9046, 9047, 9048, 9049, 9050, 9051, 9052, 9053, 9054, 9055, 9056, 9057, 9058, 9059, 9060, 9061, 9062, 9063, 9064, 9065, 9066, 9067, 9068, 9069, 9070, 9071, 9072, 9073, 9074, 9075, 9076, 9077, 9078, 9079, 9080, 9081, 9082, 9083, 9084, 9085, 9086, 9087, 9088, 9089, 9090, 9091, 9092, 9093, 9094, 9095, 9096, 9097, 9098, 9099, 9100, 9101, 9102, 9103, 9104, 9105, 9106, 9107, 9108, 9109, 9110, 9111, 9112, 9113, 9114, 9115, 9116, 9117, 9118, 9119, 9120, 9121, 9122, 9123, 9124, 9125, 9126, 9127, 9128, 9129, 9130, 9131, 9132, 9133, 9134, 9135, 9136, 9137, 9138, 9139, 9140, 9141, 9142, 9143, 9144, 9145, 9146, 9147, 9148, 9149, 9150, 9151, 9152, 9153, 9154, 9155, 9156, 9157, 9158, 9159, 9160, 9161, 9162, 9163, 9164, 9165, 9166, 9167, 9168, 9169, 9170, 9171, 9172, 9173, 9174, 9175, 9176, 9177, 9178, 9179, 9180, 9181, 9182, 9183, 9184, 9185, 9186, 9187, 9188, 9189, 9190, 9191, 9192, 9193, 9194, 9195, 9196, 9197, 9198, 9199, 9200, 9201, 9202, 9203, 9204, 9205, 9206, 9207, 9208, 9209, 9210, 9211, 9212, 9213, 9214, 9215, 9216, 9217, 9218, 9219, 9220, 9221, 9222, 9223, 9224, 9225, 9226, 9227, 9228, 9229, 9230, 9231, 9232, 9233, 9234, 9235, 9236, 9237, 9238, 9239, 9240, 9241, 9242, 9243, 9244, 9245, 9246, 9247, 9248, 9249, 9250, 9251, 9252, 9253, 9254, 9255, 9256, 9257, 9258, 9259, 9260, 9261, 9262, 9263, 9264, 9265, 9266, 9267, 9268, 9269, 9270, 9271, 9272, 9273, 9274, 9275, 9276, 9277, 9278, 9279, 9280, 9281, 9282, 9283, 9284, 9285, 9286, 9287, 9288, 9289, 9290, 9291, 9292, 9293, 9294, 9295, 9296, 9297, 9298, 9299, 9300, 9301, 9302, 9303, 9304, 9305, 9306, 9307, 9308, 9309, 9310, 9311, 9312, 9313, 9314, 9315, 9316, 9317, 9318, 9319, 9320, 9321, 9322, 9323, 9324, 9325, 9326, 9327, 9328, 9329, 9330, 9331, 9332, 9333, 9334, 9335, 9336, 9337, 9338, 9339, 9340, 9341, 9342, 9343, 9344, 9345, 9346, 9347, 9348, 9349, 9350, 9351, 9352, 9353, 9354, 9355, 9356, 9357, 9358, 9359, 9360, 9361, 9362, 9363, 9364, 9365, 9366, 9367, 9368, 9369, 9370, 9371, 9372, 9373, 9374, 9375, 9376, 9377, 9378, 9379, 9380, 9381, 9382, 9383, 9384, 9385, 9386, 9387, 9388, 9389, 9390, 9391, 9392, 9393, 9394, 9395, 9396, 9397, 9398, 9399, 9400, 9401, 9402, 9403, 9404, 9405, 9406, 9407, 9408, 9409, 9410, 9411, 9412, 9413, 9414, 9415, 9416, 9417, 9418, 9419, 9420, 9421, 9422, 9423, 9424, 9425, 9426, 9427, 9428, 9429, 9430, 9431, 9432, 9433, 9434, 9435, 9436, 9437, 9438, 9439, 9440, 9441, 9442, 9443, 9444, 9445, 9446, 9447, 9448, 9449, 9450, 9451, 9452, 9453, 9454, 9455, 9456, 9457, 9458, 9459, 9460, 9461, 9462, 9463, 9464, 9465, 9466, 9467, 9468, 9469, 9470, 9471, 9472, 9473, 9474, 9475, 9476, 9477, 9478, 9479, 9480, 9481, 9482, 9483, 9484, 9485, 9486, 9487, 9488, 9489, 9490, 9491, 9492, 9493, 9494, 9495, 9496, 9497, 9498, 9499, 9500, 9501, 9502, 9503, 9504, 9505, 9506, 9507, 9508, 9509, 9510, 9511, 9512, 9513, 9514, 9515, 9516, 9517, 9518, 9519, 9520, 9521, 9522, 9523, 9524, 9525, 9526, 9527, 9528, 9529, 9530, 9531, 9532, 9533, 9534, 9535, 9536, 9537, 9538, 9539, 9540, 9541, 9542, 9543, 9544, 9545, 9546, 9547, 9548, 9549, 9550, 9551, 9552, 9553, 9554, 9555, 9556, 9557, 9558, 9559, 9560, 9561, 9562, 9563, 9564, 9565, 9566, 9567, 9568, 9569, 9570, 9571, 9572, 9573, 9574, 9575, 9576, 9577, 9578, 9579, 9580, 9581, 9582, 9583, 9584, 9585, 9586, 9587, 9588, 9589, 9590, 9591, 9592, 9593, 9594, 9595, 9596, 9597, 9598, 9599, 9600, 9601, 9602, 9603, 9604, 9605, 9606, 9607, 9608, 9609, 9610, 9611, 9612, 9613, 9614, 9615, 9616, 9617, 9618, 9619, 9620, 9621, 9622, 9623, 9624, 9625, 9626, 9627, 9628, 9629, 9630, 9631, 9632, 9633, 9634, 9635, 9636, 9637, 9638, 9639, 9640, 9641, 9642, 9643, 9644, 9645, 9646, 9647, 9648, 9649, 9650, 9651, 9652, 9653, 9654, 9655, 9656, 9657, 9658, 9659, 9660, 9661, 9662, 9663, 9664, 9665, 9666, 9667, 9668, 9669, 9670, 9671, 9672, 9673, 9674, 9675, 9676, 9677, 9678, 9679, 9680, 9681, 9682, 9683, 9684, 9685, 9686, 9687, 9688, 9689, 9690, 9691, 9692, 9693, 9694, 9695, 9696, 9697, 9698, 9699, 9700, 9701, 9702, 9703, 9704, 9705, 9706, 9707, 9708, 9709, 9710, 9711, 9712, 9713, 9714, 9715, 9716, 9717, 9718, 9719, 9720, 9721, 9722, 9723, 9724, 9725, 9726, 9727, 9728, 9729, 9730, 9731, 9732, 9733, 9734, 9735, 9736, 9737, 9738, 9739, 9740, 9741, 9742, 9743, 9744, 9745, 9746, 9747, 9748, 9749, 9750, 9751, 9752, 9753, 9754, 9755, 9756, 9757, 9758, 9759, 9760, 9761, 9762, 9763, 9764, 9765, 9766, 9767, 9768, 9769, 9770, 9771, 9772, 9773, 9774, 9775, 9776, 9777, 9778, 9779, 9780, 9781, 9782, 9783, 9784, 9785, 9786, 9787, 9788, 9789, 9790, 9791, 9792, 9793, 9794, 9795, 9796, 9797, 9798, 9799, 9800, 9801, 9802, 9803, 9804, 9805, 9806, 9807, 9808, 9809, 9810, 9811, 9812, 9813, 9814, 9815, 9816, 9817, 9818, 9819, 9820, 9821, 9822, 9823, 9824, 9825, 9826, 9827, 9828, 9829, 9830, 9831, 9832, 9833, 9834, 9835, 9836, 9837, 9838, 9839, 9840, 9841, 9842, 9843, 9844, 9845, 9846, 9847, 9848, 9849, 9850, 9851, 9852, 9853, 9854, 9855, 9856, 9857, 9858, 9859, 9860, 9861, 9862, 9863, 9864, 9865, 9866, 9867, 9868, 9869, 9870, 9871, 9872, 9873, 9874, 9875, 9876, 9877, 9878, 9879, 9880, 9881, 9882, 9883, 9884, 9885, 9886, 9887, 9888, 9889, 9890, 9891, 9892, 9893, 9894, 9895, 9896, 9897, 9898, 9899, 9900, 9901, 9902, 9903, 9904, 9905, 9906, 9907, 9908, 9909, 9910, 9911, 9912, 9913, 9914, 9915, 9916, 9917, 9918, 9919, 9920, 9921, 9922, 9923, 9924, 9925, 9926, 9927, 9928, 9929, 9930, 9931, 9932, 9933, 9934, 9935, 9936, 9937, 9938, 9939, 9940, 9941, 9942, 9943, 9944, 9945, 9946, 9947, 9948, 9949, 9950, 9951, 9952, 9953, 9954, 9955, 9956, 9957, 9958, 9959, 9960, 9961, 9962, 9963, 9964, 9965, 9966, 9967, 9968, 9969, 9970, 9971, 9972, 9973, 9974, 9975, 9976, 9977, 9978, 9979, 9980, 9981, 9982, 9983, 9984, 9985, 9986, 9987, 9988, 9989, 9990, 9991, 9992, 9993, 9994, 9995, 9996, 9997, 9998, 9999, 10000, 10001, 10002, 10003, 10004, 10005, 10006, 10007, 10008, 10009, 10010, 10011, 10012, 10013, 10014, 10015, 10016, 10017, 10018, 10019, 10020, 10021, 10022, 10023, 10024, 10025, 10026, 10027, 10028, 10029, 10030, 10031, 10032, 10033, 10034, 10035, 10036, 10037, 10038, 10039, 10040, 10041, 10042, 10043, 10044, 10045, 10046, 10047, 10048, 10049, 10050, 10051, 10052, 10053, 10054, 10055, 10056, 10057, 10058, 10059, 10060, 10061, 10062, 10063, 10064, 10065, 10066, 10067, 10068, 10069, 10070, 10071, 10072, 10073, 10074, 10075, 10076, 10077, 10078, 10079, 10080, 10081, 10082, 10083, 10084, 10085, 10086, 10087, 10088, 10089, 10090, 10091, 10092, 10093, 10094, 10095, 10096, 10097, 10098, 10099, 10100, 10101, 10102, 10103, 10104, 10105, 10106, 10107, 10108, 10109, 10110, 10111, 10112, 10113, 10114, 10115, 10116, 10117, 10118, 10119, 10120, 10121, 10122, 10123, 10124, 10125, 10126, 10127, 10128, 10129, 10130, 10131, 10132, 10133, 10134, 10135, 10136, 10137, 10138, 10139, 10140, 10141, 10142, 10143, 10144, 10145, 10146, 10147, 10148, 10149, 10150, 10151, 10152, 10153, 10154, 10155, 10156, 10157, 10158, 10159, 10160, 10161, 10162, 10163, 10164, 10165, 10166, 10167, 10168, 10169, 10170, 10171, 10172, 10173, 10174, 10175, 10176, 10177, 10178, 10179, 10180, 10181, 10182, 10183, 10184, 10185, 10186, 10187, 10188, 10189, 10190, 10191, 10192, 10193, 10194, 10195, 10196, 10197, 10198, 10199, 10200, 10201, 10202, 10203, 10204, 10205, 10206, 10207, 10208, 10209, 10210, 10211, 10212, 10213, 10214, 10215, 10216, 10217, 10218, 10219, 10220, 10221, 10222, 10223, 10224, 10225, 10226, 10227, 10228, 10229, 10230, 10231, 10232, 10233, 10234, 10235, 10236, 10237, 10238, 10239, 10240, 10241, 10242, 10243, 10244, 10245, 10246, 10247, 10248, 10249, 10250, 10251, 10252, 10253, 10254, 10255, 10256, 10257, 10258, 10259, 10260, 10261, 10262, 10263, 10264, 10265, 10266, 10267, 10268, 10269, 10270, 10271, 10272, 10273, 10274, 10275, 10276, 10277, 10278, 10279, 10280, 10281, 10282, 10283, 10284, 10285, 10286, 10287, 10288, 10289, 10290, 10291, 10292, 10293, 10294, 10295, 10296, 10297, 10298, 10299, 10300, 10301, 10302, 10303, 10304, 10305, 10306, 10307, 10308, 10309, 10310, 10311, 10312, 10313, 10314, 10315, 10316, 10317, 10318, 10319, 10320, 10321, 10322, 10323, 10324, 10325, 10326, 10327, 10328, 10329, 10330, 10331, 10332, 10333, 10334, 10335, 10336, 10337, 10338, 10339, 10340, 10341, 10342, 10343, 10344, 10345, 10346, 10347, 10348, 10349, 10350, 10351, 10352, 10353, 10354, 10355, 10356, 10357, 10358, 10359, 10360, 10361, 10362, 10363, 10364, 10365, 10366, 10367, 10368, 10369, 10370, 10371, 10372, 10373, 10374, 10375, 10376, 10377, 10378, 10379, 10380, 10381, 10382, 10383, 10384, 10385, 10386, 10387, 10388, 10389, 10390, 10391, 10392, 10393, 10394, 10395, 10396, 10397, 10398, 10399, 10400, 10401, 10402, 10403, 10404, 10405, 10406, 10407, 10408, 10409, 10410, 10411, 10412, 10413, 10414, 10415, 10416, 10417, 10418, 10419, 10420, 10421, 10422, 10423, 10424, 10425, 10426, 10427, 10428, 10429, 10430, 10431, 10432, 10433, 10434, 10435, 10436, 10437, 10438, 10439, 10440, 10441, 10442, 10443, 10444, 10445, 10446, 10447, 10448, 10449, 10450, 10451, 10452, 10453, 10454, 10455, 10456, 10457, 10458, 10459, 10460, 10461, 10462, 10463, 10464, 10465, 10466, 10467, 10468, 10469, 10470, 10471, 10472, 10473, 10474, 10475, 10476, 10477, 10478, 10479, 10480, 10481, 10482, 10483, 10484, 10485, 10486, 10487, 10488, 10489, 10490, 10491, 10492, 10493, 10494, 10495, 10496, 10497, 10498, 10499, 10500, 10501, 10502, 10503, 10504, 10505, 10506, 10507, 10508, 10509, 10510, 10511, 10512, 10513, 10514, 10515, 10516, 10517, 10518, 10519, 10520, 10521, 10522, 10523, 10524, 10525, 10526, 10527, 10528, 10529, 10530, 10531, 10532, 10533, 10534, 10535, 10536, 10537, 10538, 10539, 10540, 10541, 10542, 10543, 10544, 10545, 10546, 10547, 10548, 10549, 10550, 10551, 10552, 10553, 10554, 10555, 10556, 10557, 10558, 10559, 10560, 10561, 10562, 10563, 10564, 10565, 10566, 10567, 10568, 10569, 10570, 10571, 10572, 10573, 10574, 10575, 10576, 10577, 10578, 10579, 10580, 10581, 10582, 10583, 10584, 10585, 10586, 10587, 10588, 10589, 10590, 10591, 10592, 10593, 10594, 10595, 10596, 10597, 10598, 10599, 10600, 10601, 10602, 10603, 10604, 10605, 10606, 10607, 10608, 10609, 10610, 10611, 10612, 10613, 10614, 10615, 10616, 10617, 10618, 10619, 10620, 10621, 10622, 10623, 10624, 10625, 10626, 10627, 10628, 10629, 10630, 10631, 10632, 10633, 10634, 10635, 10636, 10637, 10638, 10639, 10640, 10641, 10642, 10643, 10644, 10645, 10646, 10647, 10648, 10649, 10650, 10651, 10652, 10653, 10654, 10655, 10656, 10657, 10658, 10659, 10660, 10661, 10662, 10663, 10664, 10665, 10666, 10667, 10668, 10669, 10670, 10671, 10672, 10673, 10674, 10675, 10676, 10677, 10678, 10679, 10680, 10681, 10682, 10683, 10684, 10685, 10686, 10687, 10688, 10689, 10690, 10691, 10692, 10693, 10694, 10695, 10696, 10697, 10698, 10699, 10700, 10701, 10702, 10703, 10704, 10705, 10706, 10707, 10708, 10709, 10710, 10711, 10712, 10713, 10714, 10715, 10716, 10717, 10718, 10719, 10720, 10721, 10722, 10723, 10724, 10725, 10726, 10727, 10728, 10729, 10730, 10731, 10732, 10733, 10734, 10735, 10736, 10737, 10738, 10739, 10740, 10741, 10742, 10743, 10744, 10745, 10746, 10747, 10748, 10749, 10750, 10751, 10752, 10753, 10754, 10755, 10756, 10757, 10758, 10759, 10760, 10761, 10762, 10763, 10764, 10765, 10766, 10767, 10768, 10769, 10770, 10771, 10772, 10773, 10774, 10775, 10776, 10777, 10778, 10779, 10780, 10781, 10782, 10783, 10784, 10785, 10786, 10787, 10788, 10789, 10790, 10791, 10792, 10793, 10794, 10795, 10796, 10797, 10798, 10799, 10800, 10801, 10802, 10803, 10804, 10805, 10806, 10807, 10808, 10809, 10810, 10811, 10812, 10813, 10814, 10815, 10816, 10817, 10818, 10819, 10820, 10821, 10822, 10823, 10824, 10825, 10826, 10827, 10828, 10829, 10830, 10831, 10832, 10833, 10834, 10835, 10836, 10837, 10838, 10839, 10840, 10841, 10842, 10843, 10844, 10845, 10846, 10847, 10848, 10849, 10850, 10851, 10852, 10853, 10854, 10855, 10856, 10857, 10858, 10859, 10860, 10861, 10862, 10863, 10864, 10865, 10866, 10867, 10868, 10869, 10870, 10871, 10872, 10873, 10874, 10875, 10876, 10877, 10878, 10879, 10880, 10881, 10882, 10883, 10884, 10885, 10886, 10887, 10888, 10889, 10890, 10891, 10892, 10893, 10894, 10895, 10896, 10897, 10898, 10899, 10900, 10901, 10902, 10903, 10904, 10905, 10906, 10907, 10908, 10909, 10910, 10911, 10912, 10913, 10914, 10915, 10916, 10917, 10918, 10919, 10920, 10921, 10922, 10923, 10924, 10925, 10926, 10927, 10928, 10929, 10930, 10931, 10932, 10933, 10934, 10935, 10936, 10937, 10938, 10939, 10940, 10941, 10942, 10943, 10944, 10945, 10946, 10947, 10948, 10949, 10950, 10951, 10952, 10953, 10954, 10955, 10956, 10957, 10958, 10959, 10960, 10961, 10962, 10963, 10964, 10965, 10966, 10967, 10968, 10969, 10970, 10971, 10972, 10973, 10974, 10975, 10976, 10977, 10978, 10979, 10980, 10981, 10982, 10983, 10984, 10985, 10986, 10987, 10988, 10989, 10990, 10991, 10992, 10993, 10994, 10995, 10996, 10997, 10998, 10999, 11000, 11001, 11002, 11003, 11004, 11005, 11006, 11007, 11008, 11009, 11010, 11011, 11012, 11013, 11014, 11015, 11016, 11017, 11018, 11019, 11020, 11021, 11022, 11023, 11024, 11025, 11026, 11027, 11028, 11029, 11030, 11031, 11032, 11033, 11034, 11035, 11036, 11037, 11038, 11039, 11040, 11041, 11042, 11043, 11044, 11045, 11046, 11047, 11048, 11049, 11050, 11051, 11052, 11053, 11054, 11055, 11056, 11057, 11058, 11059, 11060, 11061, 11062, 11063, 11064, 11065, 11066, 11067, 11068, 11069, 11070, 11071, 11072, 11073, 11074, 11075, 11076, 11077, 11078, 11079, 11080, 11081, 11082, 11083, 11084, 11085, 11086, 11087, 11088, 11089, 11090, 11091, 11092, 11093, 11094, 11095, 11096, 11097, 11098, 11099, 11100, 11101, 11102, 11103, 11104, 11105, 11106, 11107, 11108, 11109, 11110, 11111, 11112, 11113, 11114, 11115, 11116, 11117, 11118, 11119, 11120, 11121, 11122, 11123, 11124, 11125, 11126, 11127, 11128, 11129, 11130, 11131, 11132, 11133, 11134, 11135, 11136, 11137, 11138, 11139, 11140, 11141, 11142, 11143, 11144, 11145, 11146, 11147, 11148, 11149, 11150, 11151, 11152, 11153, 11154, 11155, 11156, 11157, 11158, 11159, 11160, 11161, 11162, 11163, 11164, 11165, 11166, 11167, 11168, 11169, 11170, 11171, 11172, 11173, 11174, 11175, 11176, 11177, 11178, 11179, 11180, 11181, 11182, 11183, 11184, 11185, 11186, 11187, 11188, 11189, 11190, 11191, 11192, 11193, 11194, 11195, 11196, 11197, 11198, 11199, 11200, 11201, 11202, 11203, 11204, 11205, 11206, 11207, 11208, 11209, 11210, 11211, 11212, 11213, 11214, 11215, 11216, 11217, 11218, 11219, 11220, 11221, 11222, 11223, 11224, 11225, 11226, 11227, 11228, 11229, 11230, 11231, 11232, 11233, 11234, 11235, 11236, 11237, 11238, 11239, 11240, 11241, 11242, 11243, 11244, 11245, 11246, 11247, 11248, 11249, 11250, 11251, 11252, 11253, 11254, 11255, 11256, 11257, 11258, 11259, 11260, 11261, 11262, 11263, 11264, 11265, 11266, 11267, 11268, 11269, 11270, 11271, 11272, 11273, 11274, 11275, 11276, 11277, 11278, 11279, 11280, 11281, 11282, 11283, 11284, 11285, 11286, 11287, 11288, 11289, 11290, 11291, 11292, 11293, 11294, 11295, 11296, 11297, 11298, 11299, 11300, 11301, 11302, 11303, 11304, 11305, 11306, 11307, 11308, 11309, 11310, 11311, 11312, 11313, 11314, 11315, 11316, 11317, 11318, 11319, 11320, 11321, 11322, 11323, 11324, 11325, 11326, 11327, 11328, 11329, 11330, 11331, 11332, 11333, 11334, 11335, 11336, 11337, 11338, 11339, 11340, 11341, 11342, 11343, 11344, 11345, 11346, 11347, 11348, 11349, 11350, 11351, 11352, 11353, 11354, 11355, 11356, 11357, 11358, 11359, 11360, 11361, 11362, 11363, 11364, 11365, 11366, 11367, 11368, 11369, 11370, 11371, 11372, 11373, 11374, 11375, 11376, 11377, 11378, 11379, 11380, 11381, 11382, 11383, 11384, 11385, 11386, 11387, 11388, 11389, 11390, 11391, 11392, 11393, 11394, 11395, 11396, 11397, 11398, 11399, 11400, 11401, 11402, 11403, 11404, 11405, 11406, 11407, 11408, 11409, 11410, 11411, 11412, 11413, 11414, 11415, 11416, 11417, 11418, 11419, 11420, 11421, 11422, 11423, 11424, 11425, 11426, 11427, 11428, 11429, 11430, 11431, 11432, 11433, 11434, 11435, 11436, 11437, 11438, 11439, 11440, 11441, 11442, 11443, 11444, 11445, 11446, 11447, 11448, 11449, 11450, 11451, 11452, 11453, 11454, 11455, 11456, 11457, 11458, 11459, 11460, 11461, 11462, 11463, 11464, 11465, 11466, 11467, 11468, 11469, 11470, 11471, 11472, 11473, 11474, 11475, 11476, 11477, 11478, 11479, 11480, 11481, 11482, 11483, 11484, 11485, 11486, 11487, 11488, 11489, 11490, 11491, 11492, 11493, 11494, 11495, 11496, 11497, 11498, 11499, 11500, 11501, 11502, 11503, 11504, 11505, 11506, 11507, 11508, 11509, 11510, 11511, 11512, 11513, 11514, 11515, 11516, 11517, 11518, 11519, 11520, 11521, 11522, 11523, 11524, 11525, 11526, 11527, 11528, 11529, 11530, 11531, 11532, 11533, 11534, 11535, 11536, 11537, 11538, 11539, 11540, 11541, 11542, 11543, 11544, 11545, 11546, 11547, 11548, 11549, 11550, 11551, 11552, 11553, 11554, 11555, 11556, 11557, 11558, 11559, 11560, 11561, 11562, 11563, 11564, 11565, 11566, 11567, 11568, 11569, 11570, 11571, 11572, 11573, 11574, 11575, 11576, 11577, 11578, 11579, 11580, 11581, 11582, 11583, 11584, 11585, 11586, 11587, 11588, 11589, 11590, 11591, 11592, 11593, 11594, 11595, 11596, 11597, 11598, 11599, 11600, 11601, 11602, 11603, 11604, 11605, 11606, 11607, 11608, 11609, 11610, 11611, 11612, 11613, 11614, 11615, 11616, 11617, 11618, 11619, 11620, 11621, 11622, 11623, 11624, 11625, 11626, 11627, 11628, 11629, 11630, 11631, 11632, 11633, 11634, 11635, 11636, 11637, 11638, 11639, 11640, 11641, 11642, 11643, 11644, 11645, 11646, 11647, 11648, 11649, 11650, 11651, 11652, 11653, 11654, 11655, 11656, 11657, 11658, 11659, 11660, 11661, 11662, 11663, 11664, 11665, 11666, 11667, 11668, 11669, 11670, 11671, 11672, 11673, 11674, 11675, 11676, 11677, 11678, 11679, 11680, 11681, 11682, 11683, 11684, 11685, 11686, 11687, 11688, 11689, 11690, 11691, 11692, 11693, 11694, 11695, 11696, 11697, 11698, 11699, 11700, 11701, 11702, 11703, 11704, 11705, 11706, 11707, 11708, 11709, 11710, 11711, 11712, 11713, 11714, 11715, 11716, 11717, 11718, 11719, 11720, 11721, 11722, 11723, 11724, 11725, 11726, 11727, 11728, 11729, 11730, 11731, 11732, 11733, 11734, 11735, 11736, 11737, 11738, 11739, 11740, 11741, 11742, 11743, 11744, 11745, 11746, 11747, 11748, 11749, 11750, 11751, 11752, 11753, 11754, 11755, 11756, 11757, 11758, 11759, 11760, 11761, 11762, 11763, 11764, 11765, 11766, 11767, 11768, 11769, 11770, 11771, 11772, 11773, 11774, 11775, 11776, 11777, 11778, 11779, 11780, 11781, 11782, 11783, 11784, 11785, 11786, 11787, 11788, 11789, 11790, 11791, 11792, 11793, 11794, 11795, 11796, 11797, 11798, 11799, 11800, 11801, 11802, 11803, 11804, 11805, 11806, 11807, 11808, 11809, 11810, 11811, 11812, 11813, 11814, 11815, 11816, 11817, 11818, 11819, 11820, 11821, 11822, 11823, 11824, 11825, 11826, 11827, 11828, 11829, 11830, 11831, 11832, 11833, 11834, 11835, 11836, 11837, 11838, 11839, 11840, 11841, 11842, 11843, 11844, 11845, 11846, 11847, 11848, 11849, 11850, 11851, 11852, 11853, 11854, 11855, 11856, 11857, 11858, 11859, 11860, 11861, 11862, 11863, 11864, 11865, 11866, 11867, 11868, 11869, 11870, 11871, 11872, 11873, 11874, 11875, 11876, 11877, 11878, 11879, 11880, 11881, 11882, 11883, 11884, 11885, 11886, 11887, 11888, 11889, 11890, 11891, 11892, 11893, 11894, 11895, 11896, 11897, 11898, 11899, 11900, 11901, 11902, 11903, 11904, 11905, 11906, 11907, 11908, 11909, 11910, 11911, 11912, 11913, 11914, 11915, 11916, 11917, 11918, 11919, 11920, 11921, 11922, 11923, 11924, 11925, 11926, 11927, 11928, 11929, 11930, 11931, 11932, 11933, 11934, 11935, 11936, 11937, 11938, 11939, 11940, 11941, 11942, 11943, 11944, 11945, 11946, 11947, 11948, 11949, 11950, 11951, 11952, 11953, 11954, 11955, 11956, 11957, 11958, 11959, 11960, 11961, 11962, 11963, 11964, 11965, 11966, 11967, 11968, 11969, 11970, 11971, 11972, 11973, 11974, 11975, 11976, 11977, 11978, 11979, 11980, 11981, 11982, 11983, 11984, 11985, 11986, 11987, 11988, 11989, 11990, 11991, 11992, 11993, 11994, 11995, 11996, 11997, 11998, 11999, 12000, 12001, 12002, 12003, 12004, 12005, 12006, 12007, 12008, 12009, 12010, 12011, 12012, 12013, 12014, 12015, 12016, 12017, 12018, 12019, 12020, 12021, 12022, 12023, 12024, 12025, 12026, 12027, 12028, 12029, 12030, 12031, 12032, 12033, 12034, 12035, 12036, 12037, 12038, 12039, 12040, 12041, 12042, 12043, 12044, 12045, 12046, 12047, 12048, 12049, 12050, 12051, 12052, 12053, 12054, 12055, 12056, 12057, 12058, 12059, 12060, 12061, 12062, 12063, 12064, 12065, 12066, 12067, 12068, 12069, 12070, 12071, 12072, 12073, 12074, 12075, 12076, 12077, 12078, 12079, 12080, 12081, 12082, 12083, 12084, 12085, 12086, 12087, 12088, 12089, 12090, 12091, 12092, 12093, 12094, 12095, 12096, 12097, 12098, 12099, 12100, 12101, 12102, 12103, 12104, 12105, 12106, 12107, 12108, 12109, 12110, 12111, 12112, 12113, 12114, 12115, 12116, 12117, 12118, 12119, 12120, 12121, 12122, 12123, 12124, 12125, 12126, 12127, 12128, 12129, 12130, 12131, 12132, 12133, 12134, 12135, 12136, 12137, 12138, 12139, 12140, 12141, 12142, 12143, 12144, 12145, 12146, 12147, 12148, 12149, 12150, 12151, 12152, 12153, 12154, 12155, 12156, 12157, 12158, 12159, 12160, 12161, 12162, 12163, 12164, 12165, 12166, 12167, 12168, 12169, 12170, 12171, 12172, 12173, 12174, 12175, 12176, 12177, 12178, 12179, 12180, 12181, 12182, 12183, 12184, 12185, 12186, 12187, 12188, 12189, 12190, 12191, 12192, 12193, 12194, 12195, 12196, 12197, 12198, 12199, 12200, 12201, 12202, 12203, 12204, 12205, 12206, 12207, 12208, 12209, 12210, 12211, 12212, 12213, 12214, 12215, 12216, 12217, 12218, 12219, 12220, 12221, 12222, 12223, 12224, 12225, 12226, 12227, 12228, 12229, 12230, 12231, 12232, 12233, 12234, 12235, 12236, 12237, 12238, 12239, 12240, 12241, 12242, 12243, 12244, 12245, 12246, 12247, 12248, 12249, 12250, 12251, 12252, 12253, 12254, 12255, 12256, 12257, 12258, 12259, 12260, 12261, 12262, 12263, 12264, 12265, 12266, 12267, 12268, 12269, 12270, 12271, 12272, 12273, 12274, 12275, 12276, 12277, 12278, 12279, 12280, 12281, 12282, 12283, 12284, 12285, 12286, 12287, 12288, 12289, 12290, 12291, 12292, 12293, 12294, 12295, 12296, 12297, 12298, 12299, 12300, 12301, 12302, 12303, 12304, 12305, 12306, 12307, 12308, 12309, 12310, 12311, 12312, 12313, 12314, 12315, 12316, 12317, 12318, 12319, 12320, 12321, 12322, 12323, 12324, 12325, 12326, 12327, 12328, 12329, 12330, 12331, 12332, 12333, 12334, 12335, 12336, 12337, 12338, 12339, 12340, 12341, 12342, 12343, 12344, 12345, 12346, 12347, 12348, 12349, 12350, 12351, 12352, 12353, 12354, 12355, 12356, 12357, 12358, 12359, 12360, 12361, 12362, 12363, 12364, 12365, 12366, 12367, 12368, 12369, 12370, 12371, 12372, 12373, 12374, 12375, 12376, 12377, 12378, 12379, 12380, 12381, 12382, 12383, 12384, 12385, 12386, 12387, 12388, 12389, 12390, 12391, 12392, 12393, 12394, 12395, 12396, 12397, 12398, 12399, 12400, 12401, 12402, 12403, 12404, 12405, 12406, 12407, 12408, 12409, 12410, 12411, 12412, 12413, 12414, 12415, 12416, 12417, 12418, 12419, 12420, 12421, 12422, 12423, 12424, 12425, 12426, 12427, 12428, 12429, 12430, 12431, 12432, 12433, 12434, 12435, 12436, 12437, 12438, 12439, 12440, 12441, 12442, 12443, 12444, 12445, 12446, 12447, 12448, 12449, 12450, 12451, 12452, 12453, 12454, 12455, 12456, 12457, 12458, 12459, 12460, 12461, 12462, 12463, 12464, 12465, 12466, 12467, 12468, 12469, 12470, 12471, 12472, 12473, 12474, 12475, 12476, 12477, 12478, 12479, 12480, 12481, 12482, 12483, 12484, 12485, 12486, 12487, 12488, 12489, 12490, 12491, 12492, 12493, 12494, 12495, 12496, 12497, 12498, 12499, 12500, 12501, 12502, 12503, 12504, 12505, 12506, 12507, 12508, 12509, 12510, 12511, 12512, 12513, 12514, 12515, 12516, 12517, 12518, 12519, 12520, 12521, 12522, 12523, 12524, 12525, 12526, 12527, 12528, 12529, 12530, 12531, 12532, 12533, 12534, 12535, 12536, 12537, 12538, 12539, 12540, 12541, 12542, 12543, 12544, 12545, 12546, 12547, 12548, 12549, 12550, 12551, 12552, 12553, 12554, 12555, 12556, 12557, 12558, 12559, 12560, 12561, 12562, 12563, 12564, 12565, 12566, 12567, 12568, 12569, 12570, 12571, 12572, 12573, 12574, 12575, 12576, 12577, 12578, 12579, 12580, 12581, 12582, 12583, 12584, 12585, 12586, 12587, 12588, 12589, 12590, 12591, 12592, 12593, 12594, 12595, 12596, 12597, 12598, 12599, 12600, 12601, 12602, 12603, 12604, 12605, 12606, 12607, 12608, 12609, 12610, 12611, 12612, 12613, 12614, 12615, 12616, 12617, 12618, 12619, 12620, 12621, 12622, 12623, 12624, 12625, 12626, 12627, 12628, 12629, 12630, 12631, 12632, 12633, 12634, 12635, 12636, 12637, 12638, 12639, 12640, 12641, 12642, 12643, 12644, 12645, 12646, 12647, 12648, 12649, 12650, 12651, 12652, 12653, 12654, 12655, 12656, 12657, 12658, 12659, 12660, 12661, 12662, 12663, 12664, 12665, 12666, 12667, 12668, 12669, 12670, 12671, 12672, 12673, 12674, 12675, 12676, 12677, 12678, 12679, 12680, 12681, 12682, 12683, 12684, 12685, 12686, 12687, 12688, 12689, 12690, 12691, 12692, 12693, 12694, 12695, 12696, 12697, 12698, 12699, 12700, 12701, 12702, 12703, 12704, 12705, 12706, 12707, 12708, 12709, 12710, 12711, 12712, 12713, 12714, 12715, 12716, 12717, 12718, 12719, 12720, 12721, 12722, 12723, 12724, 12725, 12726, 12727, 12728, 12729, 12730, 12731, 12732, 12733, 12734, 12735, 12736, 12737, 12738, 12739, 12740, 12741, 12742, 12743, 12744, 12745, 12746, 12747, 12748, 12749, 12750, 12751, 12752, 12753, 12754, 12755, 12756, 12757, 12758, 12759, 12760, 12761, 12762, 12763, 12764, 12765, 12766, 12767, 12768, 12769, 12770, 12771, 12772, 12773, 12774, 12775, 12776, 12777, 12778, 12779, 12780, 12781, 12782, 12783, 12784, 12785, 12786, 12787, 12788, 12789, 12790, 12791, 12792, 12793, 12794, 12795, 12796, 12797, 12798, 12799, 12800, 12801, 12802, 12803, 12804, 12805, 12806, 12807, 12808, 12809, 12810, 12811, 12812, 12813, 12814, 12815, 12816, 12817, 12818, 12819, 12820, 12821, 12822, 12823, 12824, 12825, 12826, 12827, 12828, 12829, 12830, 12831, 12832, 12833, 12834, 12835, 12836, 12837, 12838, 12839, 12840, 12841, 12842, 12843, 12844, 12845, 12846, 12847, 12848, 12849, 12850, 12851, 12852, 12853, 12854, 12855, 12856, 12857, 12858, 12859, 12860, 12861, 12862, 12863, 12864, 12865, 12866, 12867, 12868, 12869, 12870, 12871, 12872, 12873, 12874, 12875, 12876, 12877, 12878, 12879, 12880, 12881, 12882, 12883, 12884, 12885, 12886, 12887, 12888, 12889, 12890, 12891, 12892, 12893, 12894, 12895, 12896, 12897, 12898, 12899, 12900, 12901, 12902, 12903, 12904, 12905, 12906, 12907, 12908, 12909, 12910, 12911, 12912, 12913, 12914, 12915, 12916, 12917, 12918, 12919, 12920, 12921, 12922, 12923, 12924, 12925, 12926, 12927, 12928, 12929, 12930, 12931, 12932, 12933, 12934, 12935, 12936, 12937, 12938, 12939, 12940, 12941, 12942, 12943, 12944, 12945, 12946, 12947, 12948, 12949, 12950, 12951, 12952, 12953, 12954, 12955, 12956, 12957, 12958, 12959, 12960, 12961, 12962, 12963, 12964, 12965, 12966, 12967, 12968, 12969, 12970, 12971, 12972, 12973, 12974, 12975, 12976, 12977, 12978, 12979, 12980, 12981, 12982, 12983, 12984, 12985, 12986, 12987, 12988, 12989, 12990, 12991, 12992, 12993, 12994, 12995, 12996, 12997, 12998, 12999, 13000, 13001, 13002, 13003, 13004, 13005, 13006, 13007, 13008, 13009, 13010, 13011, 13012, 13013, 13014, 13015, 13016, 13017, 13018, 13019, 13020, 13021, 13022, 13023, 13024, 13025, 13026, 13027, 13028, 13029, 13030, 13031, 13032, 13033, 13034, 13035, 13036, 13037, 13038, 13039, 13040, 13041, 13042, 13043, 13044, 13045, 13046, 13047, 13048, 13049, 13050, 13051, 13052, 13053, 13054, 13055, 13056, 13057, 13058, 13059, 13060, 13061, 13062, 13063, 13064, 13065, 13066, 13067, 13068, 13069, 13070, 13071, 13072, 13073, 13074, 13075, 13076, 13077, 13078, 13079, 13080, 13081, 13082, 13083, 13084, 13085, 13086, 13087, 13088, 13089, 13090, 13091, 13092, 13093, 13094, 13095, 13096, 13097, 13098, 13099, 13100, 13101, 13102, 13103, 13104, 13105, 13106, 13107, 13108, 13109, 13110, 13111, 13112, 13113, 13114, 13115, 13116, 13117, 13118, 13119, 13120, 13121, 13122, 13123, 13124, 13125, 13126, 13127, 13128, 13129, 13130, 13131, 13132, 13133, 13134, 13135, 13136, 13137, 13138, 13139, 13140, 13141, 13142, 13143, 13144, 13145, 13146, 13147, 13148, 13149, 13150, 13151, 13152, 13153, 13154, 13155, 13156, 13157, 13158, 13159, 13160, 13161, 13162, 13163, 13164, 13165, 13166, 13167, 13168, 13169, 13170, 13171, 13172, 13173, 13174, 13175, 13176, 13177, 13178, 13179, 13180, 13181, 13182, 13183, 13184, 13185, 13186, 13187, 13188, 13189, 13190, 13191, 13192, 13193, 13194, 13195, 13196, 13197, 13198, 13199, 13200, 13201, 13202, 13203, 13204, 13205, 13206, 13207, 13208, 13209, 13210, 13211, 13212, 13213, 13214, 13215, 13216, 13217, 13218, 13219, 13220, 13221, 13222, 13223, 13224, 13225, 13226, 13227, 13228, 13229, 13230, 13231, 13232, 13233, 13234, 13235, 13236, 13237, 13238, 13239, 13240, 13241, 13242, 13243, 13244, 13245, 13246, 13247, 13248, 13249, 13250, 13251, 13252, 13253, 13254, 13255, 13256, 13257, 13258, 13259, 13260, 13261, 13262, 13263, 13264, 13265, 13266, 13267, 13268, 13269, 13270, 13271, 13272, 13273, 13274, 13275, 13276, 13277, 13278, 13279, 13280, 13281, 13282, 13283, 13284, 13285, 13286, 13287, 13288, 13289, 13290, 13291, 13292, 13293, 13294, 13295, 13296, 13297, 13298, 13299, 13300, 13301, 13302, 13303, 13304, 13305, 13306, 13307, 13308, 13309, 13310, 13311, 13312, 13313, 13314, 13315, 13316, 13317, 13318, 13319, 13320, 13321, 13322, 13323, 13324, 13325, 13326, 13327, 13328, 13329, 13330, 13331, 13332, 13333, 13334, 13335, 13336, 13337, 13338, 13339, 13340, 13341, 13342, 13343, 13344, 13345, 13346, 13347, 13348, 13349, 13350, 13351, 13352, 13353, 13354, 13355, 13356, 13357, 13358, 13359, 13360, 13361, 13362, 13363, 13364, 13365, 13366, 13367, 13368, 13369, 13370, 13371, 13372, 13373, 13374, 13375, 13376, 13377, 13378, 13379, 13380, 13381, 13382, 13383, 13384, 13385, 13386, 13387, 13388, 13389, 13390, 13391, 13392, 13393, 13394, 13395, 13396, 13397, 13398, 13399, 13400, 13401, 13402, 13403, 13404, 13405, 13406, 13407, 13408, 13409, 13410, 13411, 13412, 13413, 13414, 13415, 13416, 13417, 13418, 13419, 13420, 13421, 13422, 13423, 13424, 13425, 13426, 13427, 13428, 13429, 13430, 13431, 13432, 13433, 13434, 13435, 13436, 13437, 13438, 13439, 13440, 13441, 13442, 13443, 13444, 13445, 13446, 13447, 13448, 13449, 13450, 13451, 13452, 13453, 13454, 13455, 13456, 13457, 13458, 13459, 13460, 13461, 13462, 13463, 13464, 13465, 13466, 13467, 13468, 13469, 13470, 13471, 13472, 13473, 13474, 13475, 13476, 13477, 13478, 13479, 13480, 13481, 13482, 13483, 13484, 13485, 13486, 13487, 13488, 13489, 13490, 13491, 13492, 13493, 13494, 13495, 13496, 13497, 13498, 13499, 13500, 13501, 13502, 13503, 13504, 13505, 13506, 13507, 13508, 13509, 13510, 13511, 13512, 13513, 13514, 13515, 13516, 13517, 13518, 13519, 13520, 13521, 13522, 13523, 13524, 13525, 13526, 13527, 13528, 13529, 13530, 13531, 13532, 13533, 13534, 13535, 13536, 13537, 13538, 13539, 13540, 13541, 13542, 13543, 13544, 13545, 13546, 13547, 13548, 13549, 13550, 13551, 13552, 13553, 13554, 13555, 13556, 13557, 13558, 13559, 13560, 13561, 13562, 13563, 13564, 13565, 13566, 13567, 13568, 13569, 13570, 13571, 13572, 13573, 13574, 13575, 13576, 13577, 13578, 13579, 13580, 13581, 13582, 13583, 13584, 13585, 13586, 13587, 13588, 13589, 13590, 13591, 13592, 13593, 13594, 13595, 13596, 13597, 13598, 13599, 13600, 13601, 13602, 13603, 13604, 13605, 13606, 13607, 13608, 13609, 13610, 13611, 13612, 13613, 13614, 13615, 13616, 13617, 13618, 13619, 13620, 13621, 13622, 13623, 13624, 13625, 13626, 13627, 13628, 13629, 13630, 13631, 13632, 13633, 13634, 13635, 13636, 13637, 13638, 13639, 13640, 13641, 13642, 13643, 13644, 13645, 13646, 13647, 13648, 13649, 13650, 13651, 13652, 13653, 13654, 13655, 13656, 13657, 13658, 13659, 13660, 13661, 13662, 13663, 13664, 13665, 13666, 13667, 13668, 13669, 13670, 13671, 13672, 13673, 13674, 13675, 13676, 13677, 13678, 13679, 13680, 13681, 13682, 13683, 13684, 13685, 13686, 13687, 13688, 13689, 13690, 13691, 13692, 13693, 13694, 13695, 13696, 13697, 13698, 13699, 13700, 13701, 13702, 13703, 13704, 13705, 13706, 13707, 13708, 13709, 13710, 13711, 13712, 13713, 13714, 13715, 13716, 13717, 13718, 13719, 13720, 13721, 13722, 13723, 13724, 13725, 13726, 13727, 13728, 13729, 13730, 13731, 13732, 13733, 13734, 13735, 13736, 13737, 13738, 13739, 13740, 13741, 13742, 13743, 13744, 13745, 13746, 13747, 13748, 13749, 13750, 13751, 13752, 13753, 13754, 13755, 13756, 13757, 13758, 13759, 13760, 13761, 13762, 13763, 13764, 13765, 13766, 13767, 13768, 13769, 13770, 13771, 13772, 13773, 13774, 13775, 13776, 13777, 13778, 13779, 13780, 13781, 13782, 13783, 13784, 13785, 13786, 13787, 13788, 13789, 13790, 13791, 13792, 13793, 13794, 13795, 13796, 13797, 13798, 13799, 13800, 13801, 13802, 13803, 13804, 13805, 13806, 13807, 13808, 13809, 13810, 13811, 13812, 13813, 13814, 13815, 13816, 13817, 13818, 13819, 13820, 13821, 13822, 13823, 13824, 13825, 13826, 13827, 13828, 13829, 13830, 13831, 13832, 13833, 13834, 13835, 13836, 13837, 13838, 13839, 13840, 13841, 13842, 13843, 13844, 13845, 13846, 13847, 13848, 13849, 13850, 13851, 13852, 13853, 13854, 13855, 13856, 13857, 13858, 13859, 13860, 13861, 13862, 13863, 13864, 13865, 13866, 13867, 13868, 13869, 13870, 13871, 13872, 13873, 13874, 13875, 13876, 13877, 13878, 13879, 13880, 13881, 13882, 13883, 13884, 13885, 13886, 13887, 13888, 13889, 13890, 13891, 13892, 13893, 13894, 13895, 13896, 13897, 13898, 13899, 13900, 13901, 13902, 13903, 13904, 13905, 13906, 13907, 13908, 13909, 13910, 13911, 13912, 13913, 13914, 13915, 13916, 13917, 13918, 13919, 13920, 13921, 13922, 13923, 13924, 13925, 13926, 13927, 13928, 13929, 13930, 13931, 13932, 13933, 13934, 13935, 13936, 13937, 13938, 13939, 13940, 13941, 13942, 13943, 13944, 13945, 13946, 13947, 13948, 13949, 13950, 13951, 13952, 13953, 13954, 13955, 13956, 13957, 13958, 13959, 13960, 13961, 13962, 13963, 13964, 13965, 13966, 13967, 13968, 13969, 13970, 13971, 13972, 13973, 13974, 13975, 13976, 13977, 13978, 13979, 13980, 13981, 13982, 13983, 13984, 13985, 13986, 13987, 13988, 13989, 13990, 13991, 13992, 13993, 13994, 13995, 13996, 13997, 13998, 13999, 14000, 14001, 14002, 14003, 14004, 14005, 14006, 14007, 14008, 14009, 14010, 14011, 14012, 14013, 14014, 14015, 14016, 14017, 14018, 14019, 14020, 14021, 14022, 14023, 14024, 14025, 14026, 14027, 14028, 14029, 14030, 14031, 14032, 14033, 14034, 14035, 14036, 14037, 14038, 14039, 14040, 14041, 14042, 14043, 14044, 14045, 14046, 14047, 14048, 14049, 14050, 14051, 14052, 14053, 14054, 14055, 14056, 14057, 14058, 14059, 14060, 14061, 14062, 14063, 14064, 14065, 14066, 14067, 14068, 14069, 14070, 14071, 14072, 14073, 14074, 14075, 14076, 14077, 14078, 14079, 14080, 14081, 14082, 14083, 14084, 14085, 14086, 14087, 14088, 14089, 14090, 14091, 14092, 14093, 14094, 14095, 14096, 14097, 14098, 14099, 14100, 14101, 14102, 14103, 14104, 14105, 14106, 14107, 14108, 14109, 14110, 14111, 14112, 14113, 14114, 14115, 14116, 14117, 14118, 14119, 14120, 14121, 14122, 14123, 14124, 14125, 14126, 14127, 14128, 14129, 14130, 14131, 14132, 14133, 14134, 14135, 14136, 14137, 14138, 14139, 14140, 14141, 14142, 14143, 14144, 14145, 14146, 14147, 14148, 14149, 14150, 14151, 14152, 14153, 14154, 14155, 14156, 14157, 14158, 14159, 14160, 14161, 14162, 14163, 14164, 14165, 14166, 14167, 14168, 14169, 14170, 14171, 14172, 14173, 14174, 14175, 14176, 14177, 14178, 14179, 14180, 14181, 14182, 14183, 14184, 14185, 14186, 14187, 14188, 14189, 14190, 14191, 14192, 14193, 14194, 14195, 14196, 14197, 14198, 14199, 14200, 14201, 14202, 14203, 14204, 14205, 14206, 14207, 14208, 14209, 14210, 14211, 14212, 14213, 14214, 14215, 14216, 14217, 14218, 14219, 14220, 14221, 14222, 14223, 14224, 14225, 14226, 14227, 14228, 14229, 14230, 14231, 14232, 14233, 14234, 14235, 14236, 14237, 14238, 14239, 14240, 14241, 14242, 14243, 14244, 14245, 14246, 14247, 14248, 14249, 14250, 14251, 14252, 14253, 14254, 14255, 14256, 14257, 14258, 14259, 14260, 14261, 14262, 14263, 14264, 14265, 14266, 14267, 14268, 14269, 14270, 14271, 14272, 14273, 14274, 14275, 14276, 14277, 14278, 14279, 14280, 14281, 14282, 14283, 14284, 14285, 14286, 14287, 14288, 14289, 14290, 14291, 14292, 14293, 14294, 14295, 14296, 14297, 14298, 14299, 14300, 14301, 14302, 14303, 14304, 14305, 14306, 14307, 14308, 14309, 14310, 14311, 14312, 14313, 14314, 14315, 14316, 14317, 14318, 14319, 14320, 14321, 14322, 14323, 14324, 14325, 14326, 14327, 14328, 14329, 14330, 14331, 14332, 14333, 14334, 14335, 14336, 14337, 14338, 14339, 14340, 14341, 14342, 14343, 14344, 14345, 14346, 14347, 14348, 14349, 14350, 14351, 14352, 14353, 14354, 14355, 14356, 14357, 14358, 14359, 14360, 14361, 14362, 14363, 14364, 14365, 14366, 14367, 14368, 14369, 14370, 14371, 14372, 14373, 14374, 14375, 14376, 14377, 14378, 14379, 14380, 14381, 14382, 14383, 14384, 14385, 14386, 14387, 14388, 14389, 14390, 14391, 14392, 14393, 14394, 14395, 14396, 14397, 14398, 14399, 14400, 14401, 14402, 14403, 14404, 14405, 14406, 14407, 14408, 14409, 14410, 14411, 14412, 14413, 14414, 14415, 14416, 14417, 14418, 14419, 14420, 14421, 14422, 14423, 14424, 14425, 14426, 14427, 14428, 14429, 14430, 14431, 14432, 14433, 14434, 14435, 14436, 14437, 14438, 14439, 14440, 14441, 14442, 14443, 14444, 14445, 14446, 14447, 14448, 14449, 14450, 14451, 14452, 14453, 14454, 14455, 14456, 14457, 14458, 14459, 14460, 14461, 14462, 14463, 14464, 14465, 14466, 14467, 14468, 14469, 14470, 14471, 14472, 14473, 14474, 14475, 14476, 14477, 14478, 14479, 14480, 14481, 14482, 14483, 14484, 14485, 14486, 14487, 14488, 14489, 14490, 14491, 14492, 14493, 14494, 14495, 14496, 14497, 14498, 14499, 14500, 14501, 14502, 14503, 14504, 14505, 14506, 14507, 14508, 14509, 14510, 14511, 14512, 14513, 14514, 14515, 14516, 14517, 14518, 14519, 14520, 14521, 14522, 14523, 14524, 14525, 14526, 14527, 14528, 14529, 14530, 14531, 14532, 14533, 14534, 14535, 14536, 14537, 14538, 14539, 14540, 14541, 14542, 14543, 14544, 14545, 14546, 14547, 14548, 14549, 14550, 14551, 14552, 14553, 14554, 14555, 14556, 14557, 14558, 14559, 14560, 14561, 14562, 14563, 14564, 14565, 14566, 14567, 14568, 14569, 14570, 14571, 14572, 14573, 14574, 14575, 14576, 14577, 14578, 14579, 14580, 14581, 14582, 14583, 14584, 14585, 14586, 14587, 14588, 14589, 14590, 14591, 14592, 14593, 14594, 14595, 14596, 14597, 14598, 14599, 14600, 14601, 14602, 14603, 14604, 14605, 14606, 14607, 14608, 14609, 14610, 14611, 14612, 14613, 14614, 14615, 14616, 14617, 14618, 14619, 14620, 14621, 14622, 14623, 14624, 14625, 14626, 14627, 14628, 14629, 14630, 14631, 14632, 14633, 14634, 14635, 14636, 14637, 14638, 14639, 14640, 14641, 14642, 14643, 14644, 14645, 14646, 14647, 14648, 14649, 14650, 14651, 14652, 14653, 14654, 14655, 14656, 14657, 14658, 14659, 14660, 14661, 14662, 14663, 14664, 14665, 14666, 14667, 14668, 14669, 14670, 14671, 14672, 14673, 14674, 14675, 14676, 14677, 14678, 14679, 14680, 14681, 14682, 14683, 14684, 14685, 14686, 14687, 14688, 14689, 14690, 14691, 14692, 14693, 14694, 14695, 14696, 14697, 14698, 14699, 14700, 14701, 14702, 14703, 14704, 14705, 14706, 14707, 14708, 14709, 14710, 14711, 14712, 14713, 14714, 14715, 14716, 14717, 14718, 14719, 14720, 14721, 14722, 14723, 14724, 14725, 14726, 14727, 14728, 14729, 14730, 14731, 14732, 14733, 14734, 14735, 14736, 14737, 14738, 14739, 14740, 14741, 14742, 14743, 14744, 14745, 14746, 14747, 14748, 14749, 14750, 14751, 14752, 14753, 14754, 14755, 14756, 14757, 14758, 14759, 14760, 14761, 14762, 14763, 14764, 14765, 14766, 14767, 14768, 14769, 14770, 14771, 14772, 14773, 14774, 14775, 14776, 14777, 14778, 14779, 14780, 14781, 14782, 14783, 14784, 14785, 14786, 14787, 14788, 14789, 14790, 14791, 14792, 14793, 14794, 14795, 14796, 14797, 14798, 14799, 14800, 14801, 14802, 14803, 14804, 14805, 14806, 14807, 14808, 14809, 14810, 14811, 14812, 14813, 14814, 14815, 14816, 14817, 14818, 14819, 14820, 14821, 14822, 14823, 14824, 14825, 14826, 14827, 14828, 14829, 14830, 14831, 14832, 14833, 14834, 14835, 14836, 14837, 14838, 14839, 14840, 14841, 14842, 14843, 14844, 14845, 14846, 14847, 14848, 14849, 14850, 14851, 14852, 14853, 14854, 14855, 14856, 14857, 14858, 14859, 14860, 14861, 14862, 14863, 14864, 14865, 14866, 14867, 14868, 14869, 14870, 14871, 14872, 14873, 14874, 14875, 14876, 14877, 14878, 14879, 14880, 14881, 14882, 14883, 14884, 14885, 14886, 14887, 14888, 14889, 14890, 14891, 14892, 14893, 14894, 14895, 14896, 14897, 14898, 14899, 14900, 14901, 14902, 14903, 14904, 14905, 14906, 14907, 14908, 14909, 14910, 14911, 14912, 14913, 14914, 14915, 14916, 14917, 14918, 14919, 14920, 14921, 14922, 14923, 14924, 14925, 14926, 14927, 14928, 14929, 14930, 14931, 14932, 14933, 14934, 14935, 14936, 14937, 14938, 14939, 14940, 14941, 14942, 14943, 14944, 14945, 14946, 14947, 14948, 14949, 14950, 14951, 14952, 14953, 14954, 14955, 14956, 14957, 14958, 14959, 14960, 14961, 14962, 14963, 14964, 14965, 14966, 14967, 14968, 14969, 14970, 14971, 14972, 14973, 14974, 14975, 14976, 14977, 14978, 14979, 14980, 14981, 14982, 14983, 14984, 14985, 14986, 14987, 14988, 14989, 14990, 14991, 14992, 14993, 14994, 14995, 14996, 14997, 14998, 14999, 15000, 15001, 15002, 15003, 15004, 15005, 15006, 15007, 15008, 15009, 15010, 15011, 15012, 15013, 15014, 15015, 15016, 15017, 15018, 15019, 15020, 15021, 15022, 15023, 15024, 15025, 15026, 15027, 15028, 15029, 15030, 15031, 15032, 15033, 15034, 15035, 15036, 15037, 15038, 15039, 15040, 15041, 15042, 15043, 15044, 15045, 15046, 15047, 15048, 15049, 15050, 15051, 15052, 15053, 15054, 15055, 15056, 15057, 15058, 15059, 15060, 15061, 15062, 15063, 15064, 15065, 15066, 15067, 15068, 15069, 15070, 15071, 15072, 15073, 15074, 15075, 15076, 15077, 15078, 15079, 15080, 15081, 15082, 15083, 15084, 15085, 15086, 15087, 15088, 15089, 15090, 15091, 15092, 15093, 15094, 15095, 15096, 15097, 15098, 15099, 15100, 15101, 15102, 15103, 15104, 15105, 15106, 15107, 15108, 15109, 15110, 15111, 15112, 15113, 15114, 15115, 15116, 15117, 15118, 15119, 15120, 15121, 15122, 15123, 15124, 15125, 15126, 15127, 15128, 15129, 15130, 15131, 15132, 15133, 15134, 15135, 15136, 15137, 15138, 15139, 15140, 15141, 15142, 15143, 15144, 15145, 15146, 15147, 15148, 15149, 15150, 15151, 15152, 15153, 15154, 15155, 15156, 15157, 15158, 15159, 15160, 15161, 15162, 15163, 15164, 15165, 15166, 15167, 15168, 15169, 15170, 15171, 15172, 15173, 15174, 15175, 15176, 15177, 15178, 15179, 15180, 15181, 15182, 15183, 15184, 15185, 15186, 15187, 15188, 15189, 15190, 15191, 15192, 15193, 15194, 15195, 15196, 15197, 15198, 15199, 15200, 15201, 15202, 15203, 15204, 15205, 15206, 15207, 15208, 15209, 15210, 15211, 15212, 15213, 15214, 15215, 15216, 15217, 15218, 15219, 15220, 15221, 15222, 15223, 15224, 15225, 15226, 15227, 15228, 15229, 15230, 15231, 15232, 15233, 15234, 15235, 15236, 15237, 15238, 15239, 15240, 15241, 15242, 15243, 15244, 15245, 15246, 15247, 15248, 15249, 15250, 15251, 15252, 15253, 15254, 15255, 15256, 15257, 15258, 15259, 15260, 15261, 15262, 15263, 15264, 15265, 15266, 15267, 15268, 15269, 15270, 15271, 15272, 15273, 15274, 15275, 15276, 15277, 15278, 15279, 15280, 15281, 15282, 15283, 15284, 15285, 15286, 15287, 15288, 15289, 15290, 15291, 15292, 15293, 15294, 15295, 15296, 15297, 15298, 15299, 15300, 15301, 15302, 15303, 15304, 15305, 15306, 15307, 15308, 15309, 15310, 15311, 15312, 15313, 15314, 15315, 15316, 15317, 15318, 15319, 15320, 15321, 15322, 15323, 15324, 15325, 15326, 15327, 15328, 15329, 15330, 15331, 15332, 15333, 15334, 15335, 15336, 15337, 15338, 15339, 15340, 15341, 15342, 15343, 15344, 15345, 15346, 15347, 15348, 15349, 15350, 15351, 15352, 15353, 15354, 15355, 15356, 15357, 15358, 15359, 15360, 15361, 15362, 15363, 15364, 15365, 15366, 15367, 15368, 15369, 15370, 15371, 15372, 15373, 15374, 15375, 15376, 15377, 15378, 15379, 15380, 15381, 15382, 15383, 15384, 15385, 15386, 15387, 15388, 15389, 15390, 15391, 15392, 15393, 15394, 15395, 15396, 15397, 15398, 15399, 15400, 15401, 15402, 15403, 15404, 15405, 15406, 15407, 15408, 15409, 15410, 15411, 15412, 15413, 15414, 15415, 15416, 15417, 15418, 15419, 15420, 15421, 15422, 15423, 15424, 15425, 15426, 15427, 15428, 15429, 15430, 15431, 15432, 15433, 15434, 15435, 15436, 15437, 15438, 15439, 15440, 15441, 15442, 15443, 15444, 15445, 15446, 15447, 15448, 15449, 15450, 15451, 15452, 15453, 15454, 15455, 15456, 15457, 15458, 15459, 15460, 15461, 15462, 15463, 15464, 15465, 15466, 15467, 15468, 15469, 15470, 15471, 15472, 15473, 15474, 15475, 15476, 15477, 15478, 15479, 15480, 15481, 15482, 15483, 15484, 15485, 15486, 15487, 15488, 15489, 15490, 15491, 15492, 15493, 15494, 15495, 15496, 15497, 15498, 15499, 15500, 15501, 15502, 15503, 15504, 15505, 15506, 15507, 15508, 15509, 15510, 15511, 15512, 15513, 15514, 15515, 15516, 15517, 15518, 15519, 15520, 15521, 15522, 15523, 15524, 15525, 15526, 15527, 15528, 15529, 15530, 15531, 15532, 15533, 15534, 15535, 15536, 15537, 15538, 15539, 15540, 15541, 15542, 15543, 15544, 15545, 15546, 15547, 15548, 15549, 15550, 15551, 15552, 15553, 15554, 15555, 15556, 15557, 15558, 15559, 15560, 15561, 15562, 15563, 15564, 15565, 15566, 15567, 15568, 15569, 15570, 15571, 15572, 15573, 15574, 15575, 15576, 15577, 15578, 15579, 15580, 15581, 15582, 15583, 15584, 15585, 15586, 15587, 15588, 15589, 15590, 15591, 15592, 15593, 15594, 15595, 15596, 15597, 15598, 15599, 15600, 15601, 15602, 15603, 15604, 15605, 15606, 15607, 15608, 15609, 15610, 15611, 15612, 15613, 15614, 15615, 15616, 15617, 15618, 15619, 15620, 15621, 15622, 15623, 15624, 15625, 15626, 15627, 15628, 15629, 15630, 15631, 15632, 15633, 15634, 15635, 15636, 15637, 15638, 15639, 15640, 15641, 15642, 15643, 15644, 15645, 15646, 15647, 15648, 15649, 15650, 15651, 15652, 15653, 15654, 15655, 15656, 15657, 15658, 15659, 15660, 15661, 15662, 15663, 15664, 15665, 15666, 15667, 15668, 15669, 15670, 15671, 15672, 15673, 15674, 15675, 15676, 15677, 15678, 15679, 15680, 15681, 15682, 15683, 15684, 15685, 15686, 15687, 15688, 15689, 15690, 15691, 15692, 15693, 15694, 15695, 15696, 15697, 15698, 15699, 15700, 15701, 15702, 15703, 15704, 15705, 15706, 15707, 15708, 15709, 15710, 15711, 15712, 15713, 15714, 15715, 15716, 15717, 15718, 15719, 15720, 15721, 15722, 15723, 15724, 15725, 15726, 15727, 15728, 15729, 15730, 15731, 15732, 15733, 15734, 15735, 15736, 15737, 15738, 15739, 15740, 15741, 15742, 15743, 15744, 15745, 15746, 15747, 15748, 15749, 15750, 15751, 15752, 15753, 15754, 15755, 15756, 15757, 15758, 15759, 15760, 15761, 15762, 15763, 15764, 15765, 15766, 15767, 15768, 15769, 15770, 15771, 15772, 15773, 15774, 15775, 15776, 15777, 15778, 15779, 15780, 15781, 15782, 15783, 15784, 15785, 15786, 15787, 15788, 15789, 15790, 15791, 15792, 15793, 15794, 15795, 15796, 15797, 15798, 15799, 15800, 15801, 15802, 15803, 15804, 15805, 15806, 15807, 15808, 15809, 15810, 15811, 15812, 15813, 15814, 15815, 15816, 15817, 15818, 15819, 15820, 15821, 15822, 15823, 15824, 15825, 15826, 15827, 15828, 15829, 15830, 15831, 15832, 15833, 15834, 15835, 15836, 15837, 15838, 15839, 15840, 15841, 15842, 15843, 15844, 15845, 15846, 15847, 15848, 15849, 15850, 15851, 15852, 15853, 15854, 15855, 15856, 15857, 15858, 15859, 15860, 15861, 15862, 15863, 15864, 15865, 15866, 15867, 15868, 15869, 15870, 15871, 15872, 15873, 15874, 15875, 15876, 15877, 15878, 15879, 15880, 15881, 15882, 15883, 15884, 15885, 15886, 15887, 15888, 15889, 15890, 15891, 15892, 15893, 15894, 15895, 15896, 15897, 15898, 15899, 15900, 15901, 15902, 15903, 15904, 15905, 15906, 15907, 15908, 15909, 15910, 15911, 15912, 15913, 15914, 15915, 15916, 15917, 15918, 15919, 15920, 15921, 15922, 15923, 15924, 15925, 15926, 15927, 15928, 15929, 15930, 15931, 15932, 15933, 15934, 15935, 15936, 15937, 15938, 15939, 15940, 15941, 15942, 15943, 15944, 15945, 15946, 15947, 15948, 15949, 15950, 15951, 15952, 15953, 15954, 15955, 15956, 15957, 15958, 15959, 15960, 15961, 15962, 15963, 15964, 15965, 15966, 15967, 15968, 15969, 15970, 15971, 15972, 15973, 15974, 15975, 15976, 15977, 15978, 15979, 15980, 15981, 15982, 15983, 15984, 15985, 15986, 15987, 15988, 15989, 15990, 15991, 15992, 15993, 15994, 15995, 15996, 15997, 15998, 15999, 16000, 16001, 16002, 16003, 16004, 16005, 16006, 16007, 16008, 16009, 16010, 16011, 16012, 16013, 16014, 16015, 16016, 16017, 16018, 16019, 16020, 16021, 16022, 16023, 16024, 16025, 16026, 16027, 16028, 16029, 16030, 16031, 16032, 16033, 16034, 16035, 16036, 16037, 16038, 16039, 16040, 16041, 16042, 16043, 16044, 16045, 16046, 16047, 16048, 16049, 16050, 16051, 16052, 16053, 16054, 16055, 16056, 16057, 16058, 16059, 16060, 16061, 16062, 16063, 16064, 16065, 16066, 16067, 16068, 16069, 16070, 16071, 16072, 16073, 16074, 16075, 16076, 16077, 16078, 16079, 16080, 16081, 16082, 16083, 16084, 16085, 16086, 16087, 16088, 16089, 16090, 16091, 16092, 16093, 16094, 16095, 16096, 16097, 16098, 16099, 16100, 16101, 16102, 16103, 16104, 16105, 16106, 16107, 16108, 16109, 16110, 16111, 16112, 16113, 16114, 16115, 16116, 16117, 16118, 16119, 16120, 16121, 16122, 16123, 16124, 16125, 16126, 16127, 16128, 16129, 16130, 16131, 16132, 16133, 16134, 16135, 16136, 16137, 16138, 16139, 16140, 16141, 16142, 16143, 16144, 16145, 16146, 16147, 16148, 16149, 16150, 16151, 16152, 16153, 16154, 16155, 16156, 16157, 16158, 16159, 16160, 16161, 16162, 16163, 16164, 16165, 16166, 16167, 16168, 16169, 16170, 16171, 16172, 16173, 16174, 16175, 16176, 16177, 16178, 16179, 16180, 16181, 16182, 16183, 16184, 16185, 16186, 16187, 16188, 16189, 16190, 16191, 16192, 16193, 16194, 16195, 16196, 16197, 16198, 16199, 16200, 16201, 16202, 16203, 16204, 16205, 16206, 16207, 16208, 16209, 16210, 16211, 16212, 16213, 16214, 16215, 16216, 16217, 16218, 16219, 16220, 16221, 16222, 16223, 16224, 16225, 16226, 16227, 16228, 16229, 16230, 16231, 16232, 16233, 16234, 16235, 16236, 16237, 16238, 16239, 16240, 16241, 16242, 16243, 16244, 16245, 16246, 16247, 16248, 16249, 16250, 16251, 16252, 16253, 16254, 16255, 16256, 16257, 16258, 16259, 16260, 16261, 16262, 16263, 16264, 16265, 16266, 16267, 16268, 16269, 16270, 16271, 16272, 16273, 16274, 16275, 16276, 16277, 16278, 16279, 16280, 16281, 16282, 16283, 16284, 16285, 16286, 16287, 16288, 16289, 16290, 16291, 16292, 16293, 16294, 16295, 16296, 16297, 16298, 16299, 16300, 16301, 16302, 16303, 16304, 16305, 16306, 16307, 16308, 16309, 16310, 16311, 16312, 16313, 16314, 16315, 16316, 16317, 16318, 16319, 16320, 16321, 16322, 16323, 16324, 16325, 16326, 16327, 16328, 16329, 16330, 16331, 16332, 16333, 16334, 16335, 16336, 16337, 16338, 16339, 16340, 16341, 16342, 16343, 16344, 16345, 16346, 16347, 16348, 16349, 16350, 16351, 16352, 16353, 16354, 16355, 16356, 16357, 16358, 16359, 16360, 16361, 16362, 16363, 16364, 16365, 16366, 16367, 16368, 16369, 16370, 16371, 16372, 16373, 16374, 16375, 16376, 16377, 16378, 16379, 16380, 16381, 16382, 16383, 16384, 16385, 16386, 16387, 16388, 16389, 16390, 16391, 16392, 16393, 16394, 16395, 16396, 16397, 16398, 16399, 16400, 16401, 16402, 16403, 16404, 16405, 16406, 16407, 16408, 16409, 16410, 16411, 16412, 16413, 16414, 16415, 16416, 16417, 16418, 16419, 16420, 16421, 16422, 16423, 16424, 16425, 16426, 16427, 16428, 16429, 16430, 16431, 16432, 16433, 16434, 16435, 16436, 16437, 16438, 16439, 16440, 16441, 16442, 16443, 16444, 16445, 16446, 16447, 16448, 16449, 16450, 16451, 16452, 16453, 16454, 16455, 16456, 16457, 16458, 16459, 16460, 16461, 16462, 16463, 16464, 16465, 16466, 16467, 16468, 16469, 16470, 16471, 16472, 16473, 16474, 16475, 16476, 16477, 16478, 16479, 16480, 16481, 16482, 16483, 16484, 16485, 16486, 16487, 16488, 16489, 16490, 16491, 16492, 16493, 16494, 16495, 16496, 16497, 16498, 16499, 16500, 16501, 16502, 16503, 16504, 16505, 16506, 16507, 16508, 16509, 16510, 16511, 16512, 16513, 16514, 16515, 16516, 16517, 16518, 16519, 16520, 16521, 16522, 16523, 16524, 16525, 16526, 16527, 16528, 16529, 16530, 16531, 16532, 16533, 16534, 16535, 16536, 16537, 16538, 16539, 16540, 16541, 16542, 16543, 16544, 16545, 16546, 16547, 16548, 16549, 16550, 16551, 16552, 16553, 16554, 16555, 16556, 16557, 16558, 16559, 16560, 16561, 16562, 16563, 16564, 16565, 16566, 16567, 16568, 16569, 16570, 16571, 16572, 16573, 16574, 16575, 16576, 16577, 16578, 16579, 16580, 16581, 16582, 16583, 16584, 16585, 16586, 16587, 16588, 16589, 16590, 16591, 16592, 16593, 16594, 16595, 16596, 16597, 16598, 16599, 16600, 16601, 16602, 16603, 16604, 16605, 16606, 16607, 16608, 16609, 16610, 16611, 16612, 16613, 16614, 16615, 16616, 16617, 16618, 16619, 16620, 16621, 16622, 16623, 16624, 16625, 16626, 16627, 16628, 16629, 16630, 16631, 16632, 16633, 16634, 16635, 16636, 16637, 16638, 16639, 16640, 16641, 16642, 16643, 16644, 16645, 16646, 16647, 16648, 16649, 16650, 16651, 16652, 16653, 16654, 16655, 16656, 16657, 16658, 16659, 16660, 16661, 16662, 16663, 16664, 16665, 16666, 16667, 16668, 16669, 16670, 16671, 16672, 16673, 16674, 16675, 16676, 16677, 16678, 16679, 16680, 16681, 16682, 16683, 16684, 16685, 16686, 16687, 16688, 16689, 16690, 16691, 16692, 16693, 16694, 16695, 16696, 16697, 16698, 16699, 16700, 16701, 16702, 16703, 16704, 16705, 16706, 16707, 16708, 16709, 16710, 16711, 16712, 16713, 16714, 16715, 16716, 16717, 16718, 16719, 16720, 16721, 16722, 16723, 16724, 16725, 16726, 16727, 16728, 16729, 16730, 16731, 16732, 16733, 16734, 16735, 16736, 16737, 16738, 16739, 16740, 16741, 16742, 16743, 16744, 16745, 16746, 16747, 16748, 16749, 16750, 16751, 16752, 16753, 16754, 16755, 16756, 16757, 16758, 16759, 16760, 16761, 16762, 16763, 16764, 16765, 16766, 16767, 16768, 16769, 16770, 16771, 16772, 16773, 16774, 16775, 16776, 16777, 16778, 16779, 16780, 16781, 16782, 16783, 16784, 16785, 16786, 16787, 16788, 16789, 16790, 16791, 16792, 16793, 16794, 16795, 16796, 16797, 16798, 16799, 16800, 16801, 16802, 16803, 16804, 16805, 16806, 16807, 16808, 16809, 16810, 16811, 16812, 16813, 16814, 16815, 16816, 16817, 16818, 16819, 16820, 16821, 16822, 16823, 16824, 16825, 16826, 16827, 16828, 16829, 16830, 16831, 16832, 16833, 16834, 16835, 16836, 16837, 16838, 16839, 16840, 16841, 16842, 16843, 16844, 16845, 16846, 16847, 16848, 16849, 16850, 16851, 16852, 16853, 16854, 16855, 16856, 16857, 16858, 16859, 16860, 16861, 16862, 16863, 16864, 16865, 16866, 16867, 16868, 16869, 16870, 16871, 16872, 16873, 16874, 16875, 16876, 16877, 16878, 16879, 16880, 16881, 16882, 16883, 16884, 16885, 16886, 16887, 16888, 16889, 16890, 16891, 16892, 16893, 16894, 16895, 16896, 16897, 16898, 16899, 16900, 16901, 16902, 16903, 16904, 16905, 16906, 16907, 16908, 16909, 16910, 16911, 16912, 16913, 16914, 16915, 16916, 16917, 16918, 16919, 16920, 16921, 16922, 16923, 16924, 16925, 16926, 16927, 16928, 16929, 16930, 16931, 16932, 16933, 16934, 16935, 16936, 16937, 16938, 16939, 16940, 16941, 16942, 16943, 16944, 16945, 16946, 16947, 16948, 16949, 16950, 16951, 16952, 16953, 16954, 16955, 16956, 16957, 16958, 16959, 16960, 16961, 16962, 16963, 16964, 16965, 16966, 16967, 16968, 16969, 16970, 16971, 16972, 16973, 16974, 16975, 16976, 16977, 16978, 16979, 16980, 16981, 16982, 16983, 16984, 16985, 16986, 16987, 16988, 16989, 16990, 16991, 16992, 16993, 16994, 16995, 16996, 16997, 16998, 16999, 17000, 17001, 17002, 17003, 17004, 17005, 17006, 17007, 17008, 17009, 17010, 17011, 17012, 17013, 17014, 17015, 17016, 17017, 17018, 17019, 17020, 17021, 17022, 17023, 17024, 17025, 17026, 17027, 17028, 17029, 17030, 17031, 17032, 17033, 17034, 17035, 17036, 17037, 17038, 17039, 17040, 17041, 17042, 17043, 17044, 17045, 17046, 17047, 17048, 17049, 17050, 17051, 17052, 17053, 17054, 17055, 17056, 17057, 17058, 17059, 17060, 17061, 17062, 17063, 17064, 17065, 17066, 17067, 17068, 17069, 17070, 17071, 17072, 17073, 17074, 17075, 17076, 17077, 17078, 17079, 17080, 17081, 17082, 17083, 17084, 17085, 17086, 17087, 17088, 17089, 17090, 17091, 17092, 17093, 17094, 17095, 17096, 17097, 17098, 17099, 17100, 17101, 17102, 17103, 17104, 17105, 17106, 17107, 17108, 17109, 17110, 17111, 17112, 17113, 17114, 17115, 17116, 17117, 17118, 17119, 17120, 17121, 17122, 17123, 17124, 17125, 17126, 17127, 17128, 17129, 17130, 17131, 17132, 17133, 17134, 17135, 17136, 17137, 17138, 17139, 17140, 17141, 17142, 17143, 17144, 17145, 17146, 17147, 17148, 17149, 17150, 17151, 17152, 17153, 17154, 17155, 17156, 17157, 17158, 17159, 17160, 17161, 17162, 17163, 17164, 17165, 17166, 17167, 17168, 17169, 17170, 17171, 17172, 17173, 17174, 17175, 17176, 17177, 17178, 17179, 17180, 17181, 17182, 17183, 17184, 17185, 17186, 17187, 17188, 17189, 17190, 17191, 17192, 17193, 17194, 17195, 17196, 17197, 17198, 17199, 17200, 17201, 17202, 17203, 17204, 17205, 17206, 17207, 17208, 17209, 17210, 17211, 17212, 17213, 17214, 17215, 17216, 17217, 17218, 17219, 17220, 17221, 17222, 17223, 17224, 17225, 17226, 17227, 17228, 17229, 17230, 17231, 17232, 17233, 17234, 17235, 17236, 17237, 17238, 17239, 17240, 17241, 17242, 17243, 17244, 17245, 17246, 17247, 17248, 17249, 17250, 17251, 17252, 17253, 17254, 17255, 17256, 17257, 17258, 17259, 17260, 17261, 17262, 17263, 17264, 17265, 17266, 17267, 17268, 17269, 17270, 17271, 17272, 17273, 17274, 17275, 17276, 17277, 17278, 17279, 17280, 17281, 17282, 17283, 17284, 17285, 17286, 17287, 17288, 17289, 17290, 17291, 17292, 17293, 17294, 17295, 17296, 17297, 17298, 17299, 17300, 17301, 17302, 17303, 17304, 17305, 17306, 17307, 17308, 17309, 17310, 17311, 17312, 17313, 17314, 17315, 17316, 17317, 17318, 17319, 17320, 17321, 17322, 17323, 17324, 17325, 17326, 17327, 17328, 17329, 17330, 17331, 17332, 17333, 17334, 17335, 17336, 17337, 17338, 17339, 17340, 17341, 17342, 17343, 17344, 17345, 17346, 17347, 17348, 17349, 17350, 17351, 17352, 17353, 17354, 17355, 17356, 17357, 17358, 17359, 17360, 17361, 17362, 17363, 17364, 17365, 17366, 17367, 17368, 17369, 17370, 17371, 17372, 17373, 17374, 17375, 17376, 17377, 17378, 17379, 17380, 17381, 17382, 17383, 17384, 17385, 17386, 17387, 17388, 17389, 17390, 17391, 17392, 17393, 17394, 17395, 17396, 17397, 17398, 17399, 17400, 17401, 17402, 17403, 17404, 17405, 17406, 17407, 17408, 17409, 17410, 17411, 17412, 17413, 17414, 17415, 17416, 17417, 17418, 17419, 17420, 17421, 17422, 17423, 17424, 17425, 17426, 17427, 17428, 17429, 17430, 17431, 17432, 17433, 17434, 17435, 17436, 17437, 17438, 17439, 17440, 17441, 17442, 17443, 17444, 17445, 17446, 17447, 17448, 17449, 17450, 17451, 17452, 17453, 17454, 17455, 17456, 17457, 17458, 17459, 17460, 17461, 17462, 17463, 17464, 17465, 17466, 17467, 17468, 17469, 17470, 17471, 17472, 17473, 17474, 17475, 17476, 17477, 17478, 17479, 17480, 17481, 17482, 17483, 17484, 17485, 17486, 17487, 17488, 17489, 17490, 17491, 17492, 17493, 17494, 17495, 17496, 17497, 17498, 17499, 17500, 17501, 17502, 17503, 17504, 17505, 17506, 17507, 17508, 17509, 17510, 17511, 17512, 17513, 17514, 17515, 17516, 17517, 17518, 17519, 17520, 17521, 17522, 17523, 17524, 17525, 17526, 17527, 17528, 17529, 17530, 17531, 17532, 17533, 17534, 17535, 17536, 17537, 17538, 17539, 17540, 17541, 17542, 17543, 17544, 17545, 17546, 17547, 17548, 17549, 17550, 17551, 17552, 17553, 17554, 17555, 17556, 17557, 17558, 17559, 17560, 17561, 17562, 17563, 17564, 17565, 17566, 17567, 17568, 17569, 17570, 17571, 17572, 17573, 17574, 17575, 17576, 17577, 17578, 17579, 17580, 17581, 17582, 17583, 17584, 17585, 17586, 17587, 17588, 17589, 17590, 17591, 17592, 17593, 17594, 17595, 17596, 17597, 17598, 17599, 17600, 17601, 17602, 17603, 17604, 17605, 17606, 17607, 17608, 17609, 17610, 17611, 17612, 17613, 17614, 17615, 17616, 17617, 17618, 17619, 17620, 17621, 17622, 17623, 17624, 17625, 17626, 17627, 17628, 17629, 17630, 17631, 17632, 17633, 17634, 17635, 17636, 17637, 17638, 17639, 17640, 17641, 17642, 17643, 17644, 17645, 17646, 17647, 17648, 17649, 17650, 17651, 17652, 17653, 17654, 17655, 17656, 17657, 17658, 17659, 17660, 17661, 17662, 17663, 17664, 17665, 17666, 17667, 17668, 17669, 17670, 17671, 17672, 17673, 17674, 17675, 17676, 17677, 17678, 17679, 17680, 17681, 17682, 17683, 17684, 17685, 17686, 17687, 17688, 17689, 17690, 17691, 17692, 17693, 17694, 17695, 17696, 17697, 17698, 17699, 17700, 17701, 17702, 17703, 17704, 17705, 17706, 17707, 17708, 17709, 17710, 17711, 17712, 17713, 17714, 17715, 17716, 17717, 17718, 17719, 17720, 17721, 17722, 17723, 17724, 17725, 17726, 17727, 17728, 17729, 17730, 17731, 17732, 17733, 17734, 17735, 17736, 17737, 17738, 17739, 17740, 17741, 17742, 17743, 17744, 17745, 17746, 17747, 17748, 17749, 17750, 17751, 17752, 17753, 17754, 17755, 17756, 17757, 17758, 17759, 17760, 17761, 17762, 17763, 17764, 17765, 17766, 17767, 17768, 17769, 17770, 17771, 17772, 17773, 17774, 17775, 17776, 17777, 17778, 17779, 17780, 17781, 17782, 17783, 17784, 17785, 17786, 17787, 17788, 17789, 17790, 17791, 17792, 17793, 17794, 17795, 17796, 17797, 17798, 17799, 17800, 17801, 17802, 17803, 17804, 17805, 17806, 17807, 17808, 17809, 17810, 17811, 17812, 17813, 17814, 17815, 17816, 17817, 17818, 17819, 17820, 17821, 17822, 17823, 17824, 17825, 17826, 17827, 17828, 17829, 17830, 17831, 17832, 17833, 17834, 17835, 17836, 17837, 17838, 17839, 17840, 17841, 17842, 17843, 17844, 17845, 17846, 17847, 17848, 17849, 17850, 17851, 17852, 17853, 17854, 17855, 17856, 17857, 17858, 17859, 17860, 17861, 17862, 17863, 17864, 17865, 17866, 17867, 17868, 17869, 17870, 17871, 17872, 17873, 17874, 17875, 17876, 17877, 17878, 17879, 17880, 17881, 17882, 17883, 17884, 17885, 17886, 17887, 17888, 17889, 17890, 17891, 17892, 17893, 17894, 17895, 17896, 17897, 17898, 17899, 17900, 17901, 17902, 17903, 17904, 17905, 17906, 17907, 17908, 17909, 17910, 17911, 17912, 17913, 17914, 17915, 17916, 17917, 17918, 17919, 17920, 17921, 17922, 17923, 17924, 17925, 17926, 17927, 17928, 17929, 17930, 17931, 17932, 17933, 17934, 17935, 17936, 17937, 17938, 17939, 17940, 17941, 17942, 17943, 17944, 17945, 17946, 17947, 17948, 17949, 17950, 17951, 17952, 17953, 17954, 17955, 17956, 17957, 17958, 17959, 17960, 17961, 17962, 17963, 17964, 17965, 17966, 17967, 17968, 17969, 17970, 17971, 17972, 17973, 17974, 17975, 17976, 17977, 17978, 17979, 17980, 17981, 17982, 17983, 17984, 17985, 17986, 17987, 17988, 17989, 17990, 17991, 17992, 17993, 17994, 17995, 17996, 17997, 17998, 17999, 18000, 18001, 18002, 18003, 18004, 18005, 18006, 18007, 18008, 18009, 18010, 18011, 18012, 18013, 18014, 18015, 18016, 18017, 18018, 18019, 18020, 18021, 18022, 18023, 18024, 18025, 18026, 18027, 18028, 18029, 18030, 18031, 18032, 18033, 18034, 18035, 18036, 18037, 18038, 18039, 18040, 18041, 18042, 18043, 18044, 18045, 18046, 18047, 18048, 18049, 18050, 18051, 18052, 18053, 18054, 18055, 18056, 18057, 18058, 18059, 18060, 18061, 18062, 18063, 18064, 18065, 18066, 18067, 18068, 18069, 18070, 18071, 18072, 18073, 18074, 18075, 18076, 18077, 18078, 18079, 18080, 18081, 18082, 18083, 18084, 18085, 18086, 18087, 18088, 18089, 18090, 18091, 18092, 18093, 18094, 18095, 18096, 18097, 18098, 18099, 18100, 18101, 18102, 18103, 18104, 18105, 18106, 18107, 18108, 18109, 18110, 18111, 18112, 18113, 18114, 18115, 18116, 18117, 18118, 18119, 18120, 18121, 18122, 18123, 18124, 18125, 18126, 18127, 18128, 18129, 18130, 18131, 18132, 18133, 18134, 18135, 18136, 18137, 18138, 18139, 18140, 18141, 18142, 18143, 18144, 18145, 18146, 18147, 18148, 18149, 18150, 18151, 18152, 18153, 18154, 18155, 18156, 18157, 18158, 18159, 18160, 18161, 18162, 18163, 18164, 18165, 18166, 18167, 18168, 18169, 18170, 18171, 18172, 18173, 18174, 18175, 18176, 18177, 18178, 18179, 18180, 18181, 18182, 18183, 18184, 18185, 18186, 18187, 18188, 18189, 18190, 18191, 18192, 18193, 18194, 18195, 18196, 18197, 18198, 18199, 18200, 18201, 18202, 18203, 18204, 18205, 18206, 18207, 18208, 18209, 18210, 18211, 18212, 18213, 18214, 18215, 18216, 18217, 18218, 18219, 18220, 18221, 18222, 18223, 18224, 18225, 18226, 18227, 18228, 18229, 18230, 18231, 18232, 18233, 18234, 18235, 18236, 18237, 18238, 18239, 18240, 18241, 18242, 18243, 18244, 18245, 18246, 18247, 18248, 18249, 18250, 18251, 18252, 18253, 18254, 18255, 18256, 18257, 18258, 18259, 18260, 18261, 18262, 18263, 18264, 18265, 18266, 18267, 18268, 18269, 18270, 18271, 18272, 18273, 18274, 18275, 18276, 18277, 18278, 18279, 18280, 18281, 18282, 18283, 18284, 18285, 18286, 18287, 18288, 18289, 18290, 18291, 18292, 18293, 18294, 18295, 18296, 18297, 18298, 18299, 18300, 18301, 18302, 18303, 18304, 18305, 18306, 18307, 18308, 18309, 18310, 18311, 18312, 18313, 18314, 18315, 18316, 18317, 18318, 18319, 18320, 18321, 18322, 18323, 18324, 18325, 18326, 18327, 18328, 18329, 18330, 18331, 18332, 18333, 18334, 18335, 18336, 18337, 18338, 18339, 18340, 18341, 18342, 18343, 18344, 18345, 18346, 18347, 18348, 18349, 18350, 18351, 18352, 18353, 18354, 18355, 18356, 18357, 18358, 18359, 18360, 18361, 18362, 18363, 18364, 18365, 18366, 18367, 18368, 18369, 18370, 18371, 18372, 18373, 18374, 18375, 18376, 18377, 18378, 18379, 18380, 18381, 18382, 18383, 18384, 18385, 18386, 18387, 18388, 18389, 18390, 18391, 18392, 18393, 18394, 18395, 18396, 18397, 18398, 18399, 18400, 18401, 18402, 18403, 18404, 18405, 18406, 18407, 18408, 18409, 18410, 18411, 18412, 18413, 18414, 18415, 18416, 18417, 18418, 18419, 18420, 18421, 18422, 18423, 18424, 18425, 18426, 18427, 18428, 18429, 18430, 18431, 18432, 18433, 18434, 18435, 18436, 18437, 18438, 18439, 18440, 18441, 18442, 18443, 18444, 18445, 18446, 18447, 18448, 18449, 18450, 18451, 18452, 18453, 18454, 18455, 18456, 18457, 18458, 18459, 18460, 18461, 18462, 18463, 18464, 18465, 18466, 18467, 18468, 18469, 18470, 18471, 18472, 18473, 18474, 18475, 18476, 18477, 18478, 18479, 18480, 18481, 18482, 18483, 18484, 18485, 18486, 18487, 18488, 18489, 18490, 18491, 18492, 18493, 18494, 18495, 18496, 18497, 18498, 18499, 18500, 18501, 18502, 18503, 18504, 18505, 18506, 18507, 18508, 18509, 18510, 18511, 18512, 18513, 18514, 18515, 18516, 18517, 18518, 18519, 18520, 18521, 18522, 18523, 18524, 18525, 18526, 18527, 18528, 18529, 18530, 18531, 18532, 18533, 18534, 18535, 18536, 18537, 18538, 18539, 18540, 18541, 18542, 18543, 18544, 18545, 18546, 18547, 18548, 18549, 18550, 18551, 18552, 18553, 18554, 18555, 18556, 18557, 18558, 18559, 18560, 18561, 18562, 18563, 18564, 18565, 18566, 18567, 18568, 18569, 18570, 18571, 18572, 18573, 18574, 18575, 18576, 18577, 18578, 18579, 18580, 18581, 18582, 18583, 18584, 18585, 18586, 18587, 18588, 18589, 18590, 18591, 18592, 18593, 18594, 18595, 18596, 18597, 18598, 18599, 18600, 18601, 18602, 18603, 18604, 18605, 18606, 18607, 18608, 18609, 18610, 18611, 18612, 18613, 18614, 18615, 18616, 18617, 18618, 18619, 18620, 18621, 18622, 18623, 18624, 18625, 18626, 18627, 18628, 18629, 18630, 18631, 18632, 18633, 18634, 18635, 18636, 18637, 18638, 18639, 18640, 18641, 18642, 18643, 18644, 18645, 18646, 18647, 18648, 18649, 18650, 18651, 18652, 18653, 18654, 18655, 18656, 18657, 18658, 18659, 18660, 18661, 18662, 18663, 18664, 18665, 18666, 18667, 18668, 18669, 18670, 18671, 18672, 18673, 18674, 18675, 18676, 18677, 18678, 18679, 18680, 18681, 18682, 18683, 18684, 18685, 18686, 18687, 18688, 18689, 18690, 18691, 18692, 18693, 18694, 18695, 18696, 18697, 18698, 18699, 18700, 18701, 18702, 18703, 18704, 18705, 18706, 18707, 18708, 18709, 18710, 18711, 18712, 18713, 18714, 18715, 18716, 18717, 18718, 18719, 18720, 18721, 18722, 18723, 18724, 18725, 18726, 18727, 18728, 18729, 18730, 18731, 18732, 18733, 18734, 18735, 18736, 18737, 18738, 18739, 18740, 18741, 18742, 18743, 18744, 18745, 18746, 18747, 18748, 18749, 18750, 18751, 18752, 18753, 18754, 18755, 18756, 18757, 18758, 18759, 18760, 18761, 18762, 18763, 18764, 18765, 18766, 18767, 18768, 18769, 18770, 18771, 18772, 18773, 18774, 18775, 18776, 18777, 18778, 18779, 18780, 18781, 18782, 18783, 18784, 18785, 18786, 18787, 18788, 18789, 18790, 18791, 18792, 18793, 18794, 18795, 18796, 18797, 18798, 18799, 18800, 18801, 18802, 18803, 18804, 18805, 18806, 18807, 18808, 18809, 18810, 18811, 18812, 18813, 18814, 18815, 18816, 18817, 18818, 18819, 18820, 18821, 18822, 18823, 18824, 18825, 18826, 18827, 18828, 18829, 18830, 18831, 18832, 18833, 18834, 18835, 18836, 18837, 18838, 18839, 18840, 18841, 18842, 18843, 18844, 18845, 18846, 18847, 18848, 18849, 18850, 18851, 18852, 18853, 18854, 18855, 18856, 18857, 18858, 18859, 18860, 18861, 18862, 18863, 18864, 18865, 18866, 18867, 18868, 18869, 18870, 18871, 18872, 18873, 18874, 18875, 18876, 18877, 18878, 18879, 18880, 18881, 18882, 18883, 18884, 18885, 18886, 18887, 18888, 18889, 18890, 18891, 18892, 18893, 18894, 18895, 18896, 18897, 18898, 18899, 18900, 18901, 18902, 18903, 18904, 18905, 18906, 18907, 18908, 18909, 18910, 18911, 18912, 18913, 18914, 18915, 18916, 18917, 18918, 18919, 18920, 18921, 18922, 18923, 18924, 18925, 18926, 18927, 18928, 18929, 18930, 18931, 18932, 18933, 18934, 18935, 18936, 18937, 18938, 18939, 18940, 18941, 18942, 18943, 18944, 18945, 18946, 18947, 18948, 18949, 18950, 18951, 18952, 18953, 18954, 18955, 18956, 18957, 18958, 18959, 18960, 18961, 18962, 18963, 18964, 18965, 18966, 18967, 18968, 18969, 18970, 18971, 18972, 18973, 18974, 18975, 18976, 18977, 18978, 18979, 18980, 18981, 18982, 18983, 18984, 18985, 18986, 18987, 18988, 18989, 18990, 18991, 18992, 18993, 18994, 18995, 18996, 18997, 18998, 18999, 19000, 19001, 19002, 19003, 19004, 19005, 19006, 19007, 19008, 19009, 19010, 19011, 19012, 19013, 19014, 19015, 19016, 19017, 19018, 19019, 19020, 19021, 19022, 19023, 19024, 19025, 19026, 19027, 19028, 19029, 19030, 19031, 19032, 19033, 19034, 19035, 19036, 19037, 19038, 19039, 19040, 19041, 19042, 19043, 19044, 19045, 19046, 19047, 19048, 19049, 19050, 19051, 19052, 19053, 19054, 19055, 19056, 19057, 19058, 19059, 19060, 19061, 19062, 19063, 19064, 19065, 19066, 19067, 19068, 19069, 19070, 19071, 19072, 19073, 19074, 19075, 19076, 19077, 19078, 19079, 19080, 19081, 19082, 19083, 19084, 19085, 19086, 19087, 19088, 19089, 19090, 19091, 19092, 19093, 19094, 19095, 19096, 19097, 19098, 19099, 19100, 19101, 19102, 19103, 19104, 19105, 19106, 19107, 19108, 19109, 19110, 19111, 19112, 19113, 19114, 19115, 19116, 19117, 19118, 19119, 19120, 19121, 19122, 19123, 19124, 19125, 19126, 19127, 19128, 19129, 19130, 19131, 19132, 19133, 19134, 19135, 19136, 19137, 19138, 19139, 19140, 19141, 19142, 19143, 19144, 19145, 19146, 19147, 19148, 19149, 19150, 19151, 19152, 19153, 19154, 19155, 19156, 19157, 19158, 19159, 19160, 19161, 19162, 19163, 19164, 19165, 19166, 19167, 19168, 19169, 19170, 19171, 19172, 19173, 19174, 19175, 19176, 19177, 19178, 19179, 19180, 19181, 19182, 19183, 19184, 19185, 19186, 19187, 19188, 19189, 19190, 19191, 19192, 19193, 19194, 19195, 19196, 19197, 19198, 19199, 19200, 19201, 19202, 19203, 19204, 19205, 19206, 19207, 19208, 19209, 19210, 19211, 19212, 19213, 19214, 19215, 19216, 19217, 19218, 19219, 19220, 19221, 19222, 19223, 19224, 19225, 19226, 19227, 19228, 19229, 19230, 19231, 19232, 19233, 19234, 19235, 19236, 19237, 19238, 19239, 19240, 19241, 19242, 19243, 19244, 19245, 19246, 19247, 19248, 19249, 19250, 19251, 19252, 19253, 19254, 19255, 19256, 19257, 19258, 19259, 19260, 19261, 19262, 19263, 19264, 19265, 19266, 19267, 19268, 19269, 19270, 19271, 19272, 19273, 19274, 19275, 19276, 19277, 19278, 19279, 19280, 19281, 19282, 19283, 19284, 19285, 19286, 19287, 19288, 19289, 19290, 19291, 19292, 19293, 19294, 19295, 19296, 19297, 19298, 19299, 19300, 19301, 19302, 19303, 19304, 19305, 19306, 19307, 19308, 19309, 19310, 19311, 19312, 19313, 19314, 19315, 19316, 19317, 19318, 19319, 19320, 19321, 19322, 19323, 19324, 19325, 19326, 19327, 19328, 19329, 19330, 19331, 19332, 19333, 19334, 19335, 19336, 19337, 19338, 19339, 19340, 19341, 19342, 19343, 19344, 19345, 19346, 19347, 19348, 19349, 19350, 19351, 19352, 19353, 19354, 19355, 19356, 19357, 19358, 19359, 19360, 19361, 19362, 19363, 19364, 19365, 19366, 19367, 19368, 19369, 19370, 19371, 19372, 19373, 19374, 19375, 19376, 19377, 19378, 19379, 19380, 19381, 19382, 19383, 19384, 19385, 19386, 19387, 19388, 19389, 19390, 19391, 19392, 19393, 19394, 19395, 19396, 19397, 19398, 19399, 19400, 19401, 19402, 19403, 19404, 19405, 19406, 19407, 19408, 19409, 19410, 19411, 19412, 19413, 19414, 19415, 19416, 19417, 19418, 19419, 19420, 19421, 19422, 19423, 19424, 19425, 19426, 19427, 19428, 19429, 19430, 19431, 19432, 19433, 19434, 19435, 19436, 19437, 19438, 19439, 19440, 19441, 19442, 19443, 19444, 19445, 19446, 19447, 19448, 19449, 19450, 19451, 19452, 19453, 19454, 19455, 19456, 19457, 19458, 19459, 19460, 19461, 19462, 19463, 19464, 19465, 19466, 19467, 19468, 19469, 19470, 19471, 19472, 19473, 19474, 19475, 19476, 19477, 19478, 19479, 19480, 19481, 19482, 19483, 19484, 19485, 19486, 19487, 19488, 19489, 19490, 19491, 19492, 19493, 19494, 19495, 19496, 19497, 19498, 19499, 19500, 19501, 19502, 19503, 19504, 19505, 19506, 19507, 19508, 19509, 19510, 19511, 19512, 19513, 19514, 19515, 19516, 19517, 19518, 19519, 19520, 19521, 19522, 19523, 19524, 19525, 19526, 19527, 19528, 19529, 19530, 19531, 19532, 19533, 19534, 19535, 19536, 19537, 19538, 19539, 19540, 19541, 19542, 19543, 19544, 19545, 19546, 19547, 19548, 19549, 19550, 19551, 19552, 19553, 19554, 19555, 19556, 19557, 19558, 19559, 19560, 19561, 19562, 19563, 19564, 19565, 19566, 19567, 19568, 19569, 19570, 19571, 19572, 19573, 19574, 19575, 19576, 19577, 19578, 19579, 19580, 19581, 19582, 19583, 19584, 19585, 19586, 19587, 19588, 19589, 19590, 19591, 19592, 19593, 19594, 19595, 19596, 19597, 19598, 19599, 19600, 19601, 19602, 19603, 19604, 19605, 19606, 19607, 19608, 19609, 19610, 19611, 19612, 19613, 19614, 19615, 19616, 19617, 19618, 19619, 19620, 19621, 19622, 19623, 19624, 19625, 19626, 19627, 19628, 19629, 19630, 19631, 19632, 19633, 19634, 19635, 19636, 19637, 19638, 19639, 19640, 19641, 19642, 19643, 19644, 19645, 19646, 19647, 19648, 19649, 19650, 19651, 19652, 19653, 19654, 19655, 19656, 19657, 19658, 19659, 19660, 19661, 19662, 19663, 19664, 19665, 19666, 19667, 19668, 19669, 19670, 19671, 19672, 19673, 19674, 19675, 19676, 19677, 19678, 19679, 19680, 19681, 19682, 19683, 19684, 19685, 19686, 19687, 19688, 19689, 19690, 19691, 19692, 19693, 19694, 19695, 19696, 19697, 19698, 19699, 19700, 19701, 19702, 19703, 19704, 19705, 19706, 19707, 19708, 19709, 19710, 19711, 19712, 19713, 19714, 19715, 19716, 19717, 19718, 19719, 19720, 19721, 19722, 19723, 19724, 19725, 19726, 19727, 19728, 19729, 19730, 19731, 19732, 19733, 19734, 19735, 19736, 19737, 19738, 19739, 19740, 19741, 19742, 19743, 19744, 19745, 19746, 19747, 19748, 19749, 19750, 19751, 19752, 19753, 19754, 19755, 19756, 19757, 19758, 19759, 19760, 19761, 19762, 19763, 19764, 19765, 19766, 19767, 19768, 19769, 19770, 19771, 19772, 19773, 19774, 19775, 19776, 19777, 19778, 19779, 19780, 19781, 19782, 19783, 19784, 19785, 19786, 19787, 19788, 19789, 19790, 19791, 19792, 19793, 19794, 19795, 19796, 19797, 19798, 19799, 19800, 19801, 19802, 19803, 19804, 19805, 19806, 19807, 19808, 19809, 19810, 19811, 19812, 19813, 19814, 19815, 19816, 19817, 19818, 19819, 19820, 19821, 19822, 19823, 19824, 19825, 19826, 19827, 19828, 19829, 19830, 19831, 19832, 19833, 19834, 19835, 19836, 19837, 19838, 19839, 19840, 19841, 19842, 19843, 19844, 19845, 19846, 19847, 19848, 19849, 19850, 19851, 19852, 19853, 19854, 19855, 19856, 19857, 19858, 19859, 19860, 19861, 19862, 19863, 19864, 19865, 19866, 19867, 19868, 19869, 19870, 19871, 19872, 19873, 19874, 19875, 19876, 19877, 19878, 19879, 19880, 19881, 19882, 19883, 19884, 19885, 19886, 19887, 19888, 19889, 19890, 19891, 19892, 19893, 19894, 19895, 19896, 19897, 19898, 19899, 19900, 19901, 19902, 19903, 19904, 19905, 19906, 19907, 19908, 19909, 19910, 19911, 19912, 19913, 19914, 19915, 19916, 19917, 19918, 19919, 19920, 19921, 19922, 19923, 19924, 19925, 19926, 19927, 19928, 19929, 19930, 19931, 19932, 19933, 19934, 19935, 19936, 19937, 19938, 19939, 19940, 19941, 19942, 19943, 19944, 19945, 19946, 19947, 19948, 19949, 19950, 19951, 19952, 19953, 19954, 19955, 19956, 19957, 19958, 19959, 19960, 19961, 19962, 19963, 19964, 19965, 19966, 19967, 19968, 19969, 19970, 19971, 19972, 19973, 19974, 19975, 19976, 19977, 19978, 19979, 19980, 19981, 19982, 19983, 19984, 19985, 19986, 19987, 19988, 19989, 19990, 19991, 19992, 19993, 19994, 19995, 19996, 19997, 19998, 19999, 20000, 20001, 20002, 20003, 20004, 20005, 20006, 20007, 20008, 20009, 20010, 20011, 20012, 20013, 20014, 20015, 20016, 20017, 20018, 20019, 20020, 20021, 20022, 20023, 20024, 20025, 20026, 20027, 20028, 20029, 20030, 20031, 20032, 20033, 20034, 20035, 20036, 20037, 20038, 20039, 20040, 20041, 20042, 20043, 20044, 20045, 20046, 20047, 20048, 20049, 20050, 20051, 20052, 20053, 20054, 20055, 20056, 20057, 20058, 20059, 20060, 20061, 20062, 20063, 20064, 20065, 20066, 20067, 20068, 20069, 20070, 20071, 20072, 20073, 20074, 20075, 20076, 20077, 20078, 20079, 20080, 20081, 20082, 20083, 20084, 20085, 20086, 20087, 20088, 20089, 20090, 20091, 20092, 20093, 20094, 20095, 20096, 20097, 20098, 20099, 20100, 20101, 20102, 20103, 20104, 20105, 20106, 20107, 20108, 20109, 20110, 20111, 20112, 20113, 20114, 20115, 20116, 20117, 20118, 20119, 20120, 20121, 20122, 20123, 20124, 20125, 20126, 20127, 20128, 20129, 20130, 20131, 20132, 20133, 20134, 20135, 20136, 20137, 20138, 20139, 20140, 20141, 20142, 20143, 20144, 20145, 20146, 20147, 20148, 20149, 20150, 20151, 20152, 20153, 20154, 20155, 20156, 20157, 20158, 20159, 20160, 20161, 20162, 20163, 20164, 20165, 20166, 20167, 20168, 20169, 20170, 20171, 20172, 20173, 20174, 20175, 20176, 20177, 20178, 20179, 20180, 20181, 20182, 20183, 20184, 20185, 20186, 20187, 20188, 20189, 20190, 20191, 20192, 20193, 20194, 20195, 20196, 20197, 20198, 20199, 20200, 20201, 20202, 20203, 20204, 20205, 20206, 20207, 20208, 20209, 20210, 20211, 20212, 20213, 20214, 20215, 20216, 20217, 20218, 20219, 20220, 20221, 20222, 20223, 20224, 20225, 20226, 20227, 20228, 20229, 20230, 20231, 20232, 20233, 20234, 20235, 20236, 20237, 20238, 20239, 20240, 20241, 20242, 20243, 20244, 20245, 20246, 20247, 20248, 20249, 20250, 20251, 20252, 20253, 20254, 20255, 20256, 20257, 20258, 20259, 20260, 20261, 20262, 20263, 20264, 20265, 20266, 20267, 20268, 20269, 20270, 20271, 20272, 20273, 20274, 20275, 20276, 20277, 20278, 20279, 20280, 20281, 20282, 20283, 20284, 20285, 20286, 20287, 20288, 20289, 20290, 20291, 20292, 20293, 20294, 20295, 20296, 20297, 20298, 20299, 20300, 20301, 20302, 20303, 20304, 20305, 20306, 20307, 20308, 20309, 20310, 20311, 20312, 20313, 20314, 20315, 20316, 20317, 20318, 20319, 20320, 20321, 20322, 20323, 20324, 20325, 20326, 20327, 20328, 20329, 20330, 20331, 20332, 20333, 20334, 20335, 20336, 20337, 20338, 20339, 20340, 20341, 20342, 20343, 20344, 20345, 20346, 20347, 20348, 20349, 20350, 20351, 20352, 20353, 20354, 20355, 20356, 20357, 20358, 20359, 20360, 20361, 20362, 20363, 20364, 20365, 20366, 20367, 20368, 20369, 20370, 20371, 20372, 20373, 20374, 20375, 20376, 20377, 20378, 20379, 20380, 20381, 20382, 20383, 20384, 20385, 20386, 20387, 20388, 20389, 20390, 20391, 20392, 20393, 20394, 20395, 20396, 20397, 20398, 20399, 20400, 20401, 20402, 20403, 20404, 20405, 20406, 20407, 20408, 20409, 20410, 20411, 20412, 20413, 20414, 20415, 20416, 20417, 20418, 20419, 20420, 20421, 20422, 20423, 20424, 20425, 20426, 20427, 20428, 20429, 20430, 20431, 20432, 20433, 20434, 20435, 20436, 20437, 20438, 20439, 20440, 20441, 20442, 20443, 20444, 20445, 20446, 20447, 20448, 20449, 20450, 20451, 20452, 20453, 20454, 20455, 20456, 20457, 20458, 20459, 20460, 20461, 20462, 20463, 20464, 20465, 20466, 20467, 20468, 20469, 20470, 20471, 20472, 20473, 20474, 20475, 20476, 20477, 20478, 20479, 20480, 20481, 20482, 20483, 20484, 20485, 20486, 20487, 20488, 20489, 20490, 20491, 20492, 20493, 20494, 20495, 20496, 20497, 20498, 20499, 20500, 20501, 20502, 20503, 20504, 20505, 20506, 20507, 20508, 20509, 20510, 20511, 20512, 20513, 20514, 20515, 20516, 20517, 20518, 20519, 20520, 20521, 20522, 20523, 20524, 20525, 20526, 20527, 20528, 20529, 20530, 20531, 20532, 20533, 20534, 20535, 20536, 20537, 20538, 20539, 20540, 20541, 20542, 20543, 20544, 20545, 20546, 20547, 20548, 20549, 20550, 20551, 20552, 20553, 20554, 20555, 20556, 20557, 20558, 20559, 20560, 20561, 20562, 20563, 20564, 20565, 20566, 20567, 20568, 20569, 20570, 20571, 20572, 20573, 20574, 20575, 20576, 20577, 20578, 20579, 20580, 20581, 20582, 20583, 20584, 20585, 20586, 20587, 20588, 20589, 20590, 20591, 20592, 20593, 20594, 20595, 20596, 20597, 20598, 20599, 20600, 20601, 20602, 20603, 20604, 20605, 20606, 20607, 20608, 20609, 20610, 20611, 20612, 20613, 20614, 20615, 20616, 20617, 20618, 20619, 20620, 20621, 20622, 20623, 20624, 20625, 20626, 20627, 20628, 20629, 20630, 20631, 20632, 20633, 20634, 20635, 20636, 20637, 20638, 20639, 20640, 20641, 20642, 20643, 20644, 20645, 20646, 20647, 20648, 20649, 20650, 20651, 20652, 20653, 20654, 20655, 20656, 20657, 20658, 20659, 20660, 20661, 20662, 20663, 20664, 20665, 20666, 20667, 20668, 20669, 20670, 20671, 20672, 20673, 20674, 20675, 20676, 20677, 20678, 20679, 20680, 20681, 20682, 20683, 20684, 20685, 20686, 20687, 20688, 20689, 20690, 20691, 20692, 20693, 20694, 20695, 20696, 20697, 20698, 20699, 20700, 20701, 20702, 20703, 20704, 20705, 20706, 20707, 20708, 20709, 20710, 20711, 20712, 20713, 20714, 20715, 20716, 20717, 20718, 20719, 20720, 20721, 20722, 20723, 20724, 20725, 20726, 20727, 20728, 20729, 20730, 20731, 20732, 20733, 20734, 20735, 20736, 20737, 20738, 20739, 20740, 20741, 20742, 20743, 20744, 20745, 20746, 20747, 20748, 20749, 20750, 20751, 20752, 20753, 20754, 20755, 20756, 20757, 20758, 20759, 20760, 20761, 20762, 20763, 20764, 20765, 20766, 20767, 20768, 20769, 20770, 20771, 20772, 20773, 20774, 20775, 20776, 20777, 20778, 20779, 20780, 20781, 20782, 20783, 20784, 20785, 20786, 20787, 20788, 20789, 20790, 20791, 20792, 20793, 20794, 20795, 20796, 20797, 20798, 20799, 20800, 20801, 20802, 20803, 20804, 20805, 20806, 20807, 20808, 20809, 20810, 20811, 20812, 20813, 20814, 20815, 20816, 20817, 20818, 20819, 20820, 20821, 20822, 20823, 20824, 20825, 20826, 20827, 20828, 20829, 20830, 20831, 20832, 20833, 20834, 20835, 20836, 20837, 20838, 20839, 20840, 20841, 20842, 20843, 20844, 20845, 20846, 20847, 20848, 20849, 20850, 20851, 20852, 20853, 20854, 20855, 20856, 20857, 20858, 20859, 20860, 20861, 20862, 20863, 20864, 20865, 20866, 20867, 20868, 20869, 20870, 20871, 20872, 20873, 20874, 20875, 20876, 20877, 20878, 20879, 20880, 20881, 20882, 20883, 20884, 20885, 20886, 20887, 20888, 20889, 20890, 20891, 20892, 20893, 20894, 20895, 20896, 20897, 20898, 20899, 20900, 20901, 20902, 20903, 20904, 20905, 20906, 20907, 20908, 20909, 20910, 20911, 20912, 20913, 20914, 20915, 20916, 20917, 20918, 20919, 20920, 20921, 20922, 20923, 20924, 20925, 20926, 20927, 20928, 20929, 20930, 20931, 20932, 20933, 20934, 20935, 20936, 20937, 20938, 20939, 20940, 20941, 20942, 20943, 20944, 20945, 20946, 20947, 20948, 20949, 20950, 20951, 20952, 20953, 20954, 20955, 20956, 20957, 20958, 20959, 20960, 20961, 20962, 20963, 20964, 20965, 20966, 20967, 20968, 20969, 20970, 20971, 20972, 20973, 20974, 20975, 20976, 20977, 20978, 20979, 20980, 20981, 20982, 20983, 20984, 20985, 20986, 20987, 20988, 20989, 20990, 20991, 20992, 20993, 20994, 20995, 20996, 20997, 20998, 20999, 21000, 21001, 21002, 21003, 21004, 21005, 21006, 21007, 21008, 21009, 21010, 21011, 21012, 21013, 21014, 21015, 21016, 21017, 21018, 21019, 21020, 21021, 21022, 21023, 21024, 21025, 21026, 21027, 21028, 21029, 21030, 21031, 21032, 21033, 21034, 21035, 21036, 21037, 21038, 21039, 21040, 21041, 21042, 21043, 21044, 21045, 21046, 21047, 21048, 21049, 21050, 21051, 21052, 21053, 21054, 21055, 21056, 21057, 21058, 21059, 21060, 21061, 21062, 21063, 21064, 21065, 21066, 21067, 21068, 21069, 21070, 21071, 21072, 21073, 21074, 21075, 21076, 21077, 21078, 21079, 21080, 21081, 21082, 21083, 21084, 21085, 21086, 21087, 21088, 21089, 21090, 21091, 21092, 21093, 21094, 21095, 21096, 21097, 21098, 21099, 21100, 21101, 21102, 21103, 21104, 21105, 21106, 21107, 21108, 21109, 21110, 21111, 21112, 21113, 21114, 21115, 21116, 21117, 21118, 21119, 21120, 21121, 21122, 21123, 21124, 21125, 21126, 21127, 21128, 21129, 21130, 21131, 21132, 21133, 21134, 21135, 21136, 21137, 21138, 21139, 21140, 21141, 21142, 21143, 21144, 21145, 21146, 21147, 21148, 21149, 21150, 21151, 21152, 21153, 21154, 21155, 21156, 21157, 21158, 21159, 21160, 21161, 21162, 21163, 21164, 21165, 21166, 21167, 21168, 21169, 21170, 21171, 21172, 21173, 21174, 21175, 21176, 21177, 21178, 21179, 21180, 21181, 21182, 21183, 21184, 21185, 21186, 21187, 21188, 21189, 21190, 21191, 21192, 21193, 21194, 21195, 21196, 21197, 21198, 21199, 21200, 21201, 21202, 21203, 21204, 21205, 21206, 21207, 21208, 21209, 21210, 21211, 21212, 21213, 21214, 21215, 21216, 21217, 21218, 21219, 21220, 21221, 21222, 21223, 21224, 21225, 21226, 21227, 21228, 21229, 21230, 21231, 21232, 21233, 21234, 21235, 21236, 21237, 21238, 21239, 21240, 21241, 21242, 21243, 21244, 21245, 21246, 21247, 21248, 21249, 21250, 21251, 21252, 21253, 21254, 21255, 21256, 21257, 21258, 21259, 21260, 21261, 21262, 21263, 21264, 21265, 21266, 21267, 21268, 21269, 21270, 21271, 21272, 21273, 21274, 21275, 21276, 21277, 21278, 21279, 21280, 21281, 21282, 21283, 21284, 21285, 21286, 21287, 21288, 21289, 21290, 21291, 21292, 21293, 21294, 21295, 21296, 21297, 21298, 21299, 21300, 21301, 21302, 21303, 21304, 21305, 21306, 21307, 21308, 21309, 21310, 21311, 21312, 21313, 21314, 21315, 21316, 21317, 21318, 21319, 21320, 21321, 21322, 21323, 21324, 21325, 21326, 21327, 21328, 21329, 21330, 21331, 21332, 21333, 21334, 21335, 21336, 21337, 21338, 21339, 21340, 21341, 21342, 21343, 21344, 21345, 21346, 21347, 21348, 21349, 21350, 21351, 21352, 21353, 21354, 21355, 21356, 21357, 21358, 21359, 21360, 21361, 21362, 21363, 21364, 21365, 21366, 21367, 21368, 21369, 21370, 21371, 21372, 21373, 21374, 21375, 21376, 21377, 21378, 21379, 21380, 21381, 21382, 21383, 21384, 21385, 21386, 21387, 21388, 21389, 21390, 21391, 21392, 21393, 21394, 21395, 21396, 21397, 21398, 21399, 21400, 21401, 21402, 21403, 21404, 21405, 21406, 21407, 21408, 21409, 21410, 21411, 21412, 21413, 21414, 21415, 21416, 21417, 21418, 21419, 21420, 21421, 21422, 21423, 21424, 21425, 21426, 21427, 21428, 21429, 21430, 21431, 21432, 21433, 21434, 21435, 21436, 21437, 21438, 21439, 21440, 21441, 21442, 21443, 21444, 21445, 21446, 21447, 21448, 21449, 21450, 21451, 21452, 21453, 21454, 21455, 21456, 21457, 21458, 21459, 21460, 21461, 21462, 21463, 21464, 21465, 21466, 21467, 21468, 21469, 21470, 21471, 21472, 21473, 21474, 21475, 21476, 21477, 21478, 21479, 21480, 21481, 21482, 21483, 21484, 21485, 21486, 21487, 21488, 21489, 21490, 21491, 21492, 21493, 21494, 21495, 21496, 21497, 21498, 21499, 21500, 21501, 21502, 21503, 21504, 21505, 21506, 21507, 21508, 21509, 21510, 21511, 21512, 21513, 21514, 21515, 21516, 21517, 21518, 21519, 21520, 21521, 21522, 21523, 21524, 21525, 21526, 21527, 21528, 21529, 21530, 21531, 21532, 21533, 21534, 21535, 21536, 21537, 21538, 21539, 21540, 21541, 21542, 21543, 21544, 21545, 21546, 21547, 21548, 21549, 21550, 21551, 21552, 21553, 21554, 21555, 21556, 21557, 21558, 21559, 21560, 21561, 21562, 21563, 21564, 21565, 21566, 21567, 21568, 21569, 21570, 21571, 21572, 21573, 21574, 21575, 21576, 21577, 21578, 21579, 21580, 21581, 21582, 21583, 21584, 21585, 21586, 21587, 21588, 21589, 21590, 21591, 21592, 21593, 21594, 21595, 21596, 21597, 21598, 21599, 21600, 21601, 21602, 21603, 21604, 21605, 21606, 21607, 21608, 21609, 21610, 21611, 21612, 21613, 21614, 21615, 21616, 21617, 21618, 21619, 21620, 21621, 21622, 21623, 21624, 21625, 21626, 21627, 21628, 21629, 21630, 21631, 21632, 21633, 21634, 21635, 21636, 21637, 21638, 21639, 21640, 21641, 21642, 21643, 21644, 21645, 21646, 21647, 21648, 21649, 21650, 21651, 21652, 21653, 21654, 21655, 21656, 21657, 21658, 21659, 21660, 21661, 21662, 21663, 21664, 21665, 21666, 21667, 21668, 21669, 21670, 21671, 21672, 21673, 21674, 21675, 21676, 21677, 21678, 21679, 21680, 21681, 21682, 21683, 21684, 21685, 21686, 21687, 21688, 21689, 21690, 21691, 21692, 21693, 21694, 21695, 21696, 21697, 21698, 21699, 21700, 21701, 21702, 21703, 21704, 21705, 21706, 21707, 21708, 21709, 21710, 21711, 21712, 21713, 21714, 21715, 21716, 21717, 21718, 21719, 21720, 21721, 21722, 21723, 21724, 21725, 21726, 21727, 21728, 21729, 21730, 21731, 21732, 21733, 21734, 21735, 21736, 21737, 21738, 21739, 21740, 21741, 21742, 21743, 21744, 21745, 21746, 21747, 21748, 21749, 21750, 21751, 21752, 21753, 21754, 21755, 21756, 21757, 21758, 21759, 21760, 21761, 21762, 21763, 21764, 21765, 21766, 21767, 21768, 21769, 21770, 21771, 21772, 21773, 21774, 21775, 21776, 21777, 21778, 21779, 21780, 21781, 21782, 21783, 21784, 21785, 21786, 21787, 21788, 21789, 21790, 21791, 21792, 21793, 21794, 21795, 21796, 21797, 21798, 21799, 21800, 21801, 21802, 21803, 21804, 21805, 21806, 21807, 21808, 21809, 21810, 21811, 21812, 21813, 21814, 21815, 21816, 21817, 21818, 21819, 21820, 21821, 21822, 21823, 21824, 21825, 21826, 21827, 21828, 21829, 21830, 21831, 21832, 21833, 21834, 21835, 21836, 21837, 21838, 21839, 21840, 21841, 21842, 21843, 21844, 21845, 21846, 21847, 21848, 21849, 21850, 21851, 21852, 21853, 21854, 21855, 21856, 21857, 21858, 21859, 21860, 21861, 21862, 21863, 21864, 21865, 21866, 21867, 21868, 21869, 21870, 21871, 21872, 21873, 21874, 21875, 21876, 21877, 21878, 21879, 21880, 21881, 21882, 21883, 21884, 21885, 21886, 21887, 21888, 21889, 21890, 21891, 21892, 21893, 21894, 21895, 21896, 21897, 21898, 21899, 21900, 21901, 21902, 21903, 21904, 21905, 21906, 21907, 21908, 21909, 21910, 21911, 21912, 21913, 21914, 21915, 21916, 21917, 21918, 21919, 21920, 21921, 21922, 21923, 21924, 21925, 21926, 21927, 21928, 21929, 21930, 21931, 21932, 21933, 21934, 21935, 21936, 21937, 21938, 21939, 21940, 21941, 21942, 21943, 21944, 21945, 21946, 21947, 21948, 21949, 21950, 21951, 21952, 21953, 21954, 21955, 21956, 21957, 21958, 21959, 21960, 21961, 21962, 21963, 21964, 21965, 21966, 21967, 21968, 21969, 21970, 21971, 21972, 21973, 21974, 21975, 21976, 21977, 21978, 21979, 21980, 21981, 21982, 21983, 21984, 21985, 21986, 21987, 21988, 21989, 21990, 21991, 21992, 21993, 21994, 21995, 21996, 21997, 21998, 21999, 22000, 22001, 22002, 22003, 22004, 22005, 22006, 22007, 22008, 22009, 22010, 22011, 22012, 22013, 22014, 22015, 22016, 22017, 22018, 22019, 22020, 22021, 22022, 22023, 22024, 22025, 22026, 22027, 22028, 22029, 22030, 22031, 22032, 22033, 22034, 22035, 22036, 22037, 22038, 22039, 22040, 22041, 22042, 22043, 22044, 22045, 22046, 22047, 22048, 22049, 22050, 22051, 22052, 22053, 22054, 22055, 22056, 22057, 22058, 22059, 22060, 22061, 22062, 22063, 22064, 22065, 22066, 22067, 22068, 22069, 22070, 22071, 22072, 22073, 22074, 22075, 22076, 22077, 22078, 22079, 22080, 22081, 22082, 22083, 22084, 22085, 22086, 22087, 22088, 22089, 22090, 22091, 22092, 22093, 22094, 22095, 22096, 22097, 22098, 22099, 22100, 22101, 22102, 22103, 22104, 22105, 22106, 22107, 22108, 22109, 22110, 22111, 22112, 22113, 22114, 22115, 22116, 22117, 22118, 22119, 22120, 22121, 22122, 22123, 22124, 22125, 22126, 22127, 22128, 22129, 22130, 22131, 22132, 22133, 22134, 22135, 22136, 22137, 22138, 22139, 22140, 22141, 22142, 22143, 22144, 22145, 22146, 22147, 22148, 22149, 22150, 22151, 22152, 22153, 22154, 22155, 22156, 22157, 22158, 22159, 22160, 22161, 22162, 22163, 22164, 22165, 22166, 22167, 22168, 22169, 22170, 22171, 22172, 22173, 22174, 22175, 22176, 22177, 22178, 22179, 22180, 22181, 22182, 22183, 22184, 22185, 22186, 22187, 22188, 22189, 22190, 22191, 22192, 22193, 22194, 22195, 22196, 22197, 22198, 22199, 22200, 22201, 22202, 22203, 22204, 22205, 22206, 22207, 22208, 22209, 22210, 22211, 22212, 22213, 22214, 22215, 22216, 22217, 22218, 22219, 22220, 22221, 22222, 22223, 22224, 22225, 22226, 22227, 22228, 22229, 22230, 22231, 22232, 22233, 22234, 22235, 22236, 22237, 22238, 22239, 22240, 22241, 22242, 22243, 22244, 22245, 22246, 22247, 22248, 22249, 22250, 22251, 22252, 22253, 22254, 22255, 22256, 22257, 22258, 22259, 22260, 22261, 22262, 22263, 22264, 22265, 22266, 22267, 22268, 22269, 22270, 22271, 22272, 22273, 22274, 22275, 22276, 22277, 22278, 22279, 22280, 22281, 22282, 22283, 22284, 22285, 22286, 22287, 22288, 22289, 22290, 22291, 22292, 22293, 22294, 22295, 22296, 22297, 22298, 22299, 22300, 22301, 22302, 22303, 22304, 22305, 22306, 22307, 22308, 22309, 22310, 22311, 22312, 22313, 22314, 22315, 22316, 22317, 22318, 22319, 22320, 22321, 22322, 22323, 22324, 22325, 22326, 22327, 22328, 22329, 22330, 22331, 22332, 22333, 22334, 22335, 22336, 22337, 22338, 22339, 22340, 22341, 22342, 22343, 22344, 22345, 22346, 22347, 22348, 22349, 22350, 22351, 22352, 22353, 22354, 22355, 22356, 22357, 22358, 22359, 22360, 22361, 22362, 22363, 22364, 22365, 22366, 22367, 22368, 22369, 22370, 22371, 22372, 22373, 22374, 22375, 22376, 22377, 22378, 22379, 22380, 22381, 22382, 22383, 22384, 22385, 22386, 22387, 22388, 22389, 22390, 22391, 22392, 22393, 22394, 22395, 22396, 22397, 22398, 22399, 22400, 22401, 22402, 22403, 22404, 22405, 22406, 22407, 22408, 22409, 22410, 22411, 22412, 22413, 22414, 22415, 22416, 22417, 22418, 22419, 22420, 22421, 22422, 22423, 22424, 22425, 22426, 22427, 22428, 22429, 22430, 22431, 22432, 22433, 22434, 22435, 22436, 22437, 22438, 22439, 22440, 22441, 22442, 22443, 22444, 22445, 22446, 22447, 22448, 22449, 22450, 22451, 22452, 22453, 22454, 22455, 22456, 22457, 22458, 22459, 22460, 22461, 22462, 22463, 22464, 22465, 22466, 22467, 22468, 22469, 22470, 22471, 22472, 22473, 22474, 22475, 22476, 22477, 22478, 22479, 22480, 22481, 22482, 22483, 22484, 22485, 22486, 22487, 22488, 22489, 22490, 22491, 22492, 22493, 22494, 22495, 22496, 22497, 22498, 22499, 22500, 22501, 22502, 22503, 22504, 22505, 22506, 22507, 22508, 22509, 22510, 22511, 22512, 22513, 22514, 22515, 22516, 22517, 22518, 22519, 22520, 22521, 22522, 22523, 22524, 22525, 22526, 22527, 22528, 22529, 22530, 22531, 22532, 22533, 22534, 22535, 22536, 22537, 22538, 22539, 22540, 22541, 22542, 22543, 22544, 22545, 22546, 22547, 22548, 22549, 22550, 22551, 22552, 22553, 22554, 22555, 22556, 22557, 22558, 22559, 22560, 22561, 22562, 22563, 22564, 22565, 22566, 22567, 22568, 22569, 22570, 22571, 22572, 22573, 22574, 22575, 22576, 22577, 22578, 22579, 22580, 22581, 22582, 22583, 22584, 22585, 22586, 22587, 22588, 22589, 22590, 22591, 22592, 22593, 22594, 22595, 22596, 22597, 22598, 22599, 22600, 22601, 22602, 22603, 22604, 22605, 22606, 22607, 22608, 22609, 22610, 22611, 22612, 22613, 22614, 22615, 22616, 22617, 22618, 22619, 22620, 22621, 22622, 22623, 22624, 22625, 22626, 22627, 22628, 22629, 22630, 22631, 22632, 22633, 22634, 22635, 22636, 22637, 22638, 22639, 22640, 22641, 22642, 22643, 22644, 22645, 22646, 22647, 22648, 22649, 22650, 22651, 22652, 22653, 22654, 22655, 22656, 22657, 22658, 22659, 22660, 22661, 22662, 22663, 22664, 22665, 22666, 22667, 22668, 22669, 22670, 22671, 22672, 22673, 22674, 22675, 22676, 22677, 22678, 22679, 22680, 22681, 22682, 22683, 22684, 22685, 22686, 22687, 22688, 22689, 22690, 22691, 22692, 22693, 22694, 22695, 22696, 22697, 22698, 22699, 22700, 22701, 22702, 22703, 22704, 22705, 22706, 22707, 22708, 22709, 22710, 22711, 22712, 22713, 22714, 22715, 22716, 22717, 22718, 22719, 22720, 22721, 22722, 22723, 22724, 22725, 22726, 22727, 22728, 22729, 22730, 22731, 22732, 22733, 22734, 22735, 22736, 22737, 22738, 22739, 22740, 22741, 22742, 22743, 22744, 22745, 22746, 22747, 22748, 22749, 22750, 22751, 22752, 22753, 22754, 22755, 22756, 22757, 22758, 22759, 22760, 22761, 22762, 22763, 22764, 22765, 22766, 22767, 22768, 22769, 22770, 22771, 22772, 22773, 22774, 22775, 22776, 22777, 22778, 22779, 22780, 22781, 22782, 22783, 22784, 22785, 22786, 22787, 22788, 22789, 22790, 22791, 22792, 22793, 22794, 22795, 22796, 22797, 22798, 22799, 22800, 22801, 22802, 22803, 22804, 22805, 22806, 22807, 22808, 22809, 22810, 22811, 22812, 22813, 22814, 22815, 22816, 22817, 22818, 22819, 22820, 22821, 22822, 22823, 22824, 22825, 22826, 22827, 22828, 22829, 22830, 22831, 22832, 22833, 22834, 22835, 22836, 22837, 22838, 22839, 22840, 22841, 22842, 22843, 22844, 22845, 22846, 22847, 22848, 22849, 22850, 22851, 22852, 22853, 22854, 22855, 22856, 22857, 22858, 22859, 22860, 22861, 22862, 22863, 22864, 22865, 22866, 22867, 22868, 22869, 22870, 22871, 22872, 22873, 22874, 22875, 22876, 22877, 22878, 22879, 22880, 22881, 22882, 22883, 22884, 22885, 22886, 22887, 22888, 22889, 22890, 22891, 22892, 22893, 22894, 22895, 22896, 22897, 22898, 22899, 22900, 22901, 22902, 22903, 22904, 22905, 22906, 22907, 22908, 22909, 22910, 22911, 22912, 22913, 22914, 22915, 22916, 22917, 22918, 22919, 22920, 22921, 22922, 22923, 22924, 22925, 22926, 22927, 22928, 22929, 22930, 22931, 22932, 22933, 22934, 22935, 22936, 22937, 22938, 22939, 22940, 22941, 22942, 22943, 22944, 22945, 22946, 22947, 22948, 22949, 22950, 22951, 22952, 22953, 22954, 22955, 22956, 22957, 22958, 22959, 22960, 22961, 22962, 22963, 22964, 22965, 22966, 22967, 22968, 22969, 22970, 22971, 22972, 22973, 22974, 22975, 22976, 22977, 22978, 22979, 22980, 22981, 22982, 22983, 22984, 22985, 22986, 22987, 22988, 22989, 22990, 22991, 22992, 22993, 22994, 22995, 22996, 22997, 22998, 22999, 23000, 23001, 23002, 23003, 23004, 23005, 23006, 23007, 23008, 23009, 23010, 23011, 23012, 23013, 23014, 23015, 23016, 23017, 23018, 23019, 23020, 23021, 23022, 23023, 23024, 23025, 23026, 23027, 23028, 23029, 23030, 23031, 23032, 23033, 23034, 23035, 23036, 23037, 23038, 23039, 23040, 23041, 23042, 23043, 23044, 23045, 23046, 23047, 23048, 23049, 23050, 23051, 23052, 23053, 23054, 23055, 23056, 23057, 23058, 23059, 23060, 23061, 23062, 23063, 23064, 23065, 23066, 23067, 23068, 23069, 23070, 23071, 23072, 23073, 23074, 23075, 23076, 23077, 23078, 23079, 23080, 23081, 23082, 23083, 23084, 23085, 23086, 23087, 23088, 23089, 23090, 23091, 23092, 23093, 23094, 23095, 23096, 23097, 23098, 23099, 23100, 23101, 23102, 23103, 23104, 23105, 23106, 23107, 23108, 23109, 23110, 23111, 23112, 23113, 23114, 23115, 23116, 23117, 23118, 23119, 23120, 23121, 23122, 23123, 23124, 23125, 23126, 23127, 23128, 23129, 23130, 23131, 23132, 23133, 23134, 23135, 23136, 23137, 23138, 23139, 23140, 23141, 23142, 23143, 23144, 23145, 23146, 23147, 23148, 23149, 23150, 23151, 23152, 23153, 23154, 23155, 23156, 23157, 23158, 23159, 23160, 23161, 23162, 23163, 23164, 23165, 23166, 23167, 23168, 23169, 23170, 23171, 23172, 23173, 23174, 23175, 23176, 23177, 23178, 23179, 23180, 23181, 23182, 23183, 23184, 23185, 23186, 23187, 23188, 23189, 23190, 23191, 23192, 23193, 23194, 23195, 23196, 23197, 23198, 23199, 23200, 23201, 23202, 23203, 23204, 23205, 23206, 23207, 23208, 23209, 23210, 23211, 23212, 23213, 23214, 23215, 23216, 23217, 23218, 23219, 23220, 23221, 23222, 23223, 23224, 23225, 23226, 23227, 23228, 23229, 23230, 23231, 23232, 23233, 23234, 23235, 23236, 23237, 23238, 23239, 23240, 23241, 23242, 23243, 23244, 23245, 23246, 23247, 23248, 23249, 23250, 23251, 23252, 23253, 23254, 23255, 23256, 23257, 23258, 23259, 23260, 23261, 23262, 23263, 23264, 23265, 23266, 23267, 23268, 23269, 23270, 23271, 23272, 23273, 23274, 23275, 23276, 23277, 23278, 23279, 23280, 23281, 23282, 23283, 23284, 23285, 23286, 23287, 23288, 23289, 23290, 23291, 23292, 23293, 23294, 23295, 23296, 23297, 23298, 23299, 23300, 23301, 23302, 23303, 23304, 23305, 23306, 23307, 23308, 23309, 23310, 23311, 23312, 23313, 23314, 23315, 23316, 23317, 23318, 23319, 23320, 23321, 23322, 23323, 23324, 23325, 23326, 23327, 23328, 23329, 23330, 23331, 23332, 23333, 23334, 23335, 23336, 23337, 23338, 23339, 23340, 23341, 23342, 23343, 23344, 23345, 23346, 23347, 23348, 23349, 23350, 23351, 23352, 23353, 23354, 23355, 23356, 23357, 23358, 23359, 23360, 23361, 23362, 23363, 23364, 23365, 23366, 23367, 23368, 23369, 23370, 23371, 23372, 23373, 23374, 23375, 23376, 23377, 23378, 23379, 23380, 23381, 23382, 23383, 23384, 23385, 23386, 23387, 23388, 23389, 23390, 23391, 23392, 23393, 23394, 23395, 23396, 23397, 23398, 23399, 23400, 23401, 23402, 23403, 23404, 23405, 23406, 23407, 23408, 23409, 23410, 23411, 23412, 23413, 23414, 23415, 23416, 23417, 23418, 23419, 23420, 23421, 23422, 23423, 23424, 23425, 23426, 23427, 23428, 23429, 23430, 23431, 23432, 23433, 23434, 23435, 23436, 23437, 23438, 23439, 23440, 23441, 23442, 23443, 23444, 23445, 23446, 23447, 23448, 23449, 23450, 23451, 23452, 23453, 23454, 23455, 23456, 23457, 23458, 23459, 23460, 23461, 23462, 23463, 23464, 23465, 23466, 23467, 23468, 23469, 23470, 23471, 23472, 23473, 23474, 23475, 23476, 23477, 23478, 23479, 23480, 23481, 23482, 23483, 23484, 23485, 23486, 23487, 23488, 23489, 23490, 23491, 23492, 23493, 23494, 23495, 23496, 23497, 23498, 23499, 23500, 23501, 23502, 23503, 23504, 23505, 23506, 23507, 23508, 23509, 23510, 23511, 23512, 23513, 23514, 23515, 23516, 23517, 23518, 23519, 23520, 23521, 23522, 23523, 23524, 23525, 23526, 23527, 23528, 23529, 23530, 23531, 23532, 23533, 23534, 23535, 23536, 23537, 23538, 23539, 23540, 23541, 23542, 23543, 23544, 23545, 23546, 23547, 23548, 23549, 23550, 23551, 23552, 23553, 23554, 23555, 23556, 23557, 23558, 23559, 23560, 23561, 23562, 23563, 23564, 23565, 23566, 23567, 23568, 23569, 23570, 23571, 23572, 23573, 23574, 23575, 23576, 23577, 23578, 23579, 23580, 23581, 23582, 23583, 23584, 23585, 23586, 23587, 23588, 23589, 23590, 23591, 23592, 23593, 23594, 23595, 23596, 23597, 23598, 23599, 23600, 23601, 23602, 23603, 23604, 23605, 23606, 23607, 23608, 23609, 23610, 23611, 23612, 23613, 23614, 23615, 23616, 23617, 23618, 23619, 23620, 23621, 23622, 23623, 23624, 23625, 23626, 23627, 23628, 23629, 23630, 23631, 23632, 23633, 23634, 23635, 23636, 23637, 23638, 23639, 23640, 23641, 23642, 23643, 23644, 23645, 23646, 23647, 23648, 23649, 23650, 23651, 23652, 23653, 23654, 23655, 23656, 23657, 23658, 23659, 23660, 23661, 23662, 23663, 23664, 23665, 23666, 23667, 23668, 23669, 23670, 23671, 23672, 23673, 23674, 23675, 23676, 23677, 23678, 23679, 23680, 23681, 23682, 23683, 23684, 23685, 23686, 23687, 23688, 23689, 23690, 23691, 23692, 23693, 23694, 23695, 23696, 23697, 23698, 23699, 23700, 23701, 23702, 23703, 23704, 23705, 23706, 23707, 23708, 23709, 23710, 23711, 23712, 23713, 23714, 23715, 23716, 23717, 23718, 23719, 23720, 23721, 23722, 23723, 23724, 23725, 23726, 23727, 23728, 23729, 23730, 23731, 23732, 23733, 23734, 23735, 23736, 23737, 23738, 23739, 23740, 23741, 23742, 23743, 23744, 23745, 23746, 23747, 23748, 23749, 23750, 23751, 23752, 23753, 23754, 23755, 23756, 23757, 23758, 23759, 23760, 23761, 23762, 23763, 23764, 23765, 23766, 23767, 23768, 23769, 23770, 23771, 23772, 23773, 23774, 23775, 23776, 23777, 23778, 23779, 23780, 23781, 23782, 23783, 23784, 23785, 23786, 23787, 23788, 23789, 23790, 23791, 23792, 23793, 23794, 23795, 23796, 23797, 23798, 23799, 23800, 23801, 23802, 23803, 23804, 23805, 23806, 23807, 23808, 23809, 23810, 23811, 23812, 23813, 23814, 23815, 23816, 23817, 23818, 23819, 23820, 23821, 23822, 23823, 23824, 23825, 23826, 23827, 23828, 23829, 23830, 23831, 23832, 23833, 23834, 23835, 23836, 23837, 23838, 23839, 23840, 23841, 23842, 23843, 23844, 23845, 23846, 23847, 23848, 23849, 23850, 23851, 23852, 23853, 23854, 23855, 23856, 23857, 23858, 23859, 23860, 23861, 23862, 23863, 23864, 23865, 23866, 23867, 23868, 23869, 23870, 23871, 23872, 23873, 23874, 23875, 23876, 23877, 23878, 23879, 23880, 23881, 23882, 23883, 23884, 23885, 23886, 23887, 23888, 23889, 23890, 23891, 23892, 23893, 23894, 23895, 23896, 23897, 23898, 23899, 23900, 23901, 23902, 23903, 23904, 23905, 23906, 23907, 23908, 23909, 23910, 23911, 23912, 23913, 23914, 23915, 23916, 23917, 23918, 23919, 23920, 23921, 23922, 23923, 23924, 23925, 23926, 23927, 23928, 23929, 23930, 23931, 23932, 23933, 23934, 23935, 23936, 23937, 23938, 23939, 23940, 23941, 23942, 23943, 23944, 23945, 23946, 23947, 23948, 23949, 23950, 23951, 23952, 23953, 23954, 23955, 23956, 23957, 23958, 23959, 23960, 23961, 23962, 23963, 23964, 23965, 23966, 23967, 23968, 23969, 23970, 23971, 23972, 23973, 23974, 23975, 23976, 23977, 23978, 23979, 23980, 23981, 23982, 23983, 23984, 23985, 23986, 23987, 23988, 23989, 23990, 23991, 23992, 23993, 23994, 23995, 23996, 23997, 23998, 23999, 24000, 24001, 24002, 24003, 24004, 24005, 24006, 24007, 24008, 24009, 24010, 24011, 24012, 24013, 24014, 24015, 24016, 24017, 24018, 24019, 24020, 24021, 24022, 24023, 24024, 24025, 24026, 24027, 24028, 24029, 24030, 24031, 24032, 24033, 24034, 24035, 24036, 24037, 24038, 24039, 24040, 24041, 24042, 24043, 24044, 24045, 24046, 24047, 24048, 24049, 24050, 24051, 24052, 24053, 24054, 24055, 24056, 24057, 24058, 24059, 24060, 24061, 24062, 24063, 24064, 24065, 24066, 24067, 24068, 24069, 24070, 24071, 24072, 24073, 24074, 24075, 24076, 24077, 24078, 24079, 24080, 24081, 24082, 24083, 24084, 24085, 24086, 24087, 24088, 24089, 24090, 24091, 24092, 24093, 24094, 24095, 24096, 24097, 24098, 24099, 24100, 24101, 24102, 24103, 24104, 24105, 24106, 24107, 24108, 24109, 24110, 24111, 24112, 24113, 24114, 24115, 24116, 24117, 24118, 24119, 24120, 24121, 24122, 24123, 24124, 24125, 24126, 24127, 24128, 24129, 24130, 24131, 24132, 24133, 24134, 24135, 24136, 24137, 24138, 24139, 24140, 24141, 24142, 24143, 24144, 24145, 24146, 24147, 24148, 24149, 24150, 24151, 24152, 24153, 24154, 24155, 24156, 24157, 24158, 24159, 24160, 24161, 24162, 24163, 24164, 24165, 24166, 24167, 24168, 24169, 24170, 24171, 24172, 24173, 24174, 24175, 24176, 24177, 24178, 24179, 24180, 24181, 24182, 24183, 24184, 24185, 24186, 24187, 24188, 24189, 24190, 24191, 24192, 24193, 24194, 24195, 24196, 24197, 24198, 24199, 24200, 24201, 24202, 24203, 24204, 24205, 24206, 24207, 24208, 24209, 24210, 24211, 24212, 24213, 24214, 24215, 24216, 24217, 24218, 24219, 24220, 24221, 24222, 24223, 24224, 24225, 24226, 24227, 24228, 24229, 24230, 24231, 24232, 24233, 24234, 24235, 24236, 24237, 24238, 24239, 24240, 24241, 24242, 24243, 24244, 24245, 24246, 24247, 24248, 24249, 24250, 24251, 24252, 24253, 24254, 24255, 24256, 24257, 24258, 24259, 24260, 24261, 24262, 24263, 24264, 24265, 24266, 24267, 24268, 24269, 24270, 24271, 24272, 24273, 24274, 24275, 24276, 24277, 24278, 24279, 24280, 24281, 24282, 24283, 24284, 24285, 24286, 24287, 24288, 24289, 24290, 24291, 24292, 24293, 24294, 24295, 24296, 24297, 24298, 24299, 24300, 24301, 24302, 24303, 24304, 24305, 24306, 24307, 24308, 24309, 24310, 24311, 24312, 24313, 24314, 24315, 24316, 24317, 24318, 24319, 24320, 24321, 24322, 24323, 24324, 24325, 24326, 24327, 24328, 24329, 24330, 24331, 24332, 24333, 24334, 24335, 24336, 24337, 24338, 24339, 24340, 24341, 24342, 24343, 24344, 24345, 24346, 24347, 24348, 24349, 24350, 24351, 24352, 24353, 24354, 24355, 24356, 24357, 24358, 24359, 24360, 24361, 24362, 24363, 24364, 24365, 24366, 24367, 24368, 24369, 24370, 24371, 24372, 24373, 24374, 24375, 24376, 24377, 24378, 24379, 24380, 24381, 24382, 24383, 24384, 24385, 24386, 24387, 24388, 24389, 24390, 24391, 24392, 24393, 24394, 24395, 24396, 24397, 24398, 24399, 24400, 24401, 24402, 24403, 24404, 24405, 24406, 24407, 24408, 24409, 24410, 24411, 24412, 24413, 24414, 24415, 24416, 24417, 24418, 24419, 24420, 24421, 24422, 24423, 24424, 24425, 24426, 24427, 24428, 24429, 24430, 24431, 24432, 24433, 24434, 24435, 24436, 24437, 24438, 24439, 24440, 24441, 24442, 24443, 24444, 24445, 24446, 24447, 24448, 24449, 24450, 24451, 24452, 24453, 24454, 24455, 24456, 24457, 24458, 24459, 24460, 24461, 24462, 24463, 24464, 24465, 24466, 24467, 24468, 24469, 24470, 24471, 24472, 24473, 24474, 24475, 24476, 24477, 24478, 24479, 24480, 24481, 24482, 24483, 24484, 24485, 24486, 24487, 24488, 24489, 24490, 24491, 24492, 24493, 24494, 24495, 24496, 24497, 24498, 24499, 24500, 24501, 24502, 24503, 24504, 24505, 24506, 24507, 24508, 24509, 24510, 24511, 24512, 24513, 24514, 24515, 24516, 24517, 24518, 24519, 24520, 24521, 24522, 24523, 24524, 24525, 24526, 24527, 24528, 24529, 24530, 24531, 24532, 24533, 24534, 24535, 24536, 24537, 24538, 24539, 24540, 24541, 24542, 24543, 24544, 24545, 24546, 24547, 24548, 24549, 24550, 24551, 24552, 24553, 24554, 24555, 24556, 24557, 24558, 24559, 24560, 24561, 24562, 24563, 24564, 24565, 24566, 24567, 24568, 24569, 24570, 24571, 24572, 24573, 24574, 24575, 24576, 24577, 24578, 24579, 24580, 24581, 24582, 24583, 24584, 24585, 24586, 24587, 24588, 24589, 24590, 24591, 24592, 24593, 24594, 24595, 24596, 24597, 24598, 24599, 24600, 24601, 24602, 24603, 24604, 24605, 24606, 24607, 24608, 24609, 24610, 24611, 24612, 24613, 24614, 24615, 24616, 24617, 24618, 24619, 24620, 24621, 24622, 24623, 24624, 24625, 24626, 24627, 24628, 24629, 24630, 24631, 24632, 24633, 24634, 24635, 24636, 24637, 24638, 24639, 24640, 24641, 24642, 24643, 24644, 24645, 24646, 24647, 24648, 24649, 24650, 24651, 24652, 24653, 24654, 24655, 24656, 24657, 24658, 24659, 24660, 24661, 24662, 24663, 24664, 24665, 24666, 24667, 24668, 24669, 24670, 24671, 24672, 24673, 24674, 24675, 24676, 24677, 24678, 24679, 24680, 24681, 24682, 24683, 24684, 24685, 24686, 24687, 24688, 24689, 24690, 24691, 24692, 24693, 24694, 24695, 24696, 24697, 24698, 24699, 24700, 24701, 24702, 24703, 24704, 24705, 24706, 24707, 24708, 24709, 24710, 24711, 24712, 24713, 24714, 24715, 24716, 24717, 24718, 24719, 24720, 24721, 24722, 24723, 24724, 24725, 24726, 24727, 24728, 24729, 24730, 24731, 24732, 24733, 24734, 24735, 24736, 24737, 24738, 24739, 24740, 24741, 24742, 24743, 24744, 24745, 24746, 24747, 24748, 24749, 24750, 24751, 24752, 24753, 24754, 24755, 24756, 24757, 24758, 24759, 24760, 24761, 24762, 24763, 24764, 24765, 24766, 24767, 24768, 24769, 24770, 24771, 24772, 24773, 24774, 24775, 24776, 24777, 24778, 24779, 24780, 24781, 24782, 24783, 24784, 24785, 24786, 24787, 24788, 24789, 24790, 24791, 24792, 24793, 24794, 24795, 24796, 24797, 24798, 24799, 24800, 24801, 24802, 24803, 24804, 24805, 24806, 24807, 24808, 24809, 24810, 24811, 24812, 24813, 24814, 24815, 24816, 24817, 24818, 24819, 24820, 24821, 24822, 24823, 24824, 24825, 24826, 24827, 24828, 24829, 24830, 24831, 24832, 24833, 24834, 24835, 24836, 24837, 24838, 24839, 24840, 24841, 24842, 24843, 24844, 24845, 24846, 24847, 24848, 24849, 24850, 24851, 24852, 24853, 24854, 24855, 24856, 24857, 24858, 24859, 24860, 24861, 24862, 24863, 24864, 24865, 24866, 24867, 24868, 24869, 24870, 24871, 24872, 24873, 24874, 24875, 24876, 24877, 24878, 24879, 24880, 24881, 24882, 24883, 24884, 24885, 24886, 24887, 24888, 24889, 24890, 24891, 24892, 24893, 24894, 24895, 24896, 24897, 24898, 24899, 24900, 24901, 24902, 24903, 24904, 24905, 24906, 24907, 24908, 24909, 24910, 24911, 24912, 24913, 24914, 24915, 24916, 24917, 24918, 24919, 24920, 24921, 24922, 24923, 24924, 24925, 24926, 24927, 24928, 24929, 24930, 24931, 24932, 24933, 24934, 24935, 24936, 24937, 24938, 24939, 24940, 24941, 24942, 24943, 24944, 24945, 24946, 24947, 24948, 24949, 24950, 24951, 24952, 24953, 24954, 24955, 24956, 24957, 24958, 24959, 24960, 24961, 24962, 24963, 24964, 24965, 24966, 24967, 24968, 24969, 24970, 24971, 24972, 24973, 24974, 24975, 24976, 24977, 24978, 24979, 24980, 24981, 24982, 24983, 24984, 24985, 24986, 24987, 24988, 24989, 24990, 24991, 24992, 24993, 24994, 24995, 24996, 24997, 24998, 24999, 25000, 25001, 25002, 25003, 25004, 25005, 25006, 25007, 25008, 25009, 25010, 25011, 25012, 25013, 25014, 25015, 25016, 25017, 25018, 25019, 25020, 25021, 25022, 25023, 25024, 25025, 25026, 25027, 25028, 25029, 25030, 25031, 25032, 25033, 25034, 25035, 25036, 25037, 25038, 25039, 25040, 25041, 25042, 25043, 25044, 25045, 25046, 25047, 25048, 25049, 25050, 25051, 25052, 25053, 25054, 25055, 25056, 25057, 25058, 25059, 25060, 25061, 25062, 25063, 25064, 25065, 25066, 25067, 25068, 25069, 25070, 25071, 25072, 25073, 25074, 25075, 25076, 25077, 25078, 25079, 25080, 25081, 25082, 25083, 25084, 25085, 25086, 25087, 25088, 25089, 25090, 25091, 25092, 25093, 25094, 25095, 25096, 25097, 25098, 25099, 25100, 25101, 25102, 25103, 25104, 25105, 25106, 25107, 25108, 25109, 25110, 25111, 25112, 25113, 25114, 25115, 25116, 25117, 25118, 25119, 25120, 25121, 25122, 25123, 25124, 25125, 25126, 25127, 25128, 25129, 25130, 25131, 25132, 25133, 25134, 25135, 25136, 25137, 25138, 25139, 25140, 25141, 25142, 25143, 25144, 25145, 25146, 25147, 25148, 25149, 25150, 25151, 25152, 25153, 25154, 25155, 25156, 25157, 25158, 25159, 25160, 25161, 25162, 25163, 25164, 25165, 25166, 25167, 25168, 25169, 25170, 25171, 25172, 25173, 25174, 25175, 25176, 25177, 25178, 25179, 25180, 25181, 25182, 25183, 25184, 25185, 25186, 25187, 25188, 25189, 25190, 25191, 25192, 25193, 25194, 25195, 25196, 25197, 25198, 25199, 25200, 25201, 25202, 25203, 25204, 25205, 25206, 25207, 25208, 25209, 25210, 25211, 25212, 25213, 25214, 25215, 25216, 25217, 25218, 25219, 25220, 25221, 25222, 25223, 25224, 25225, 25226, 25227, 25228, 25229, 25230, 25231, 25232, 25233, 25234, 25235, 25236, 25237, 25238, 25239, 25240, 25241, 25242, 25243, 25244, 25245, 25246, 25247, 25248, 25249, 25250, 25251, 25252, 25253, 25254, 25255, 25256, 25257, 25258, 25259, 25260, 25261, 25262, 25263, 25264, 25265, 25266, 25267, 25268, 25269, 25270, 25271, 25272, 25273, 25274, 25275, 25276, 25277, 25278, 25279, 25280, 25281, 25282, 25283, 25284, 25285, 25286, 25287, 25288, 25289, 25290, 25291, 25292, 25293, 25294, 25295, 25296, 25297, 25298, 25299, 25300, 25301, 25302, 25303, 25304, 25305, 25306, 25307, 25308, 25309, 25310, 25311, 25312, 25313, 25314, 25315, 25316, 25317, 25318, 25319, 25320, 25321, 25322, 25323, 25324, 25325, 25326, 25327, 25328, 25329, 25330, 25331, 25332, 25333, 25334, 25335, 25336, 25337, 25338, 25339, 25340, 25341, 25342, 25343, 25344, 25345, 25346, 25347, 25348, 25349, 25350, 25351, 25352, 25353, 25354, 25355, 25356, 25357, 25358, 25359, 25360, 25361, 25362, 25363, 25364, 25365, 25366, 25367, 25368, 25369, 25370, 25371, 25372, 25373, 25374, 25375, 25376, 25377, 25378, 25379, 25380, 25381, 25382, 25383, 25384, 25385, 25386, 25387, 25388, 25389, 25390, 25391, 25392, 25393, 25394, 25395, 25396, 25397, 25398, 25399, 25400, 25401, 25402, 25403, 25404, 25405, 25406, 25407, 25408, 25409, 25410, 25411, 25412, 25413, 25414, 25415, 25416, 25417, 25418, 25419, 25420, 25421, 25422, 25423, 25424, 25425, 25426, 25427, 25428, 25429, 25430, 25431, 25432, 25433, 25434, 25435, 25436, 25437, 25438, 25439, 25440, 25441, 25442, 25443, 25444, 25445, 25446, 25447, 25448, 25449, 25450, 25451, 25452, 25453, 25454, 25455, 25456, 25457, 25458, 25459, 25460, 25461, 25462, 25463, 25464, 25465, 25466, 25467, 25468, 25469, 25470, 25471, 25472, 25473, 25474, 25475, 25476, 25477, 25478, 25479, 25480, 25481, 25482, 25483, 25484, 25485, 25486, 25487, 25488, 25489, 25490, 25491, 25492, 25493, 25494, 25495, 25496, 25497, 25498, 25499, 25500, 25501, 25502, 25503, 25504, 25505, 25506, 25507, 25508, 25509, 25510, 25511, 25512, 25513, 25514, 25515, 25516, 25517, 25518, 25519, 25520, 25521, 25522, 25523, 25524, 25525, 25526, 25527, 25528, 25529, 25530, 25531, 25532, 25533, 25534, 25535, 25536, 25537, 25538, 25539, 25540, 25541, 25542, 25543, 25544, 25545, 25546, 25547, 25548, 25549, 25550, 25551, 25552, 25553, 25554, 25555, 25556, 25557, 25558, 25559, 25560, 25561, 25562, 25563, 25564, 25565, 25566, 25567, 25568, 25569, 25570, 25571, 25572, 25573, 25574, 25575, 25576, 25577, 25578, 25579, 25580, 25581, 25582, 25583, 25584, 25585, 25586, 25587, 25588, 25589, 25590, 25591, 25592, 25593, 25594, 25595, 25596, 25597, 25598, 25599, 25600, 25601, 25602, 25603, 25604, 25605, 25606, 25607, 25608, 25609, 25610, 25611, 25612, 25613, 25614, 25615, 25616, 25617, 25618, 25619, 25620, 25621, 25622, 25623, 25624, 25625, 25626, 25627, 25628, 25629, 25630, 25631, 25632, 25633, 25634, 25635, 25636, 25637, 25638, 25639, 25640, 25641, 25642, 25643, 25644, 25645, 25646, 25647, 25648, 25649, 25650, 25651, 25652, 25653, 25654, 25655, 25656, 25657, 25658, 25659, 25660, 25661, 25662, 25663, 25664, 25665, 25666, 25667, 25668, 25669, 25670, 25671, 25672, 25673, 25674, 25675, 25676, 25677, 25678, 25679, 25680, 25681, 25682, 25683, 25684, 25685, 25686, 25687, 25688, 25689, 25690, 25691, 25692, 25693, 25694, 25695, 25696, 25697, 25698, 25699, 25700, 25701, 25702, 25703, 25704, 25705, 25706, 25707, 25708, 25709, 25710, 25711, 25712, 25713, 25714, 25715, 25716, 25717, 25718, 25719, 25720, 25721, 25722, 25723, 25724, 25725, 25726, 25727, 25728, 25729, 25730, 25731, 25732, 25733, 25734, 25735, 25736, 25737, 25738, 25739, 25740, 25741, 25742, 25743, 25744, 25745, 25746, 25747, 25748, 25749, 25750, 25751, 25752, 25753, 25754, 25755, 25756, 25757, 25758, 25759, 25760, 25761, 25762, 25763, 25764, 25765, 25766, 25767, 25768, 25769, 25770, 25771, 25772, 25773, 25774, 25775, 25776, 25777, 25778, 25779, 25780, 25781, 25782, 25783, 25784, 25785, 25786, 25787, 25788, 25789, 25790, 25791, 25792, 25793, 25794, 25795, 25796, 25797, 25798, 25799, 25800, 25801, 25802, 25803, 25804, 25805, 25806, 25807, 25808, 25809, 25810, 25811, 25812, 25813, 25814, 25815, 25816, 25817, 25818, 25819, 25820, 25821, 25822, 25823, 25824, 25825, 25826, 25827, 25828, 25829, 25830, 25831, 25832, 25833, 25834, 25835, 25836, 25837, 25838, 25839, 25840, 25841, 25842, 25843, 25844, 25845, 25846, 25847, 25848, 25849, 25850, 25851, 25852, 25853, 25854, 25855, 25856, 25857, 25858, 25859, 25860, 25861, 25862, 25863, 25864, 25865, 25866, 25867, 25868, 25869, 25870, 25871, 25872, 25873, 25874, 25875, 25876, 25877, 25878, 25879, 25880, 25881, 25882, 25883, 25884, 25885, 25886, 25887, 25888, 25889, 25890, 25891, 25892, 25893, 25894, 25895, 25896, 25897, 25898, 25899, 25900, 25901, 25902, 25903, 25904, 25905, 25906, 25907, 25908, 25909, 25910, 25911, 25912, 25913, 25914, 25915, 25916, 25917, 25918, 25919, 25920, 25921, 25922, 25923, 25924, 25925, 25926, 25927, 25928, 25929, 25930, 25931, 25932, 25933, 25934, 25935, 25936, 25937, 25938, 25939, 25940, 25941, 25942, 25943, 25944, 25945, 25946, 25947, 25948, 25949, 25950, 25951, 25952, 25953, 25954, 25955, 25956, 25957, 25958, 25959, 25960, 25961, 25962, 25963, 25964, 25965, 25966, 25967, 25968, 25969, 25970, 25971, 25972, 25973, 25974, 25975, 25976, 25977, 25978, 25979, 25980, 25981, 25982, 25983, 25984, 25985, 25986, 25987, 25988, 25989, 25990, 25991, 25992, 25993, 25994, 25995, 25996, 25997, 25998, 25999, 26000, 26001, 26002, 26003, 26004, 26005, 26006, 26007, 26008, 26009, 26010, 26011, 26012, 26013, 26014, 26015, 26016, 26017, 26018, 26019, 26020, 26021, 26022, 26023, 26024, 26025, 26026, 26027, 26028, 26029, 26030, 26031, 26032, 26033, 26034, 26035, 26036, 26037, 26038, 26039, 26040, 26041, 26042, 26043, 26044, 26045, 26046, 26047, 26048, 26049, 26050, 26051, 26052, 26053, 26054, 26055, 26056, 26057, 26058, 26059, 26060, 26061, 26062, 26063, 26064, 26065, 26066, 26067, 26068, 26069, 26070, 26071, 26072, 26073, 26074, 26075, 26076, 26077, 26078, 26079, 26080, 26081, 26082, 26083, 26084, 26085, 26086, 26087, 26088, 26089, 26090, 26091, 26092, 26093, 26094, 26095, 26096, 26097, 26098, 26099, 26100, 26101, 26102, 26103, 26104, 26105, 26106, 26107, 26108, 26109, 26110, 26111, 26112, 26113, 26114, 26115, 26116, 26117, 26118, 26119, 26120, 26121, 26122, 26123, 26124, 26125, 26126, 26127, 26128, 26129, 26130, 26131, 26132, 26133, 26134, 26135, 26136, 26137, 26138, 26139, 26140, 26141, 26142, 26143, 26144, 26145, 26146, 26147, 26148, 26149, 26150, 26151, 26152, 26153, 26154, 26155, 26156, 26157, 26158, 26159, 26160, 26161, 26162, 26163, 26164, 26165, 26166, 26167, 26168, 26169, 26170, 26171, 26172, 26173, 26174, 26175, 26176, 26177, 26178, 26179, 26180, 26181, 26182, 26183, 26184, 26185, 26186, 26187, 26188, 26189, 26190, 26191, 26192, 26193, 26194, 26195, 26196, 26197, 26198, 26199, 26200, 26201, 26202, 26203, 26204, 26205, 26206, 26207, 26208, 26209, 26210, 26211, 26212, 26213, 26214, 26215, 26216, 26217, 26218, 26219, 26220, 26221, 26222, 26223, 26224, 26225, 26226, 26227, 26228, 26229, 26230, 26231, 26232, 26233, 26234, 26235, 26236, 26237, 26238, 26239, 26240, 26241, 26242, 26243, 26244, 26245, 26246, 26247, 26248, 26249, 26250, 26251, 26252, 26253, 26254, 26255, 26256, 26257, 26258, 26259, 26260, 26261, 26262, 26263, 26264, 26265, 26266, 26267, 26268, 26269, 26270, 26271, 26272, 26273, 26274, 26275, 26276, 26277, 26278, 26279, 26280, 26281, 26282, 26283, 26284, 26285, 26286, 26287, 26288, 26289, 26290, 26291, 26292, 26293, 26294, 26295, 26296, 26297, 26298, 26299, 26300, 26301, 26302, 26303, 26304, 26305, 26306, 26307, 26308, 26309, 26310, 26311, 26312, 26313, 26314, 26315, 26316, 26317, 26318, 26319, 26320, 26321, 26322, 26323, 26324, 26325, 26326, 26327, 26328, 26329, 26330, 26331, 26332, 26333, 26334, 26335, 26336, 26337, 26338, 26339, 26340, 26341, 26342, 26343, 26344, 26345, 26346, 26347, 26348, 26349, 26350, 26351, 26352, 26353, 26354, 26355, 26356, 26357, 26358, 26359, 26360, 26361, 26362, 26363, 26364, 26365, 26366, 26367, 26368, 26369, 26370, 26371, 26372, 26373, 26374, 26375, 26376, 26377, 26378, 26379, 26380, 26381, 26382, 26383, 26384, 26385, 26386, 26387, 26388, 26389, 26390, 26391, 26392, 26393, 26394, 26395, 26396, 26397, 26398, 26399, 26400, 26401, 26402, 26403, 26404, 26405, 26406, 26407, 26408, 26409, 26410, 26411, 26412, 26413, 26414, 26415, 26416, 26417, 26418, 26419, 26420, 26421, 26422, 26423, 26424, 26425, 26426, 26427, 26428, 26429, 26430, 26431, 26432, 26433, 26434, 26435, 26436, 26437, 26438, 26439, 26440, 26441, 26442, 26443, 26444, 26445, 26446, 26447, 26448, 26449, 26450, 26451, 26452, 26453, 26454, 26455, 26456, 26457, 26458, 26459, 26460, 26461, 26462, 26463, 26464, 26465, 26466, 26467, 26468, 26469, 26470, 26471, 26472, 26473, 26474, 26475, 26476, 26477, 26478, 26479, 26480, 26481, 26482, 26483, 26484, 26485, 26486, 26487, 26488, 26489, 26490, 26491, 26492, 26493, 26494, 26495, 26496, 26497, 26498, 26499, 26500, 26501, 26502, 26503, 26504, 26505, 26506, 26507, 26508, 26509, 26510, 26511, 26512, 26513, 26514, 26515, 26516, 26517, 26518, 26519, 26520, 26521, 26522, 26523, 26524, 26525, 26526, 26527, 26528, 26529, 26530, 26531, 26532, 26533, 26534, 26535, 26536, 26537, 26538, 26539, 26540, 26541, 26542, 26543, 26544, 26545, 26546, 26547, 26548, 26549, 26550, 26551, 26552, 26553, 26554, 26555, 26556, 26557, 26558, 26559, 26560, 26561, 26562, 26563, 26564, 26565, 26566, 26567, 26568, 26569, 26570, 26571, 26572, 26573, 26574, 26575, 26576, 26577, 26578, 26579, 26580, 26581, 26582, 26583, 26584, 26585, 26586, 26587, 26588, 26589, 26590, 26591, 26592, 26593, 26594, 26595, 26596, 26597, 26598, 26599, 26600, 26601, 26602, 26603, 26604, 26605, 26606, 26607, 26608, 26609, 26610, 26611, 26612, 26613, 26614, 26615, 26616, 26617, 26618, 26619, 26620, 26621, 26622, 26623, 26624, 26625, 26626, 26627, 26628, 26629, 26630, 26631, 26632, 26633, 26634, 26635, 26636, 26637, 26638, 26639, 26640, 26641, 26642, 26643, 26644, 26645, 26646, 26647, 26648, 26649, 26650, 26651, 26652, 26653, 26654, 26655, 26656, 26657, 26658, 26659, 26660, 26661, 26662, 26663, 26664, 26665, 26666, 26667, 26668, 26669, 26670, 26671, 26672, 26673, 26674, 26675, 26676, 26677, 26678, 26679, 26680, 26681, 26682, 26683, 26684, 26685, 26686, 26687, 26688, 26689, 26690, 26691, 26692, 26693, 26694, 26695, 26696, 26697, 26698, 26699, 26700, 26701, 26702, 26703, 26704, 26705, 26706, 26707, 26708, 26709, 26710, 26711, 26712, 26713, 26714, 26715, 26716, 26717, 26718, 26719, 26720, 26721, 26722, 26723, 26724, 26725, 26726, 26727, 26728, 26729, 26730, 26731, 26732, 26733, 26734, 26735, 26736, 26737, 26738, 26739, 26740, 26741, 26742, 26743, 26744, 26745, 26746, 26747, 26748, 26749, 26750, 26751, 26752, 26753, 26754, 26755, 26756, 26757, 26758, 26759, 26760, 26761, 26762, 26763, 26764, 26765, 26766, 26767, 26768, 26769, 26770, 26771, 26772, 26773, 26774, 26775, 26776, 26777, 26778, 26779, 26780, 26781, 26782, 26783, 26784, 26785, 26786, 26787, 26788, 26789, 26790, 26791, 26792, 26793, 26794, 26795, 26796, 26797, 26798, 26799, 26800, 26801, 26802, 26803, 26804, 26805, 26806, 26807, 26808, 26809, 26810, 26811, 26812, 26813, 26814, 26815, 26816, 26817, 26818, 26819, 26820, 26821, 26822, 26823, 26824, 26825, 26826, 26827, 26828, 26829, 26830, 26831, 26832, 26833, 26834, 26835, 26836, 26837, 26838, 26839, 26840, 26841, 26842, 26843, 26844, 26845, 26846, 26847, 26848, 26849, 26850, 26851, 26852, 26853, 26854, 26855, 26856, 26857, 26858, 26859, 26860, 26861, 26862, 26863, 26864, 26865, 26866, 26867, 26868, 26869, 26870, 26871, 26872, 26873, 26874, 26875, 26876, 26877, 26878, 26879, 26880, 26881, 26882, 26883, 26884, 26885, 26886, 26887, 26888, 26889, 26890, 26891, 26892, 26893, 26894, 26895, 26896, 26897, 26898, 26899, 26900, 26901, 26902, 26903, 26904, 26905, 26906, 26907, 26908, 26909, 26910, 26911, 26912, 26913, 26914, 26915, 26916, 26917, 26918, 26919, 26920, 26921, 26922, 26923, 26924, 26925, 26926, 26927, 26928, 26929, 26930, 26931, 26932, 26933, 26934, 26935, 26936, 26937, 26938, 26939, 26940, 26941, 26942, 26943, 26944, 26945, 26946, 26947, 26948, 26949, 26950, 26951, 26952, 26953, 26954, 26955, 26956, 26957, 26958, 26959, 26960, 26961, 26962, 26963, 26964, 26965, 26966, 26967, 26968, 26969, 26970, 26971, 26972, 26973, 26974, 26975, 26976, 26977, 26978, 26979, 26980, 26981, 26982, 26983, 26984, 26985, 26986, 26987, 26988, 26989, 26990, 26991, 26992, 26993, 26994, 26995, 26996, 26997, 26998, 26999, 27000, 27001, 27002, 27003, 27004, 27005, 27006, 27007, 27008, 27009, 27010, 27011, 27012, 27013, 27014, 27015, 27016, 27017, 27018, 27019, 27020, 27021, 27022, 27023, 27024, 27025, 27026, 27027, 27028, 27029, 27030, 27031, 27032, 27033, 27034, 27035, 27036, 27037, 27038, 27039, 27040, 27041, 27042, 27043, 27044, 27045, 27046, 27047, 27048, 27049, 27050, 27051, 27052, 27053, 27054, 27055, 27056, 27057, 27058, 27059, 27060, 27061, 27062, 27063, 27064, 27065, 27066, 27067, 27068, 27069, 27070, 27071, 27072, 27073, 27074, 27075, 27076, 27077, 27078, 27079, 27080, 27081, 27082, 27083, 27084, 27085, 27086, 27087, 27088, 27089, 27090, 27091, 27092, 27093, 27094, 27095, 27096, 27097, 27098, 27099, 27100, 27101, 27102, 27103, 27104, 27105, 27106, 27107, 27108, 27109, 27110, 27111, 27112, 27113, 27114, 27115, 27116, 27117, 27118, 27119, 27120, 27121, 27122, 27123, 27124, 27125, 27126, 27127, 27128, 27129, 27130, 27131, 27132, 27133, 27134, 27135, 27136, 27137, 27138, 27139, 27140, 27141, 27142, 27143, 27144, 27145, 27146, 27147, 27148, 27149, 27150, 27151, 27152, 27153, 27154, 27155, 27156, 27157, 27158, 27159, 27160, 27161, 27162, 27163, 27164, 27165, 27166, 27167, 27168, 27169, 27170, 27171, 27172, 27173, 27174, 27175, 27176, 27177, 27178, 27179, 27180, 27181, 27182, 27183, 27184, 27185, 27186, 27187, 27188, 27189, 27190, 27191, 27192, 27193, 27194, 27195, 27196, 27197, 27198, 27199, 27200, 27201, 27202, 27203, 27204, 27205, 27206, 27207, 27208, 27209, 27210, 27211, 27212, 27213, 27214, 27215, 27216, 27217, 27218, 27219, 27220, 27221, 27222, 27223, 27224, 27225, 27226, 27227, 27228, 27229, 27230, 27231, 27232, 27233, 27234, 27235, 27236, 27237, 27238, 27239, 27240, 27241, 27242, 27243, 27244, 27245, 27246, 27247, 27248, 27249, 27250, 27251, 27252, 27253, 27254, 27255, 27256, 27257, 27258, 27259, 27260, 27261, 27262, 27263, 27264, 27265, 27266, 27267, 27268, 27269, 27270, 27271, 27272, 27273, 27274, 27275, 27276, 27277, 27278, 27279, 27280, 27281, 27282, 27283, 27284, 27285, 27286, 27287, 27288, 27289, 27290, 27291, 27292, 27293, 27294, 27295, 27296, 27297, 27298, 27299, 27300, 27301, 27302, 27303, 27304, 27305, 27306, 27307, 27308, 27309, 27310, 27311, 27312, 27313, 27314, 27315, 27316, 27317, 27318, 27319, 27320, 27321, 27322, 27323, 27324, 27325, 27326, 27327, 27328, 27329, 27330, 27331, 27332, 27333, 27334, 27335, 27336, 27337, 27338, 27339, 27340, 27341, 27342, 27343, 27344, 27345, 27346, 27347, 27348, 27349, 27350, 27351, 27352, 27353, 27354, 27355, 27356, 27357, 27358, 27359, 27360, 27361, 27362, 27363, 27364, 27365, 27366, 27367, 27368, 27369, 27370, 27371, 27372, 27373, 27374, 27375, 27376, 27377, 27378, 27379, 27380, 27381, 27382, 27383, 27384, 27385, 27386, 27387, 27388, 27389, 27390, 27391, 27392, 27393, 27394, 27395, 27396, 27397, 27398, 27399, 27400, 27401, 27402, 27403, 27404, 27405, 27406, 27407, 27408, 27409, 27410, 27411, 27412, 27413, 27414, 27415, 27416, 27417, 27418, 27419, 27420, 27421, 27422, 27423, 27424, 27425, 27426, 27427, 27428, 27429, 27430, 27431, 27432, 27433, 27434, 27435, 27436, 27437, 27438, 27439, 27440, 27441, 27442, 27443, 27444, 27445, 27446, 27447, 27448, 27449, 27450, 27451, 27452, 27453, 27454, 27455, 27456, 27457, 27458, 27459, 27460, 27461, 27462, 27463, 27464, 27465, 27466, 27467, 27468, 27469, 27470, 27471, 27472, 27473, 27474, 27475, 27476, 27477, 27478, 27479, 27480, 27481, 27482, 27483, 27484, 27485, 27486, 27487, 27488, 27489, 27490, 27491, 27492, 27493, 27494, 27495, 27496, 27497, 27498, 27499, 27500, 27501, 27502, 27503, 27504, 27505, 27506, 27507, 27508, 27509, 27510, 27511, 27512, 27513, 27514, 27515, 27516, 27517, 27518, 27519, 27520, 27521, 27522, 27523, 27524, 27525, 27526, 27527, 27528, 27529, 27530, 27531, 27532, 27533, 27534, 27535, 27536, 27537, 27538, 27539, 27540, 27541, 27542, 27543, 27544, 27545, 27546, 27547, 27548, 27549, 27550, 27551, 27552, 27553, 27554, 27555, 27556, 27557, 27558, 27559, 27560, 27561, 27562, 27563, 27564, 27565, 27566, 27567, 27568, 27569, 27570, 27571, 27572, 27573, 27574, 27575, 27576, 27577, 27578, 27579, 27580, 27581, 27582, 27583, 27584, 27585, 27586, 27587, 27588, 27589, 27590, 27591, 27592, 27593, 27594, 27595, 27596, 27597, 27598, 27599, 27600, 27601, 27602, 27603, 27604, 27605, 27606, 27607, 27608, 27609, 27610, 27611, 27612, 27613, 27614, 27615, 27616, 27617, 27618, 27619, 27620, 27621, 27622, 27623, 27624, 27625, 27626, 27627, 27628, 27629, 27630, 27631, 27632, 27633, 27634, 27635, 27636, 27637, 27638, 27639, 27640, 27641, 27642, 27643, 27644, 27645, 27646, 27647, 27648, 27649, 27650, 27651, 27652, 27653, 27654, 27655, 27656, 27657, 27658, 27659, 27660, 27661, 27662, 27663, 27664, 27665, 27666, 27667, 27668, 27669, 27670, 27671, 27672, 27673, 27674, 27675, 27676, 27677, 27678, 27679, 27680, 27681, 27682, 27683, 27684, 27685, 27686, 27687, 27688, 27689, 27690, 27691, 27692, 27693, 27694, 27695, 27696, 27697, 27698, 27699, 27700, 27701, 27702, 27703, 27704, 27705, 27706, 27707, 27708, 27709, 27710, 27711, 27712, 27713, 27714, 27715, 27716, 27717, 27718, 27719, 27720, 27721, 27722, 27723, 27724, 27725, 27726, 27727, 27728, 27729, 27730, 27731, 27732, 27733, 27734, 27735, 27736, 27737, 27738, 27739, 27740, 27741, 27742, 27743, 27744, 27745, 27746, 27747, 27748, 27749, 27750, 27751, 27752, 27753, 27754, 27755, 27756, 27757, 27758, 27759, 27760, 27761, 27762, 27763, 27764, 27765, 27766, 27767, 27768, 27769, 27770, 27771, 27772, 27773, 27774, 27775, 27776, 27777, 27778, 27779, 27780, 27781, 27782, 27783, 27784, 27785, 27786, 27787, 27788, 27789, 27790, 27791, 27792, 27793, 27794, 27795, 27796, 27797, 27798, 27799, 27800, 27801, 27802, 27803, 27804, 27805, 27806, 27807, 27808, 27809, 27810, 27811, 27812, 27813, 27814, 27815, 27816, 27817, 27818, 27819, 27820, 27821, 27822, 27823, 27824, 27825, 27826, 27827, 27828, 27829, 27830, 27831, 27832, 27833, 27834, 27835, 27836, 27837, 27838, 27839, 27840, 27841, 27842, 27843, 27844, 27845, 27846, 27847, 27848, 27849, 27850, 27851, 27852, 27853, 27854, 27855, 27856, 27857, 27858, 27859, 27860, 27861, 27862, 27863, 27864, 27865, 27866, 27867, 27868, 27869, 27870, 27871, 27872, 27873, 27874, 27875, 27876, 27877, 27878, 27879, 27880, 27881, 27882, 27883, 27884, 27885, 27886, 27887, 27888, 27889, 27890, 27891, 27892, 27893, 27894, 27895, 27896, 27897, 27898, 27899, 27900, 27901, 27902, 27903, 27904, 27905, 27906, 27907, 27908, 27909, 27910, 27911, 27912, 27913, 27914, 27915, 27916, 27917, 27918, 27919, 27920, 27921, 27922, 27923, 27924, 27925, 27926, 27927, 27928, 27929, 27930, 27931, 27932, 27933, 27934, 27935, 27936, 27937, 27938, 27939, 27940, 27941, 27942, 27943, 27944, 27945, 27946, 27947, 27948, 27949, 27950, 27951, 27952, 27953, 27954, 27955, 27956, 27957, 27958, 27959, 27960, 27961, 27962, 27963, 27964, 27965, 27966, 27967, 27968, 27969, 27970, 27971, 27972, 27973, 27974, 27975, 27976, 27977, 27978, 27979, 27980, 27981, 27982, 27983, 27984, 27985, 27986, 27987, 27988, 27989, 27990, 27991, 27992, 27993, 27994, 27995, 27996, 27997, 27998, 27999, 28000, 28001, 28002, 28003, 28004, 28005, 28006, 28007, 28008, 28009, 28010, 28011, 28012, 28013, 28014, 28015, 28016, 28017, 28018, 28019, 28020, 28021, 28022, 28023, 28024, 28025, 28026, 28027, 28028, 28029, 28030, 28031, 28032, 28033, 28034, 28035, 28036, 28037, 28038, 28039, 28040, 28041, 28042, 28043, 28044, 28045, 28046, 28047, 28048, 28049, 28050, 28051, 28052, 28053, 28054, 28055, 28056, 28057, 28058, 28059, 28060, 28061, 28062, 28063, 28064, 28065, 28066, 28067, 28068, 28069, 28070, 28071, 28072, 28073, 28074, 28075, 28076, 28077, 28078, 28079, 28080, 28081, 28082, 28083, 28084, 28085, 28086, 28087, 28088, 28089, 28090, 28091, 28092, 28093, 28094, 28095, 28096, 28097, 28098, 28099, 28100, 28101, 28102, 28103, 28104, 28105, 28106, 28107, 28108, 28109, 28110, 28111, 28112, 28113, 28114, 28115, 28116, 28117, 28118, 28119, 28120, 28121, 28122, 28123, 28124, 28125, 28126, 28127, 28128, 28129, 28130, 28131, 28132, 28133, 28134, 28135, 28136, 28137, 28138, 28139, 28140, 28141, 28142, 28143, 28144, 28145, 28146, 28147, 28148, 28149, 28150, 28151, 28152, 28153, 28154, 28155, 28156, 28157, 28158, 28159, 28160, 28161, 28162, 28163, 28164, 28165, 28166, 28167, 28168, 28169, 28170, 28171, 28172, 28173, 28174, 28175, 28176, 28177, 28178, 28179, 28180, 28181, 28182, 28183, 28184, 28185, 28186, 28187, 28188, 28189, 28190, 28191, 28192, 28193, 28194, 28195, 28196, 28197, 28198, 28199, 28200, 28201, 28202, 28203, 28204, 28205, 28206, 28207, 28208, 28209, 28210, 28211, 28212, 28213, 28214, 28215, 28216, 28217, 28218, 28219, 28220, 28221, 28222, 28223, 28224, 28225, 28226, 28227, 28228, 28229, 28230, 28231, 28232, 28233, 28234, 28235, 28236, 28237, 28238, 28239, 28240, 28241, 28242, 28243, 28244, 28245, 28246, 28247, 28248, 28249, 28250, 28251, 28252, 28253, 28254, 28255, 28256, 28257, 28258, 28259, 28260, 28261, 28262, 28263, 28264, 28265, 28266, 28267, 28268, 28269, 28270, 28271, 28272, 28273, 28274, 28275, 28276, 28277, 28278, 28279, 28280, 28281, 28282, 28283, 28284, 28285, 28286, 28287, 28288, 28289, 28290, 28291, 28292, 28293, 28294, 28295, 28296, 28297, 28298, 28299, 28300, 28301, 28302, 28303, 28304, 28305, 28306, 28307, 28308, 28309, 28310, 28311, 28312, 28313, 28314, 28315, 28316, 28317, 28318, 28319, 28320, 28321, 28322, 28323, 28324, 28325, 28326, 28327, 28328, 28329, 28330, 28331, 28332, 28333, 28334, 28335, 28336, 28337, 28338, 28339, 28340, 28341, 28342, 28343, 28344, 28345, 28346, 28347, 28348, 28349, 28350, 28351, 28352, 28353, 28354, 28355, 28356, 28357, 28358, 28359, 28360, 28361, 28362, 28363, 28364, 28365, 28366, 28367, 28368, 28369, 28370, 28371, 28372, 28373, 28374, 28375, 28376, 28377, 28378, 28379, 28380, 28381, 28382, 28383, 28384, 28385, 28386, 28387, 28388, 28389, 28390, 28391, 28392, 28393, 28394, 28395, 28396, 28397, 28398, 28399, 28400, 28401, 28402, 28403, 28404, 28405, 28406, 28407, 28408, 28409, 28410, 28411, 28412, 28413, 28414, 28415, 28416, 28417, 28418, 28419, 28420, 28421, 28422, 28423, 28424, 28425, 28426, 28427, 28428, 28429, 28430, 28431, 28432, 28433, 28434, 28435, 28436, 28437, 28438, 28439, 28440, 28441, 28442, 28443, 28444, 28445, 28446, 28447, 28448, 28449, 28450, 28451, 28452, 28453, 28454, 28455, 28456, 28457, 28458, 28459, 28460, 28461, 28462, 28463, 28464, 28465, 28466, 28467, 28468, 28469, 28470, 28471, 28472, 28473, 28474, 28475, 28476, 28477, 28478, 28479, 28480, 28481, 28482, 28483, 28484, 28485, 28486, 28487, 28488, 28489, 28490, 28491, 28492, 28493, 28494, 28495, 28496, 28497, 28498, 28499, 28500, 28501, 28502, 28503, 28504, 28505, 28506, 28507, 28508, 28509, 28510, 28511, 28512, 28513, 28514, 28515, 28516, 28517, 28518, 28519, 28520, 28521, 28522, 28523, 28524, 28525, 28526, 28527, 28528, 28529, 28530, 28531, 28532, 28533, 28534, 28535, 28536, 28537, 28538, 28539, 28540, 28541, 28542, 28543, 28544, 28545, 28546, 28547, 28548, 28549, 28550, 28551, 28552, 28553, 28554, 28555, 28556, 28557, 28558, 28559, 28560, 28561, 28562, 28563, 28564, 28565, 28566, 28567, 28568, 28569, 28570, 28571, 28572, 28573, 28574, 28575, 28576, 28577, 28578, 28579, 28580, 28581, 28582, 28583, 28584, 28585, 28586, 28587, 28588, 28589, 28590, 28591, 28592, 28593, 28594, 28595, 28596, 28597, 28598, 28599, 28600, 28601, 28602, 28603, 28604, 28605, 28606, 28607, 28608, 28609, 28610, 28611, 28612, 28613, 28614, 28615, 28616, 28617, 28618, 28619, 28620, 28621, 28622, 28623, 28624, 28625, 28626, 28627, 28628, 28629, 28630, 28631, 28632, 28633, 28634, 28635, 28636, 28637, 28638, 28639, 28640, 28641, 28642, 28643, 28644, 28645, 28646, 28647, 28648, 28649, 28650, 28651, 28652, 28653, 28654, 28655, 28656, 28657, 28658, 28659, 28660, 28661, 28662, 28663, 28664, 28665, 28666, 28667, 28668, 28669, 28670, 28671, 28672, 28673, 28674, 28675, 28676, 28677, 28678, 28679, 28680, 28681, 28682, 28683, 28684, 28685, 28686, 28687, 28688, 28689, 28690, 28691, 28692, 28693, 28694, 28695, 28696, 28697, 28698, 28699, 28700, 28701, 28702, 28703, 28704, 28705, 28706, 28707, 28708, 28709, 28710, 28711, 28712, 28713, 28714, 28715, 28716, 28717, 28718, 28719, 28720, 28721, 28722, 28723, 28724, 28725, 28726, 28727, 28728, 28729, 28730, 28731, 28732, 28733, 28734, 28735, 28736, 28737, 28738, 28739, 28740, 28741, 28742, 28743, 28744, 28745, 28746, 28747, 28748, 28749, 28750, 28751, 28752, 28753, 28754, 28755, 28756, 28757, 28758, 28759, 28760, 28761, 28762, 28763, 28764, 28765, 28766, 28767, 28768, 28769, 28770, 28771, 28772, 28773, 28774, 28775, 28776, 28777, 28778, 28779, 28780, 28781, 28782, 28783, 28784, 28785, 28786, 28787, 28788, 28789, 28790, 28791, 28792, 28793, 28794, 28795, 28796, 28797, 28798, 28799, 28800, 28801, 28802, 28803, 28804, 28805, 28806, 28807, 28808, 28809, 28810, 28811, 28812, 28813, 28814, 28815, 28816, 28817, 28818, 28819, 28820, 28821, 28822, 28823, 28824, 28825, 28826, 28827, 28828, 28829, 28830, 28831, 28832, 28833, 28834, 28835, 28836, 28837, 28838, 28839, 28840, 28841, 28842, 28843, 28844, 28845, 28846, 28847, 28848, 28849, 28850, 28851, 28852, 28853, 28854, 28855, 28856, 28857, 28858, 28859, 28860, 28861, 28862, 28863, 28864, 28865, 28866, 28867, 28868, 28869, 28870, 28871, 28872, 28873, 28874, 28875, 28876, 28877, 28878, 28879, 28880, 28881, 28882, 28883, 28884, 28885, 28886, 28887, 28888, 28889, 28890, 28891, 28892, 28893, 28894, 28895, 28896, 28897, 28898, 28899, 28900, 28901, 28902, 28903, 28904, 28905, 28906, 28907, 28908, 28909, 28910, 28911, 28912, 28913, 28914, 28915, 28916, 28917, 28918, 28919, 28920, 28921, 28922, 28923, 28924, 28925, 28926, 28927, 28928, 28929, 28930, 28931, 28932, 28933, 28934, 28935, 28936, 28937, 28938, 28939, 28940, 28941, 28942, 28943, 28944, 28945, 28946, 28947, 28948, 28949, 28950, 28951, 28952, 28953, 28954, 28955, 28956, 28957, 28958, 28959, 28960, 28961, 28962, 28963, 28964, 28965, 28966, 28967, 28968, 28969, 28970, 28971, 28972, 28973, 28974, 28975, 28976, 28977, 28978, 28979, 28980, 28981, 28982, 28983, 28984, 28985, 28986, 28987, 28988, 28989, 28990, 28991, 28992, 28993, 28994, 28995, 28996, 28997, 28998, 28999, 29000, 29001, 29002, 29003, 29004, 29005, 29006, 29007, 29008, 29009, 29010, 29011, 29012, 29013, 29014, 29015, 29016, 29017, 29018, 29019, 29020, 29021, 29022, 29023, 29024, 29025, 29026, 29027, 29028, 29029, 29030, 29031, 29032, 29033, 29034, 29035, 29036, 29037, 29038, 29039, 29040, 29041, 29042, 29043, 29044, 29045, 29046, 29047, 29048, 29049, 29050, 29051, 29052, 29053, 29054, 29055, 29056, 29057, 29058, 29059, 29060, 29061, 29062, 29063, 29064, 29065, 29066, 29067, 29068, 29069, 29070, 29071, 29072, 29073, 29074, 29075, 29076, 29077, 29078, 29079, 29080, 29081, 29082, 29083, 29084, 29085, 29086, 29087, 29088, 29089, 29090, 29091, 29092, 29093, 29094, 29095, 29096, 29097, 29098, 29099, 29100, 29101, 29102, 29103, 29104, 29105, 29106, 29107, 29108, 29109, 29110, 29111, 29112, 29113, 29114, 29115, 29116, 29117, 29118, 29119, 29120, 29121, 29122, 29123, 29124, 29125, 29126, 29127, 29128, 29129, 29130, 29131, 29132, 29133, 29134, 29135, 29136, 29137, 29138, 29139, 29140, 29141, 29142, 29143, 29144, 29145, 29146, 29147, 29148, 29149, 29150, 29151, 29152, 29153, 29154, 29155, 29156, 29157, 29158, 29159, 29160, 29161, 29162, 29163, 29164, 29165, 29166, 29167, 29168, 29169, 29170, 29171, 29172, 29173, 29174, 29175, 29176, 29177, 29178, 29179, 29180, 29181, 29182, 29183, 29184, 29185, 29186, 29187, 29188, 29189, 29190, 29191, 29192, 29193, 29194, 29195, 29196, 29197, 29198, 29199, 29200, 29201, 29202, 29203, 29204, 29205, 29206, 29207, 29208, 29209, 29210, 29211, 29212, 29213, 29214, 29215, 29216, 29217, 29218, 29219, 29220, 29221, 29222, 29223, 29224, 29225, 29226, 29227, 29228, 29229, 29230, 29231, 29232, 29233, 29234, 29235, 29236, 29237, 29238, 29239, 29240, 29241, 29242, 29243, 29244, 29245, 29246, 29247, 29248, 29249, 29250, 29251, 29252, 29253, 29254, 29255, 29256, 29257, 29258, 29259, 29260, 29261, 29262, 29263, 29264, 29265, 29266, 29267, 29268, 29269, 29270, 29271, 29272, 29273, 29274, 29275, 29276, 29277, 29278, 29279, 29280, 29281, 29282, 29283, 29284, 29285, 29286, 29287, 29288, 29289, 29290, 29291, 29292, 29293, 29294, 29295, 29296, 29297, 29298, 29299, 29300, 29301, 29302, 29303, 29304, 29305, 29306, 29307, 29308, 29309, 29310, 29311, 29312, 29313, 29314, 29315, 29316, 29317, 29318, 29319, 29320, 29321, 29322, 29323, 29324, 29325, 29326, 29327, 29328, 29329, 29330, 29331, 29332, 29333, 29334, 29335, 29336, 29337, 29338, 29339, 29340, 29341, 29342, 29343, 29344, 29345, 29346, 29347, 29348, 29349, 29350, 29351, 29352, 29353, 29354, 29355, 29356, 29357, 29358, 29359, 29360, 29361, 29362, 29363, 29364, 29365, 29366, 29367, 29368, 29369, 29370, 29371, 29372, 29373, 29374, 29375, 29376, 29377, 29378, 29379, 29380, 29381, 29382, 29383, 29384, 29385, 29386, 29387, 29388, 29389, 29390, 29391, 29392, 29393, 29394, 29395, 29396, 29397, 29398, 29399, 29400, 29401, 29402, 29403, 29404, 29405, 29406, 29407, 29408, 29409, 29410, 29411, 29412, 29413, 29414, 29415, 29416, 29417, 29418, 29419, 29420, 29421, 29422, 29423, 29424, 29425, 29426, 29427, 29428, 29429, 29430, 29431, 29432, 29433, 29434, 29435, 29436, 29437, 29438, 29439, 29440, 29441, 29442, 29443, 29444, 29445, 29446, 29447, 29448, 29449, 29450, 29451, 29452, 29453, 29454, 29455, 29456, 29457, 29458, 29459, 29460, 29461, 29462, 29463, 29464, 29465, 29466, 29467, 29468, 29469, 29470, 29471, 29472, 29473, 29474, 29475, 29476, 29477, 29478, 29479, 29480, 29481, 29482, 29483, 29484, 29485, 29486, 29487, 29488, 29489, 29490, 29491, 29492, 29493, 29494, 29495, 29496, 29497, 29498, 29499, 29500, 29501, 29502, 29503, 29504, 29505, 29506, 29507, 29508, 29509, 29510, 29511, 29512, 29513, 29514, 29515, 29516, 29517, 29518, 29519, 29520, 29521, 29522, 29523, 29524, 29525, 29526, 29527, 29528, 29529, 29530, 29531, 29532, 29533, 29534, 29535, 29536, 29537, 29538, 29539, 29540, 29541, 29542, 29543, 29544, 29545, 29546, 29547, 29548, 29549, 29550, 29551, 29552, 29553, 29554, 29555, 29556, 29557, 29558, 29559, 29560, 29561, 29562, 29563, 29564, 29565, 29566, 29567, 29568, 29569, 29570, 29571, 29572, 29573, 29574, 29575, 29576, 29577, 29578, 29579, 29580, 29581, 29582, 29583, 29584, 29585, 29586, 29587, 29588, 29589, 29590, 29591, 29592, 29593, 29594, 29595, 29596, 29597, 29598, 29599, 29600, 29601, 29602, 29603, 29604, 29605, 29606, 29607, 29608, 29609, 29610, 29611, 29612, 29613, 29614, 29615, 29616, 29617, 29618, 29619, 29620, 29621, 29622, 29623, 29624, 29625, 29626, 29627, 29628, 29629, 29630, 29631, 29632, 29633, 29634, 29635, 29636, 29637, 29638, 29639, 29640, 29641, 29642, 29643, 29644, 29645, 29646, 29647, 29648, 29649, 29650, 29651, 29652, 29653, 29654, 29655, 29656, 29657, 29658, 29659, 29660, 29661, 29662, 29663, 29664, 29665, 29666, 29667, 29668, 29669, 29670, 29671, 29672, 29673, 29674, 29675, 29676, 29677, 29678, 29679, 29680, 29681, 29682, 29683, 29684, 29685, 29686, 29687, 29688, 29689, 29690, 29691, 29692, 29693, 29694, 29695, 29696, 29697, 29698, 29699, 29700, 29701, 29702, 29703, 29704, 29705, 29706, 29707, 29708, 29709, 29710, 29711, 29712, 29713, 29714, 29715, 29716, 29717, 29718, 29719, 29720, 29721, 29722, 29723, 29724, 29725, 29726, 29727, 29728, 29729, 29730, 29731, 29732, 29733, 29734, 29735, 29736, 29737, 29738, 29739, 29740, 29741, 29742, 29743, 29744, 29745, 29746, 29747, 29748, 29749, 29750, 29751, 29752, 29753, 29754, 29755, 29756, 29757, 29758, 29759, 29760, 29761, 29762, 29763, 29764, 29765, 29766, 29767, 29768, 29769, 29770, 29771, 29772, 29773, 29774, 29775, 29776, 29777, 29778, 29779, 29780, 29781, 29782, 29783, 29784, 29785, 29786, 29787, 29788, 29789, 29790, 29791, 29792, 29793, 29794, 29795, 29796, 29797, 29798, 29799, 29800, 29801, 29802, 29803, 29804, 29805, 29806, 29807, 29808, 29809, 29810, 29811, 29812, 29813, 29814, 29815, 29816, 29817, 29818, 29819, 29820, 29821, 29822, 29823, 29824, 29825, 29826, 29827, 29828, 29829, 29830, 29831, 29832, 29833, 29834, 29835, 29836, 29837, 29838, 29839, 29840, 29841, 29842, 29843, 29844, 29845, 29846, 29847, 29848, 29849, 29850, 29851, 29852, 29853, 29854, 29855, 29856, 29857, 29858, 29859, 29860, 29861, 29862, 29863, 29864, 29865, 29866, 29867, 29868, 29869, 29870, 29871, 29872, 29873, 29874, 29875, 29876, 29877, 29878, 29879, 29880, 29881, 29882, 29883, 29884, 29885, 29886, 29887, 29888, 29889, 29890, 29891, 29892, 29893, 29894, 29895, 29896, 29897, 29898, 29899, 29900, 29901, 29902, 29903, 29904, 29905, 29906, 29907, 29908, 29909, 29910, 29911, 29912, 29913, 29914, 29915, 29916, 29917, 29918, 29919, 29920, 29921, 29922, 29923, 29924, 29925, 29926, 29927, 29928, 29929, 29930, 29931, 29932, 29933, 29934, 29935, 29936, 29937, 29938, 29939, 29940, 29941, 29942, 29943, 29944, 29945, 29946, 29947, 29948, 29949, 29950, 29951, 29952, 29953, 29954, 29955, 29956, 29957, 29958, 29959, 29960, 29961, 29962, 29963, 29964, 29965, 29966, 29967, 29968, 29969, 29970, 29971, 29972, 29973, 29974, 29975, 29976, 29977, 29978, 29979, 29980, 29981, 29982, 29983, 29984, 29985, 29986, 29987, 29988, 29989, 29990, 29991, 29992, 29993, 29994, 29995, 29996, 29997, 29998, 29999, 30000, 30001, 30002, 30003, 30004, 30005, 30006, 30007, 30008, 30009, 30010, 30011, 30012, 30013, 30014, 30015, 30016, 30017, 30018, 30019, 30020, 30021, 30022, 30023, 30024, 30025, 30026, 30027, 30028, 30029, 30030, 30031, 30032, 30033, 30034, 30035, 30036, 30037, 30038, 30039, 30040, 30041, 30042, 30043, 30044, 30045, 30046, 30047, 30048, 30049, 30050, 30051, 30052, 30053, 30054, 30055, 30056, 30057, 30058, 30059, 30060, 30061, 30062, 30063, 30064, 30065, 30066, 30067, 30068, 30069, 30070, 30071, 30072, 30073, 30074, 30075, 30076, 30077, 30078, 30079, 30080, 30081, 30082, 30083, 30084, 30085, 30086, 30087, 30088, 30089, 30090, 30091, 30092, 30093, 30094, 30095, 30096, 30097, 30098, 30099, 30100, 30101, 30102, 30103, 30104, 30105, 30106, 30107, 30108, 30109, 30110, 30111, 30112, 30113, 30114, 30115, 30116, 30117, 30118, 30119, 30120, 30121, 30122, 30123, 30124, 30125, 30126, 30127, 30128, 30129, 30130, 30131, 30132, 30133, 30134, 30135, 30136, 30137, 30138, 30139, 30140, 30141, 30142, 30143, 30144, 30145, 30146, 30147, 30148, 30149, 30150, 30151, 30152, 30153, 30154, 30155, 30156, 30157, 30158, 30159, 30160, 30161, 30162, 30163, 30164, 30165, 30166, 30167, 30168, 30169, 30170, 30171, 30172, 30173, 30174, 30175, 30176, 30177, 30178, 30179, 30180, 30181, 30182, 30183, 30184, 30185, 30186, 30187, 30188, 30189, 30190, 30191, 30192, 30193, 30194, 30195, 30196, 30197, 30198, 30199, 30200, 30201, 30202, 30203, 30204, 30205, 30206, 30207, 30208, 30209, 30210, 30211, 30212, 30213, 30214, 30215, 30216, 30217, 30218, 30219, 30220, 30221, 30222, 30223, 30224, 30225, 30226, 30227, 30228, 30229, 30230, 30231, 30232, 30233, 30234, 30235, 30236, 30237, 30238, 30239, 30240, 30241, 30242, 30243, 30244, 30245, 30246, 30247, 30248, 30249, 30250, 30251, 30252, 30253, 30254, 30255, 30256, 30257, 30258, 30259, 30260, 30261, 30262, 30263, 30264, 30265, 30266, 30267, 30268, 30269, 30270, 30271, 30272, 30273, 30274, 30275, 30276, 30277, 30278, 30279, 30280, 30281, 30282, 30283, 30284, 30285, 30286, 30287, 30288, 30289, 30290, 30291, 30292, 30293, 30294, 30295, 30296, 30297, 30298, 30299, 30300, 30301, 30302, 30303, 30304, 30305, 30306, 30307, 30308, 30309, 30310, 30311, 30312, 30313, 30314, 30315, 30316, 30317, 30318, 30319, 30320, 30321, 30322, 30323, 30324, 30325, 30326, 30327, 30328, 30329, 30330, 30331, 30332, 30333, 30334, 30335, 30336, 30337, 30338, 30339, 30340, 30341, 30342, 30343, 30344, 30345, 30346, 30347, 30348, 30349, 30350, 30351, 30352, 30353, 30354, 30355, 30356, 30357, 30358, 30359, 30360, 30361, 30362, 30363, 30364, 30365, 30366, 30367, 30368, 30369, 30370, 30371, 30372, 30373, 30374, 30375, 30376, 30377, 30378, 30379, 30380, 30381, 30382, 30383, 30384, 30385, 30386, 30387, 30388, 30389, 30390, 30391, 30392, 30393, 30394, 30395, 30396, 30397, 30398, 30399, 30400, 30401, 30402, 30403, 30404, 30405, 30406, 30407, 30408, 30409, 30410, 30411, 30412, 30413, 30414, 30415, 30416, 30417, 30418, 30419, 30420, 30421, 30422, 30423, 30424, 30425, 30426, 30427, 30428, 30429, 30430, 30431, 30432, 30433, 30434, 30435, 30436, 30437, 30438, 30439, 30440, 30441, 30442, 30443, 30444, 30445, 30446, 30447, 30448, 30449, 30450, 30451, 30452, 30453, 30454, 30455, 30456, 30457, 30458, 30459, 30460, 30461, 30462, 30463, 30464, 30465, 30466, 30467, 30468, 30469, 30470, 30471, 30472, 30473, 30474, 30475, 30476, 30477, 30478, 30479, 30480, 30481, 30482, 30483, 30484, 30485, 30486, 30487, 30488, 30489, 30490, 30491, 30492, 30493, 30494, 30495, 30496, 30497, 30498, 30499, 30500, 30501, 30502, 30503, 30504, 30505, 30506, 30507, 30508, 30509, 30510, 30511, 30512, 30513, 30514, 30515, 30516, 30517, 30518, 30519, 30520, 30521, 30522, 30523, 30524, 30525, 30526, 30527, 30528, 30529, 30530, 30531, 30532, 30533, 30534, 30535, 30536, 30537, 30538, 30539, 30540, 30541, 30542, 30543, 30544, 30545, 30546, 30547, 30548, 30549, 30550, 30551, 30552, 30553, 30554, 30555, 30556, 30557, 30558, 30559, 30560, 30561, 30562, 30563, 30564, 30565, 30566, 30567, 30568, 30569, 30570, 30571, 30572, 30573, 30574, 30575, 30576, 30577, 30578, 30579, 30580, 30581, 30582, 30583, 30584, 30585, 30586, 30587, 30588, 30589, 30590, 30591, 30592, 30593, 30594, 30595, 30596, 30597, 30598, 30599, 30600, 30601, 30602, 30603, 30604, 30605, 30606, 30607, 30608, 30609, 30610, 30611, 30612, 30613, 30614, 30615, 30616, 30617, 30618, 30619, 30620, 30621, 30622, 30623, 30624, 30625, 30626, 30627, 30628, 30629, 30630, 30631, 30632, 30633, 30634, 30635, 30636, 30637, 30638, 30639, 30640, 30641, 30642, 30643, 30644, 30645, 30646, 30647, 30648, 30649, 30650, 30651, 30652, 30653, 30654, 30655, 30656, 30657, 30658, 30659, 30660, 30661, 30662, 30663, 30664, 30665, 30666, 30667, 30668, 30669, 30670, 30671, 30672, 30673, 30674, 30675, 30676, 30677, 30678, 30679, 30680, 30681, 30682, 30683, 30684, 30685, 30686, 30687, 30688, 30689, 30690, 30691, 30692, 30693, 30694, 30695, 30696, 30697, 30698, 30699, 30700, 30701, 30702, 30703, 30704, 30705, 30706, 30707, 30708, 30709, 30710, 30711, 30712, 30713, 30714, 30715, 30716, 30717, 30718, 30719, 30720, 30721, 30722, 30723, 30724, 30725, 30726, 30727, 30728, 30729, 30730, 30731, 30732, 30733, 30734, 30735, 30736, 30737, 30738, 30739, 30740, 30741, 30742, 30743, 30744, 30745, 30746, 30747, 30748, 30749, 30750, 30751, 30752, 30753, 30754, 30755, 30756, 30757, 30758, 30759, 30760, 30761, 30762, 30763, 30764, 30765, 30766, 30767, 30768, 30769, 30770, 30771, 30772, 30773, 30774, 30775, 30776, 30777, 30778, 30779, 30780, 30781, 30782, 30783, 30784, 30785, 30786, 30787, 30788, 30789, 30790, 30791, 30792, 30793, 30794, 30795, 30796, 30797, 30798, 30799, 30800, 30801, 30802, 30803, 30804, 30805, 30806, 30807, 30808, 30809, 30810, 30811, 30812, 30813, 30814, 30815, 30816, 30817, 30818, 30819, 30820, 30821, 30822, 30823, 30824, 30825, 30826, 30827, 30828, 30829, 30830, 30831, 30832, 30833, 30834, 30835, 30836, 30837, 30838, 30839, 30840, 30841, 30842, 30843, 30844, 30845, 30846, 30847, 30848, 30849, 30850, 30851, 30852, 30853, 30854, 30855, 30856, 30857, 30858, 30859, 30860, 30861, 30862, 30863, 30864, 30865, 30866, 30867, 30868, 30869, 30870, 30871, 30872, 30873, 30874, 30875, 30876, 30877, 30878, 30879, 30880, 30881, 30882, 30883, 30884, 30885, 30886, 30887, 30888, 30889, 30890, 30891, 30892, 30893, 30894, 30895, 30896, 30897, 30898, 30899, 30900, 30901, 30902, 30903, 30904, 30905, 30906, 30907, 30908, 30909, 30910, 30911, 30912, 30913, 30914, 30915, 30916, 30917, 30918, 30919, 30920, 30921, 30922, 30923, 30924, 30925, 30926, 30927, 30928, 30929, 30930, 30931, 30932, 30933, 30934, 30935, 30936, 30937, 30938, 30939, 30940, 30941, 30942, 30943, 30944, 30945, 30946, 30947, 30948, 30949, 30950, 30951, 30952, 30953, 30954, 30955, 30956, 30957, 30958, 30959, 30960, 30961, 30962, 30963, 30964, 30965, 30966, 30967, 30968, 30969, 30970, 30971, 30972, 30973, 30974, 30975, 30976, 30977, 30978, 30979, 30980, 30981, 30982, 30983, 30984, 30985, 30986, 30987, 30988, 30989, 30990, 30991, 30992, 30993, 30994, 30995, 30996, 30997, 30998, 30999, 31000, 31001, 31002, 31003, 31004, 31005, 31006, 31007, 31008, 31009, 31010, 31011, 31012, 31013, 31014, 31015, 31016, 31017, 31018, 31019, 31020, 31021, 31022, 31023, 31024, 31025, 31026, 31027, 31028, 31029, 31030, 31031, 31032, 31033, 31034, 31035, 31036, 31037, 31038, 31039, 31040, 31041, 31042, 31043, 31044, 31045, 31046, 31047, 31048, 31049, 31050, 31051, 31052, 31053, 31054, 31055, 31056, 31057, 31058, 31059, 31060, 31061, 31062, 31063, 31064, 31065, 31066, 31067, 31068, 31069, 31070, 31071, 31072, 31073, 31074, 31075, 31076, 31077, 31078, 31079, 31080, 31081, 31082, 31083, 31084, 31085, 31086, 31087, 31088, 31089, 31090, 31091, 31092, 31093, 31094, 31095, 31096, 31097, 31098, 31099, 31100, 31101, 31102, 31103, 31104, 31105, 31106, 31107, 31108, 31109, 31110, 31111, 31112, 31113, 31114, 31115, 31116, 31117, 31118, 31119, 31120, 31121, 31122, 31123, 31124, 31125, 31126, 31127, 31128, 31129, 31130, 31131, 31132, 31133, 31134, 31135, 31136, 31137, 31138, 31139, 31140, 31141, 31142, 31143, 31144, 31145, 31146, 31147, 31148, 31149, 31150, 31151, 31152, 31153, 31154, 31155, 31156, 31157, 31158, 31159, 31160, 31161, 31162, 31163, 31164, 31165, 31166, 31167, 31168, 31169, 31170, 31171, 31172, 31173, 31174, 31175, 31176, 31177, 31178, 31179, 31180, 31181, 31182, 31183, 31184, 31185, 31186, 31187, 31188, 31189, 31190, 31191, 31192, 31193, 31194, 31195, 31196, 31197, 31198, 31199, 31200, 31201, 31202, 31203, 31204, 31205, 31206, 31207, 31208, 31209, 31210, 31211, 31212, 31213, 31214, 31215, 31216, 31217, 31218, 31219, 31220, 31221, 31222, 31223, 31224, 31225, 31226, 31227, 31228, 31229, 31230, 31231, 31232, 31233, 31234, 31235, 31236, 31237, 31238, 31239, 31240, 31241, 31242, 31243, 31244, 31245, 31246, 31247, 31248, 31249, 31250, 31251, 31252, 31253, 31254, 31255, 31256, 31257, 31258, 31259, 31260, 31261, 31262, 31263, 31264, 31265, 31266, 31267, 31268, 31269, 31270, 31271, 31272, 31273, 31274, 31275, 31276, 31277, 31278, 31279, 31280, 31281, 31282, 31283, 31284, 31285, 31286, 31287, 31288, 31289, 31290, 31291, 31292, 31293, 31294, 31295, 31296, 31297, 31298, 31299, 31300, 31301, 31302, 31303, 31304, 31305, 31306, 31307, 31308, 31309, 31310, 31311, 31312, 31313, 31314, 31315, 31316, 31317, 31318, 31319, 31320, 31321, 31322, 31323, 31324, 31325, 31326, 31327, 31328, 31329, 31330, 31331, 31332, 31333, 31334, 31335, 31336, 31337, 31338, 31339, 31340, 31341, 31342, 31343, 31344, 31345, 31346, 31347, 31348, 31349, 31350, 31351, 31352, 31353, 31354, 31355, 31356, 31357, 31358, 31359, 31360, 31361, 31362, 31363, 31364, 31365, 31366, 31367, 31368, 31369, 31370, 31371, 31372, 31373, 31374, 31375, 31376, 31377, 31378, 31379, 31380, 31381, 31382, 31383, 31384, 31385, 31386, 31387, 31388, 31389, 31390, 31391, 31392, 31393, 31394, 31395, 31396, 31397, 31398, 31399, 31400, 31401, 31402, 31403, 31404, 31405, 31406, 31407, 31408, 31409, 31410, 31411, 31412, 31413, 31414, 31415, 31416, 31417, 31418, 31419, 31420, 31421, 31422, 31423, 31424, 31425, 31426, 31427, 31428, 31429, 31430, 31431, 31432, 31433, 31434, 31435, 31436, 31437, 31438, 31439, 31440, 31441, 31442, 31443, 31444, 31445, 31446, 31447, 31448, 31449, 31450, 31451, 31452, 31453, 31454, 31455, 31456, 31457, 31458, 31459, 31460, 31461, 31462, 31463, 31464, 31465, 31466, 31467, 31468, 31469, 31470, 31471, 31472, 31473, 31474, 31475, 31476, 31477, 31478, 31479, 31480, 31481, 31482, 31483, 31484, 31485, 31486, 31487, 31488, 31489, 31490, 31491, 31492, 31493, 31494, 31495, 31496, 31497, 31498, 31499, 31500, 31501, 31502, 31503, 31504, 31505, 31506, 31507, 31508, 31509, 31510, 31511, 31512, 31513, 31514, 31515, 31516, 31517, 31518, 31519, 31520, 31521, 31522, 31523, 31524, 31525, 31526, 31527, 31528, 31529, 31530, 31531, 31532, 31533, 31534, 31535, 31536, 31537, 31538, 31539, 31540, 31541, 31542, 31543, 31544, 31545, 31546, 31547, 31548, 31549, 31550, 31551, 31552, 31553, 31554, 31555, 31556, 31557, 31558, 31559, 31560, 31561, 31562, 31563, 31564, 31565, 31566, 31567, 31568, 31569, 31570, 31571, 31572, 31573, 31574, 31575, 31576, 31577, 31578, 31579, 31580, 31581, 31582, 31583, 31584, 31585, 31586, 31587, 31588, 31589, 31590, 31591, 31592, 31593, 31594, 31595, 31596, 31597, 31598, 31599, 31600, 31601, 31602, 31603, 31604, 31605, 31606, 31607, 31608, 31609, 31610, 31611, 31612, 31613, 31614, 31615, 31616, 31617, 31618, 31619, 31620, 31621, 31622, 31623, 31624, 31625, 31626, 31627, 31628, 31629, 31630, 31631, 31632, 31633, 31634, 31635, 31636, 31637, 31638, 31639, 31640, 31641, 31642, 31643, 31644, 31645, 31646, 31647, 31648, 31649, 31650, 31651, 31652, 31653, 31654, 31655, 31656, 31657, 31658, 31659, 31660, 31661, 31662, 31663, 31664, 31665, 31666, 31667, 31668, 31669, 31670, 31671, 31672, 31673, 31674, 31675, 31676, 31677, 31678, 31679, 31680, 31681, 31682, 31683, 31684, 31685, 31686, 31687, 31688, 31689, 31690, 31691, 31692, 31693, 31694, 31695, 31696, 31697, 31698, 31699, 31700, 31701, 31702, 31703, 31704, 31705, 31706, 31707, 31708, 31709, 31710, 31711, 31712, 31713, 31714, 31715, 31716, 31717, 31718, 31719, 31720, 31721, 31722, 31723, 31724, 31725, 31726, 31727, 31728, 31729, 31730, 31731, 31732, 31733, 31734, 31735, 31736, 31737, 31738, 31739, 31740, 31741, 31742, 31743, 31744, 31745, 31746, 31747, 31748, 31749, 31750, 31751, 31752, 31753, 31754, 31755, 31756, 31757, 31758, 31759, 31760, 31761, 31762, 31763, 31764, 31765, 31766, 31767, 31768, 31769, 31770, 31771, 31772, 31773, 31774, 31775, 31776, 31777, 31778, 31779, 31780, 31781, 31782, 31783, 31784, 31785, 31786, 31787, 31788, 31789, 31790, 31791, 31792, 31793, 31794, 31795, 31796, 31797, 31798, 31799, 31800, 31801, 31802, 31803, 31804, 31805, 31806, 31807, 31808, 31809, 31810, 31811, 31812, 31813, 31814, 31815, 31816, 31817, 31818, 31819, 31820, 31821, 31822, 31823, 31824, 31825, 31826, 31827, 31828, 31829, 31830, 31831, 31832, 31833, 31834, 31835, 31836, 31837, 31838, 31839, 31840, 31841, 31842, 31843, 31844, 31845, 31846, 31847, 31848, 31849, 31850, 31851, 31852, 31853, 31854, 31855, 31856, 31857, 31858, 31859, 31860, 31861, 31862, 31863, 31864, 31865, 31866, 31867, 31868, 31869, 31870, 31871, 31872, 31873, 31874, 31875, 31876, 31877, 31878, 31879, 31880, 31881, 31882, 31883, 31884, 31885, 31886, 31887, 31888, 31889, 31890, 31891, 31892, 31893, 31894, 31895, 31896, 31897, 31898, 31899, 31900, 31901, 31902, 31903, 31904, 31905, 31906, 31907, 31908, 31909, 31910, 31911, 31912, 31913, 31914, 31915, 31916, 31917, 31918, 31919, 31920, 31921, 31922, 31923, 31924, 31925, 31926, 31927, 31928, 31929, 31930, 31931, 31932, 31933, 31934, 31935, 31936, 31937, 31938, 31939, 31940, 31941, 31942, 31943, 31944, 31945, 31946, 31947, 31948, 31949, 31950, 31951, 31952, 31953, 31954, 31955, 31956, 31957, 31958, 31959, 31960, 31961, 31962, 31963, 31964, 31965, 31966, 31967, 31968, 31969, 31970, 31971, 31972, 31973, 31974, 31975, 31976, 31977, 31978, 31979, 31980, 31981, 31982, 31983, 31984, 31985, 31986, 31987, 31988, 31989, 31990, 31991, 31992, 31993, 31994, 31995, 31996, 31997, 31998, 31999, 32000, 32001, 32002, 32003, 32004, 32005, 32006, 32007, 32008, 32009, 32010, 32011, 32012, 32013, 32014, 32015, 32016, 32017, 32018, 32019, 32020, 32021, 32022, 32023, 32024, 32025, 32026, 32027, 32028, 32029, 32030, 32031, 32032, 32033, 32034, 32035, 32036, 32037, 32038, 32039, 32040, 32041, 32042, 32043, 32044, 32045, 32046, 32047, 32048, 32049, 32050, 32051, 32052, 32053, 32054, 32055, 32056, 32057, 32058, 32059, 32060, 32061, 32062, 32063, 32064, 32065, 32066, 32067, 32068, 32069, 32070, 32071, 32072, 32073, 32074, 32075, 32076, 32077, 32078, 32079, 32080, 32081, 32082, 32083, 32084, 32085, 32086, 32087, 32088, 32089, 32090, 32091, 32092, 32093, 32094, 32095, 32096, 32097, 32098, 32099, 32100, 32101, 32102, 32103, 32104, 32105, 32106, 32107, 32108, 32109, 32110, 32111, 32112, 32113, 32114, 32115, 32116, 32117, 32118, 32119, 32120, 32121, 32122, 32123, 32124, 32125, 32126, 32127, 32128, 32129, 32130, 32131, 32132, 32133, 32134, 32135, 32136, 32137, 32138, 32139, 32140, 32141, 32142, 32143, 32144, 32145, 32146, 32147, 32148, 32149, 32150, 32151, 32152, 32153, 32154, 32155, 32156, 32157, 32158, 32159, 32160, 32161, 32162, 32163, 32164, 32165, 32166, 32167, 32168, 32169, 32170, 32171, 32172, 32173, 32174, 32175, 32176, 32177, 32178, 32179, 32180, 32181, 32182, 32183, 32184, 32185, 32186, 32187, 32188, 32189, 32190, 32191, 32192, 32193, 32194, 32195, 32196, 32197, 32198, 32199, 32200, 32201, 32202, 32203, 32204, 32205, 32206, 32207, 32208, 32209, 32210, 32211, 32212, 32213, 32214, 32215, 32216, 32217, 32218, 32219, 32220, 32221, 32222, 32223, 32224, 32225, 32226, 32227, 32228, 32229, 32230, 32231, 32232, 32233, 32234, 32235, 32236, 32237, 32238, 32239, 32240, 32241, 32242, 32243, 32244, 32245, 32246, 32247, 32248, 32249, 32250, 32251, 32252, 32253, 32254, 32255, 32256, 32257, 32258, 32259, 32260, 32261, 32262, 32263, 32264, 32265, 32266, 32267, 32268, 32269, 32270, 32271, 32272, 32273, 32274, 32275, 32276, 32277, 32278, 32279, 32280, 32281, 32282, 32283, 32284, 32285, 32286, 32287, 32288, 32289, 32290, 32291, 32292, 32293, 32294, 32295, 32296, 32297, 32298, 32299, 32300, 32301, 32302, 32303, 32304, 32305, 32306, 32307, 32308, 32309, 32310, 32311, 32312, 32313, 32314, 32315, 32316, 32317, 32318, 32319, 32320, 32321, 32322, 32323, 32324, 32325, 32326, 32327, 32328, 32329, 32330, 32331, 32332, 32333, 32334, 32335, 32336, 32337, 32338, 32339, 32340, 32341, 32342, 32343, 32344, 32345, 32346, 32347, 32348, 32349, 32350, 32351, 32352, 32353, 32354, 32355, 32356, 32357, 32358, 32359, 32360, 32361, 32362, 32363, 32364, 32365, 32366, 32367, 32368, 32369, 32370, 32371, 32372, 32373, 32374, 32375, 32376, 32377, 32378, 32379, 32380, 32381, 32382, 32383, 32384, 32385, 32386, 32387, 32388, 32389, 32390, 32391, 32392, 32393, 32394, 32395, 32396, 32397, 32398, 32399, 32400, 32401, 32402, 32403, 32404, 32405, 32406, 32407, 32408, 32409, 32410, 32411, 32412, 32413, 32414, 32415, 32416, 32417, 32418, 32419, 32420, 32421, 32422, 32423, 32424, 32425, 32426, 32427, 32428, 32429, 32430, 32431, 32432, 32433, 32434, 32435, 32436, 32437, 32438, 32439, 32440, 32441, 32442, 32443, 32444, 32445, 32446, 32447, 32448, 32449, 32450, 32451, 32452, 32453, 32454, 32455, 32456, 32457, 32458, 32459, 32460, 32461, 32462, 32463, 32464, 32465, 32466, 32467, 32468, 32469, 32470, 32471, 32472, 32473, 32474, 32475, 32476, 32477, 32478, 32479, 32480, 32481, 32482, 32483, 32484, 32485, 32486, 32487, 32488, 32489, 32490, 32491, 32492, 32493, 32494, 32495, 32496, 32497, 32498, 32499, 32500, 32501, 32502, 32503, 32504, 32505, 32506, 32507, 32508, 32509, 32510, 32511, 32512, 32513, 32514, 32515, 32516, 32517, 32518, 32519, 32520, 32521, 32522, 32523, 32524, 32525, 32526, 32527, 32528, 32529, 32530, 32531, 32532, 32533, 32534, 32535, 32536, 32537, 32538, 32539, 32540, 32541, 32542, 32543, 32544, 32545, 32546, 32547, 32548, 32549, 32550, 32551, 32552, 32553, 32554, 32555, 32556, 32557, 32558, 32559, 32560]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.294910</td>\n",
              "      <td>0.291153</td>\n",
              "      <td>0.872543</td>\n",
              "      <td>0.868129</td>\n",
              "      <td>0.872543</td>\n",
              "      <td>0.864567</td>\n",
              "      <td>00:55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.299958</td>\n",
              "      <td>1.734788</td>\n",
              "      <td>0.858569</td>\n",
              "      <td>0.861344</td>\n",
              "      <td>0.858569</td>\n",
              "      <td>0.840896</td>\n",
              "      <td>00:56</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement since epoch 0: early stopping\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.289194</td>\n",
              "      <td>9.225492</td>\n",
              "      <td>0.864865</td>\n",
              "      <td>0.861142</td>\n",
              "      <td>0.864865</td>\n",
              "      <td>0.853659</td>\n",
              "      <td>01:10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.306495</td>\n",
              "      <td>0.271624</td>\n",
              "      <td>0.873157</td>\n",
              "      <td>0.869304</td>\n",
              "      <td>0.873157</td>\n",
              "      <td>0.864578</td>\n",
              "      <td>01:03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>fbeta_score</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.328635</td>\n",
              "      <td>0.579866</td>\n",
              "      <td>0.854423</td>\n",
              "      <td>0.851579</td>\n",
              "      <td>0.854423</td>\n",
              "      <td>0.838911</td>\n",
              "      <td>00:42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.330141</td>\n",
              "      <td>0.299349</td>\n",
              "      <td>0.862869</td>\n",
              "      <td>0.870200</td>\n",
              "      <td>0.862869</td>\n",
              "      <td>0.865641</td>\n",
              "      <td>00:53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.327258</td>\n",
              "      <td>0.722311</td>\n",
              "      <td>0.868858</td>\n",
              "      <td>0.870514</td>\n",
              "      <td>0.868857</td>\n",
              "      <td>0.869619</td>\n",
              "      <td>00:43</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No improvement since epoch 1: early stopping\n"
          ]
        }
      ],
      "source": [
        "#dbck\n",
        "path = untar_data(URLs.ADULT_SAMPLE)\n",
        "df_ = pd.read_csv(path/'adult.csv')\n",
        "print(df_.head())\n",
        "label_col_ = \"salary\"\n",
        "tab_learn = train_fastai_tabular_classifier(df=df_, label_col=label_col_, cnt_cols=None, cat_cols=None, lr=0.005, max_epochs=100, model_path='/content/drive/My Drive/fastai_multimodal/model/', model_name='tabular_model')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgYAsnH7gIyE",
        "outputId": "55f89dd7-980e-4236-a3bd-68e32e5b6578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TabularModel(\n",
            "  (embeds): ModuleList(\n",
            "    (0): Embedding(74, 18)\n",
            "    (1): Embedding(117, 23)\n",
            "    (2): Embedding(90, 20)\n",
            "    (3): Embedding(17, 8)\n",
            "    (4): Embedding(93, 20)\n",
            "    (5): Embedding(8, 5)\n",
            "    (6): Embedding(43, 13)\n",
            "    (7): Embedding(16, 8)\n",
            "    (8): Embedding(6, 4)\n",
            "    (9): Embedding(7, 5)\n",
            "    (10): Embedding(3, 3)\n",
            "    (11): Embedding(10, 6)\n",
            "    (12): Embedding(3, 3)\n",
            "  )\n",
            "  (emb_drop): Dropout(p=0.0, inplace=False)\n",
            "  (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layers): Sequential(\n",
            "    (0): LinBnDrop(\n",
            "      (0): Linear(in_features=138, out_features=200, bias=False)\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): LinBnDrop(\n",
            "      (0): Linear(in_features=200, out_features=100, bias=False)\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (2): LinBnDrop(\n",
            "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Linear(in_features=200, out_features=100, bias=False)"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# understand the model architecture\n",
        "print(tab_learn.model)\n",
        "\n",
        "# identify layer from where embeddings are extracted\n",
        "tab_learn.model.layers[1][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHvmnJm8pxTL"
      },
      "source": [
        "## experiment: get_fastai_imgs_embs\n",
        "\n",
        "To extract image embedding, check out this post\n",
        "\n",
        "- https://www.kaggle.com/code/abhikjha/fastai-pytorch-hooks-random-forest/notebook#Fastai---Tabular"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJOP5g4ogBPO"
      },
      "outputs": [],
      "source": [
        "# pytorch hook\n",
        "class SaveFeatures():\n",
        "    features=None\n",
        "    def __init__(self, m): \n",
        "        self.hook = m.register_forward_hook(self.hook_fn)\n",
        "        self.features = None\n",
        "    def hook_fn(self, module, input, output): \n",
        "        out = output.detach().cpu().numpy()\n",
        "        if isinstance(self.features, type(None)):\n",
        "            self.features = out\n",
        "        else:\n",
        "            self.features = np.row_stack((self.features, out))\n",
        "    def remove(self): \n",
        "        self.hook.remove()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvVSaU0fw_oZ",
        "outputId": "8ebc945e-ba0d-4e5d-cd62-bdc73d296a60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (3): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (3): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (4): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (5): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (7): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (1): Sequential(\n",
            "    (0): AdaptiveConcatPool2d(\n",
            "      (ap): AdaptiveAvgPool2d(output_size=1)\n",
            "      (mp): AdaptiveMaxPool2d(output_size=1)\n",
            "    )\n",
            "    (1): Flatten(full=False)\n",
            "    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): Dropout(p=0.25, inplace=False)\n",
            "    (4): Linear(in_features=1024, out_features=512, bias=False)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): Dropout(p=0.5, inplace=False)\n",
            "    (8): Linear(in_features=512, out_features=2, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Linear(in_features=1024, out_features=512, bias=False)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(img_clf.model)\n",
        "# identify the layer from which you want to get embeddings \n",
        "img_clf.model[1][4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvyxGDeriqhB"
      },
      "outputs": [],
      "source": [
        "sf = SaveFeatures(img_clf.model[1][4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU2E9hFkyZKU",
        "outputId": "87fc9dce-0e23-42f8-d256-26e6b2cd13f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                  name  label\n",
            "206   train/3/9959.png      3\n",
            "974   valid/3/9177.png      3\n",
            "46    train/3/9293.png      3\n",
            "1215  valid/7/8316.png      7\n",
            "635   train/7/9571.png      7\n",
            "908   valid/3/7312.png      3\n",
            "746   valid/3/7140.png      3\n",
            "1031  valid/3/8254.png      3\n",
            "951   valid/3/9283.png      3\n",
            "952   valid/3/8354.png      3\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<fastai.data.core.TfmdDL at 0x7f147ceef250>"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# access dls from the trained classifier\n",
        "test_df = df.sample(10)\n",
        "print(test_df)\n",
        "test_dl = img_clf.dls.test_dl(test_df, with_labels=True)\n",
        "test_dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "YD0g5dKliqj0",
        "outputId": "14b96b9f-dc8e-4746-b360-ecb0c77a51c4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# run img_clf through test data\n",
        "\n",
        "preds, _ = img_clf.get_preds(dl=test_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc-Y0ngszIEW",
        "outputId": "5189da9e-0c2d-440c-e645-4af8d70df7ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10, 512)"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get embeddings of training data\n",
        "sf.features.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lxMZj356bFN"
      },
      "source": [
        "# 0) create datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azDAhIkmMosY"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "nrows = 10**20\n",
        "data_path='/content/drive/MyDrive/fastai_multimodal/datasets/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syHIx-5XO8CO"
      },
      "source": [
        "## dataset0 (cnt, cat)\n",
        "\n",
        "This example uses the\n",
        "[United States Census Income Dataset](https://archive.ics.uci.edu/ml/datasets/Census-Income+%28KDD%29)\n",
        "provided by the\n",
        "[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).\n",
        "The task is binary classification to determine whether a person makes over 50K a year.\n",
        "\n",
        "The dataset includes ~300K instances with 41 input features: 7 numerical features\n",
        "and 34 categorical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "0Z9oOpnMPHY_",
        "outputId": "66ab3cab-8689-4f41-eeaf-684d50cc41af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: (199523, 42)\n",
            "Test data shape: (99762, 42)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e397eb71-b0b4-4d10-b987-3e92da9c1cde\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>class_of_worker</th>\n",
              "      <th>detailed_industry_recode</th>\n",
              "      <th>detailed_occupation_recode</th>\n",
              "      <th>education</th>\n",
              "      <th>wage_per_hour</th>\n",
              "      <th>enroll_in_edu_inst_last_wk</th>\n",
              "      <th>marital_stat</th>\n",
              "      <th>major_industry_code</th>\n",
              "      <th>major_occupation_code</th>\n",
              "      <th>...</th>\n",
              "      <th>country_of_birth_father</th>\n",
              "      <th>country_of_birth_mother</th>\n",
              "      <th>country_of_birth_self</th>\n",
              "      <th>citizenship</th>\n",
              "      <th>own_business_or_self_employed</th>\n",
              "      <th>fill_inc_questionnaire_for_veteran's_admin</th>\n",
              "      <th>veterans_benefits</th>\n",
              "      <th>weeks_worked_in_year</th>\n",
              "      <th>year</th>\n",
              "      <th>income_level</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>199518</th>\n",
              "      <td>87</td>\n",
              "      <td>Not in universe</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7th and 8th grade</td>\n",
              "      <td>0</td>\n",
              "      <td>Not in universe</td>\n",
              "      <td>Married-civilian spouse present</td>\n",
              "      <td>Not in universe or children</td>\n",
              "      <td>Not in universe</td>\n",
              "      <td>...</td>\n",
              "      <td>Canada</td>\n",
              "      <td>United-States</td>\n",
              "      <td>United-States</td>\n",
              "      <td>Native- Born in the United States</td>\n",
              "      <td>0</td>\n",
              "      <td>Not in universe</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199519</th>\n",
              "      <td>65</td>\n",
              "      <td>Self-employed-incorporated</td>\n",
              "      <td>37</td>\n",
              "      <td>2</td>\n",
              "      <td>11th grade</td>\n",
              "      <td>0</td>\n",
              "      <td>Not in universe</td>\n",
              "      <td>Married-civilian spouse present</td>\n",
              "      <td>Business and repair services</td>\n",
              "      <td>Executive admin and managerial</td>\n",
              "      <td>...</td>\n",
              "      <td>United-States</td>\n",
              "      <td>United-States</td>\n",
              "      <td>United-States</td>\n",
              "      <td>Native- Born in the United States</td>\n",
              "      <td>0</td>\n",
              "      <td>Not in universe</td>\n",
              "      <td>2</td>\n",
              "      <td>52</td>\n",
              "      <td>94</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199520</th>\n",
              "      <td>47</td>\n",
              "      <td>Not in universe</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Some college but no degree</td>\n",
              "      <td>0</td>\n",
              "      <td>Not in universe</td>\n",
              "      <td>Married-civilian spouse present</td>\n",
              "      <td>Not in universe or children</td>\n",
              "      <td>Not in universe</td>\n",
              "      <td>...</td>\n",
              "      <td>Poland</td>\n",
              "      <td>Poland</td>\n",
              "      <td>Germany</td>\n",
              "      <td>Foreign born- U S citizen by naturalization</td>\n",
              "      <td>0</td>\n",
              "      <td>Not in universe</td>\n",
              "      <td>2</td>\n",
              "      <td>52</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199521</th>\n",
              "      <td>16</td>\n",
              "      <td>Not in universe</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10th grade</td>\n",
              "      <td>0</td>\n",
              "      <td>High school</td>\n",
              "      <td>Never married</td>\n",
              "      <td>Not in universe or children</td>\n",
              "      <td>Not in universe</td>\n",
              "      <td>...</td>\n",
              "      <td>United-States</td>\n",
              "      <td>United-States</td>\n",
              "      <td>United-States</td>\n",
              "      <td>Native- Born in the United States</td>\n",
              "      <td>0</td>\n",
              "      <td>Not in universe</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199522</th>\n",
              "      <td>32</td>\n",
              "      <td>Private</td>\n",
              "      <td>42</td>\n",
              "      <td>30</td>\n",
              "      <td>High school graduate</td>\n",
              "      <td>0</td>\n",
              "      <td>Not in universe</td>\n",
              "      <td>Never married</td>\n",
              "      <td>Medical except hospital</td>\n",
              "      <td>Other service</td>\n",
              "      <td>...</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>Foreign born- Not a citizen of U S</td>\n",
              "      <td>0</td>\n",
              "      <td>Not in universe</td>\n",
              "      <td>2</td>\n",
              "      <td>52</td>\n",
              "      <td>94</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 42 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e397eb71-b0b4-4d10-b987-3e92da9c1cde')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e397eb71-b0b4-4d10-b987-3e92da9c1cde button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e397eb71-b0b4-4d10-b987-3e92da9c1cde');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        age              class_of_worker  detailed_industry_recode  \\\n",
              "199518   87              Not in universe                         0   \n",
              "199519   65   Self-employed-incorporated                        37   \n",
              "199520   47              Not in universe                         0   \n",
              "199521   16              Not in universe                         0   \n",
              "199522   32                      Private                        42   \n",
              "\n",
              "        detailed_occupation_recode                    education  \\\n",
              "199518                           0            7th and 8th grade   \n",
              "199519                           2                   11th grade   \n",
              "199520                           0   Some college but no degree   \n",
              "199521                           0                   10th grade   \n",
              "199522                          30         High school graduate   \n",
              "\n",
              "        wage_per_hour enroll_in_edu_inst_last_wk  \\\n",
              "199518              0            Not in universe   \n",
              "199519              0            Not in universe   \n",
              "199520              0            Not in universe   \n",
              "199521              0                High school   \n",
              "199522              0            Not in universe   \n",
              "\n",
              "                            marital_stat            major_industry_code  \\\n",
              "199518   Married-civilian spouse present    Not in universe or children   \n",
              "199519   Married-civilian spouse present   Business and repair services   \n",
              "199520   Married-civilian spouse present    Not in universe or children   \n",
              "199521                     Never married    Not in universe or children   \n",
              "199522                     Never married        Medical except hospital   \n",
              "\n",
              "                  major_occupation_code  ... country_of_birth_father  \\\n",
              "199518                  Not in universe  ...                  Canada   \n",
              "199519   Executive admin and managerial  ...           United-States   \n",
              "199520                  Not in universe  ...                  Poland   \n",
              "199521                  Not in universe  ...           United-States   \n",
              "199522                    Other service  ...                       ?   \n",
              "\n",
              "       country_of_birth_mother country_of_birth_self  \\\n",
              "199518           United-States         United-States   \n",
              "199519           United-States         United-States   \n",
              "199520                  Poland               Germany   \n",
              "199521           United-States         United-States   \n",
              "199522                       ?                     ?   \n",
              "\n",
              "                                         citizenship  \\\n",
              "199518             Native- Born in the United States   \n",
              "199519             Native- Born in the United States   \n",
              "199520   Foreign born- U S citizen by naturalization   \n",
              "199521             Native- Born in the United States   \n",
              "199522           Foreign born- Not a citizen of U S    \n",
              "\n",
              "       own_business_or_self_employed  \\\n",
              "199518                             0   \n",
              "199519                             0   \n",
              "199520                             0   \n",
              "199521                             0   \n",
              "199522                             0   \n",
              "\n",
              "       fill_inc_questionnaire_for_veteran's_admin  veterans_benefits  \\\n",
              "199518                            Not in universe                  2   \n",
              "199519                            Not in universe                  2   \n",
              "199520                            Not in universe                  2   \n",
              "199521                            Not in universe                  2   \n",
              "199522                            Not in universe                  2   \n",
              "\n",
              "        weeks_worked_in_year  year income_level  \n",
              "199518                     0    95            0  \n",
              "199519                    52    94            0  \n",
              "199520                    52    95            0  \n",
              "199521                     0    95            0  \n",
              "199522                    52    94            0  \n",
              "\n",
              "[5 rows x 42 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Column names.\n",
        "CSV_HEADER = [\n",
        "    \"age\",\n",
        "    \"class_of_worker\",\n",
        "    \"detailed_industry_recode\",\n",
        "    \"detailed_occupation_recode\",\n",
        "    \"education\",\n",
        "    \"wage_per_hour\",\n",
        "    \"enroll_in_edu_inst_last_wk\",\n",
        "    \"marital_stat\",\n",
        "    \"major_industry_code\",\n",
        "    \"major_occupation_code\",\n",
        "    \"race\",\n",
        "    \"hispanic_origin\",\n",
        "    \"sex\",\n",
        "    \"member_of_a_labor_union\",\n",
        "    \"reason_for_unemployment\",\n",
        "    \"full_or_part_time_employment_stat\",\n",
        "    \"capital_gains\",\n",
        "    \"capital_losses\",\n",
        "    \"dividends_from_stocks\",\n",
        "    \"tax_filer_stat\",\n",
        "    \"region_of_previous_residence\",\n",
        "    \"state_of_previous_residence\",\n",
        "    \"detailed_household_and_family_stat\",\n",
        "    \"detailed_household_summary_in_household\",\n",
        "    \"instance_weight\",\n",
        "    \"migration_code-change_in_msa\",\n",
        "    \"migration_code-change_in_reg\",\n",
        "    \"migration_code-move_within_reg\",\n",
        "    \"live_in_this_house_1_year_ago\",\n",
        "    \"migration_prev_res_in_sunbelt\",\n",
        "    \"num_persons_worked_for_employer\",\n",
        "    \"family_members_under_18\",\n",
        "    \"country_of_birth_father\",\n",
        "    \"country_of_birth_mother\",\n",
        "    \"country_of_birth_self\",\n",
        "    \"citizenship\",\n",
        "    \"own_business_or_self_employed\",\n",
        "    \"fill_inc_questionnaire_for_veteran's_admin\",\n",
        "    \"veterans_benefits\",\n",
        "    \"weeks_worked_in_year\",\n",
        "    \"year\",\n",
        "    \"income_level\",\n",
        "]\n",
        "\n",
        "df_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/census-income.data.gz\"\n",
        "df = pd.read_csv(df_url, header=None, names=CSV_HEADER, nrows=nrows)#[['age','capital_losses', 'citizenship', 'income_level', \"instance_weight\"]] #tmp: subset cols\n",
        "\n",
        "\n",
        "test_df_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/census-income.test.gz\"\n",
        "test_df = pd.read_csv(test_df_url, header=None, names=CSV_HEADER, nrows=nrows)#[['age', 'capital_losses', 'citizenship', 'income_level', \"instance_weight\"]]\n",
        "\n",
        "print(f\"Data shape: {df.shape}\")\n",
        "print(f\"Test data shape: {test_df.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "label_col = \"income_level\"\n",
        "weight = \"instance_weight\"\n",
        "df[label_col] = df[label_col].apply(\n",
        "    lambda x: 0 if x == \" - 50000.\" else 1\n",
        ")\n",
        "test_df[label_col] = test_df[label_col].apply(\n",
        "    lambda x: 0 if x == \" - 50000.\" else 1\n",
        ")\n",
        "\n",
        "#save a copy\n",
        "df.to_csv(data_path+'df_income.csv')\n",
        "test_df.to_csv(data_path+'test_df_income.csv')\n",
        "\n",
        "#dbck\n",
        "df.tail()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4B228v3Tsh8",
        "outputId": "2658deb1-8adf-4178-c557-0d8d03fc079c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/multimodal_text_benchmark\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeKJET1oPTL6"
      },
      "source": [
        "## dataset1 (txt+img)\n",
        "\n",
        "Ref: https://keras.io/examples/nlp/multimodal_entailment/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "8TYULvo5PfN8",
        "outputId": "f98793c0-747e-45a2-bb16-8b1dfbd4bf4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['id_1', 'text_1', 'image_1', 'id_2', 'text_2', 'image_2', 'label'], dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-97f7cd6c-68bd-47e3-8350-6a3022be38d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_1</th>\n",
              "      <th>text_2</th>\n",
              "      <th>label</th>\n",
              "      <th>image_1_path</th>\n",
              "      <th>image_2_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1395</th>\n",
              "      <td>#HyderabadFC's two-goal cushion is restored as Joao Victor makes it 3-1 with a peach of a finish. \\n\\n Follow #CFCHFC live: https://t.co/FDFVZy9cNv #ISL https://t.co/bH5nazqeEc</td>\n",
              "      <td>#HyderabadFC is making merry here! Halicharan Narzary bags his second goal of the evening to hand his side a 4-1 lead. \\n\\nFollow #CFCHFC live: https://t.co/FDFVZy9cNv #ISL https://t.co/0iBUxjUk9D</td>\n",
              "      <td>NoEntailment</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1346118456538611713.jpg</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1346119737885949953.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1396</th>\n",
              "      <td>Basics Of Stock Market Online Program:\\nStock market is one of the be..For more info visit...https://t.co/aUMbf0wRwA https://t.co/qLNr0vCEmp</td>\n",
              "      <td>Basics Of Stock Market Online Program:\\nWant to have a strong foundat..For more info visit...https://t.co/S8pXJWdv2q https://t.co/mnFgk5hCNF</td>\n",
              "      <td>NoEntailment</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1359718246774022146.jpg</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1378597858790699009.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1397</th>\n",
              "      <td>466. ... Ne4\\nHalfmoves since capture or pawn advance: 32 https://t.co/CpWonN5Htn</td>\n",
              "      <td>534. ... Ng7\\nHalfmoves since capture or pawn advance: 18 https://t.co/0CD43xEnoz</td>\n",
              "      <td>NoEntailment</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1365034069474619394.png</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1373248592979369986.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1398</th>\n",
              "      <td>@HaasF1Team This is amazing!\\nAlmost as amazing as kicking Mazepin out of motorsports.\\n#WeSayNoToMazepin https://t.co/RjODrMBe5g</td>\n",
              "      <td>@HaasF1Team @nikita_mazepin Even in the simulator I bet he spun the car 🤣.   #MazeSPIN\\n#WeSayNoToMazepin https://t.co/7fXwVepV9x</td>\n",
              "      <td>NoEntailment</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1347182178619305985.jpg</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1382681029564059651.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>Vampz, let's help #Ace get to his crown so he can rule #WooyoungKingdom!\\n\\n🔵qRT with your answer and a little message, and remember to use the TAGs properly!\\n\\n@twt_VAV + @VAV_official \\n\\n#VAV | #브이에이브이| #에이스 don't forget! https://t.co/fLsmJjS9jE</td>\n",
              "      <td>Vampz, let's help #StVan get to our little princess @VavCash !\\n\\n🔵qRT with your answer and a little message, and remember to use the TAGs properly!\\n\\n@twt_VAV + @VAV_official \\n\\n#VAV | #브이에이브이| #세인트반 don't forget! https://t.co/dhgMFm14Mo</td>\n",
              "      <td>NoEntailment</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1371112589728567300.jpg</td>\n",
              "      <td>/root/.keras/datasets/tweet_images/1371644568794451968.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97f7cd6c-68bd-47e3-8350-6a3022be38d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-97f7cd6c-68bd-47e3-8350-6a3022be38d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-97f7cd6c-68bd-47e3-8350-6a3022be38d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                         text_1  \\\n",
              "1395                                                                           #HyderabadFC's two-goal cushion is restored as Joao Victor makes it 3-1 with a peach of a finish. \\n\\n Follow #CFCHFC live: https://t.co/FDFVZy9cNv #ISL https://t.co/bH5nazqeEc   \n",
              "1396                                                                                                               Basics Of Stock Market Online Program:\\nStock market is one of the be..For more info visit...https://t.co/aUMbf0wRwA https://t.co/qLNr0vCEmp   \n",
              "1397                                                                                                                                                                          466. ... Ne4\\nHalfmoves since capture or pawn advance: 32 https://t.co/CpWonN5Htn   \n",
              "1398                                                                                                                          @HaasF1Team This is amazing!\\nAlmost as amazing as kicking Mazepin out of motorsports.\\n#WeSayNoToMazepin https://t.co/RjODrMBe5g   \n",
              "1399  Vampz, let's help #Ace get to his crown so he can rule #WooyoungKingdom!\\n\\n🔵qRT with your answer and a little message, and remember to use the TAGs properly!\\n\\n@twt_VAV + @VAV_official \\n\\n#VAV | #브이에이브이| #에이스 don't forget! https://t.co/fLsmJjS9jE   \n",
              "\n",
              "                                                                                                                                                                                                                                                text_2  \\\n",
              "1395                                              #HyderabadFC is making merry here! Halicharan Narzary bags his second goal of the evening to hand his side a 4-1 lead. \\n\\nFollow #CFCHFC live: https://t.co/FDFVZy9cNv #ISL https://t.co/0iBUxjUk9D   \n",
              "1396                                                                                                      Basics Of Stock Market Online Program:\\nWant to have a strong foundat..For more info visit...https://t.co/S8pXJWdv2q https://t.co/mnFgk5hCNF   \n",
              "1397                                                                                                                                                                 534. ... Ng7\\nHalfmoves since capture or pawn advance: 18 https://t.co/0CD43xEnoz   \n",
              "1398                                                                                                                 @HaasF1Team @nikita_mazepin Even in the simulator I bet he spun the car 🤣.   #MazeSPIN\\n#WeSayNoToMazepin https://t.co/7fXwVepV9x   \n",
              "1399  Vampz, let's help #StVan get to our little princess @VavCash !\\n\\n🔵qRT with your answer and a little message, and remember to use the TAGs properly!\\n\\n@twt_VAV + @VAV_official \\n\\n#VAV | #브이에이브이| #세인트반 don't forget! https://t.co/dhgMFm14Mo   \n",
              "\n",
              "             label  \\\n",
              "1395  NoEntailment   \n",
              "1396  NoEntailment   \n",
              "1397  NoEntailment   \n",
              "1398  NoEntailment   \n",
              "1399  NoEntailment   \n",
              "\n",
              "                                                    image_1_path  \\\n",
              "1395  /root/.keras/datasets/tweet_images/1346118456538611713.jpg   \n",
              "1396  /root/.keras/datasets/tweet_images/1359718246774022146.jpg   \n",
              "1397  /root/.keras/datasets/tweet_images/1365034069474619394.png   \n",
              "1398  /root/.keras/datasets/tweet_images/1347182178619305985.jpg   \n",
              "1399  /root/.keras/datasets/tweet_images/1371112589728567300.jpg   \n",
              "\n",
              "                                                    image_2_path  \n",
              "1395  /root/.keras/datasets/tweet_images/1346119737885949953.jpg  \n",
              "1396  /root/.keras/datasets/tweet_images/1378597858790699009.jpg  \n",
              "1397  /root/.keras/datasets/tweet_images/1373248592979369986.png  \n",
              "1398  /root/.keras/datasets/tweet_images/1382681029564059651.jpg  \n",
              "1399  /root/.keras/datasets/tweet_images/1371644568794451968.jpg  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "image_base_path = tf.keras.utils.get_file(\n",
        "    \"tweet_images\",\n",
        "    \"https://github.com/sayakpaul/Multimodal-Entailment-Baseline/releases/download/v1.0.0/tweet_images.tar.gz\",\n",
        "    untar=True,\n",
        ")\n",
        "\n",
        "df = pd.read_csv(\"https://github.com/sayakpaul/Multimodal-Entailment-Baseline/raw/main/csvs/tweets.csv\", nrows=nrows)\n",
        "\n",
        "images_one_paths = []\n",
        "images_two_paths = []\n",
        "\n",
        "for idx in range(len(df)):\n",
        "    current_row = df.iloc[idx]\n",
        "    id_1 = current_row[\"id_1\"]\n",
        "    id_2 = current_row[\"id_2\"]\n",
        "    extentsion_one = current_row[\"image_1\"].split(\".\")[-1]\n",
        "    extentsion_two = current_row[\"image_2\"].split(\".\")[-1]\n",
        "\n",
        "    image_one_path = os.path.join(image_base_path, str(id_1) + f\".{extentsion_one}\")\n",
        "    image_two_path = os.path.join(image_base_path, str(id_2) + f\".{extentsion_two}\")\n",
        "\n",
        "    images_one_paths.append(image_one_path)\n",
        "    images_two_paths.append(image_two_path)\n",
        "print(df.columns)\n",
        "df[\"image_1_path\"] = images_one_paths\n",
        "df[\"image_2_path\"] = images_two_paths\n",
        "\n",
        "df.drop(['image_1', 'image_2', 'id_1', 'id_2'], axis=1, inplace=True)\n",
        "label_col = 'label'\n",
        "weight = None\n",
        "num_classes = len(df[label_col].value_counts())\n",
        "img_path = '/root/.keras/datasets/tweet_images'\n",
        "img_cols = [\"image_1_path\", \"image_2_path\"]\n",
        "\n",
        "\n",
        "#save a copy\n",
        "df.to_csv(data_path+'df_entailment.csv')\n",
        "\n",
        "#dbck\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6Upr4kgPheb"
      },
      "source": [
        "## dataset2 (cnt, cat, txt)\n",
        "\n",
        "The original task in Kaggle's <a href=\"https://www.kaggle.com/c/petfinder-adoption-prediction\" class=\"external\">PetFinder.my Adoption Prediction competition</a> was to predict the speed at which a pet will be adopted (e.g. in the first week, the first month, the first three months, and so on)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ftdSnbjRPo9z",
        "outputId": "a0321985-6417-470d-d4ea-8bbd1971b521"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip\n",
            "1671168/1668792 [==============================] - 0s 0us/step\n",
            "1679360/1668792 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9cf0361d-0529-41b6-b291-b586773cbfd1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type</th>\n",
              "      <th>Age</th>\n",
              "      <th>Breed1</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Color1</th>\n",
              "      <th>Color2</th>\n",
              "      <th>MaturitySize</th>\n",
              "      <th>FurLength</th>\n",
              "      <th>Vaccinated</th>\n",
              "      <th>Sterilized</th>\n",
              "      <th>Health</th>\n",
              "      <th>Fee</th>\n",
              "      <th>Description</th>\n",
              "      <th>PhotoAmt</th>\n",
              "      <th>AdoptionSpeed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11532</th>\n",
              "      <td>Dog</td>\n",
              "      <td>24</td>\n",
              "      <td>Poodle</td>\n",
              "      <td>Male</td>\n",
              "      <td>Brown</td>\n",
              "      <td>Golden</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Not Sure</td>\n",
              "      <td>No</td>\n",
              "      <td>Healthy</td>\n",
              "      <td>0</td>\n",
              "      <td>been at my place for a while..am hoping to find it a good home</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11533</th>\n",
              "      <td>Cat</td>\n",
              "      <td>1</td>\n",
              "      <td>Domestic Short Hair</td>\n",
              "      <td>Female</td>\n",
              "      <td>Cream</td>\n",
              "      <td>Gray</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Short</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Healthy</td>\n",
              "      <td>0</td>\n",
              "      <td>1 month old white + grey kitten for adoption near HUKM, KL, near Bdr Tun Razak Gender / medical record + costs To Be Confirmed. Adopter MUST commit to NEUTER kitten when it is : * 4-6 months old * on heat whichever comes first, provided that it is * 1.4 kg weight MINIMUM Whatsapp for adopption / FREE gift / startup kit / Sign up contract to buy supplies from Pets + Strays, comes with FREE gifts whilst stocks last</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11534</th>\n",
              "      <td>Dog</td>\n",
              "      <td>6</td>\n",
              "      <td>Schnauzer</td>\n",
              "      <td>Female</td>\n",
              "      <td>Black</td>\n",
              "      <td>White</td>\n",
              "      <td>Small</td>\n",
              "      <td>Long</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Healthy</td>\n",
              "      <td>0</td>\n",
              "      <td>ooooo</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11535</th>\n",
              "      <td>Cat</td>\n",
              "      <td>9</td>\n",
              "      <td>Domestic Short Hair</td>\n",
              "      <td>Female</td>\n",
              "      <td>Yellow</td>\n",
              "      <td>White</td>\n",
              "      <td>Small</td>\n",
              "      <td>Short</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Healthy</td>\n",
              "      <td>0</td>\n",
              "      <td>she is very shy..adventures and independent..she just hates cages..but loves climbing trees and rooftops..however she is very loving.</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11536</th>\n",
              "      <td>Dog</td>\n",
              "      <td>1</td>\n",
              "      <td>Mixed Breed</td>\n",
              "      <td>Male</td>\n",
              "      <td>Brown</td>\n",
              "      <td>No Color</td>\n",
              "      <td>Medium</td>\n",
              "      <td>Short</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Healthy</td>\n",
              "      <td>0</td>\n",
              "      <td>Fili just loves laying around and also loves being under the sun; Very laidback and quiet.</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9cf0361d-0529-41b6-b291-b586773cbfd1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9cf0361d-0529-41b6-b291-b586773cbfd1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9cf0361d-0529-41b6-b291-b586773cbfd1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Type  Age               Breed1  Gender  Color1    Color2 MaturitySize  \\\n",
              "11532  Dog   24               Poodle    Male   Brown    Golden       Medium   \n",
              "11533  Cat    1  Domestic Short Hair  Female   Cream      Gray       Medium   \n",
              "11534  Dog    6            Schnauzer  Female   Black     White        Small   \n",
              "11535  Cat    9  Domestic Short Hair  Female  Yellow     White        Small   \n",
              "11536  Dog    1          Mixed Breed    Male   Brown  No Color       Medium   \n",
              "\n",
              "      FurLength Vaccinated Sterilized   Health  Fee  \\\n",
              "11532    Medium   Not Sure         No  Healthy    0   \n",
              "11533     Short         No         No  Healthy    0   \n",
              "11534      Long        Yes         No  Healthy    0   \n",
              "11535     Short        Yes        Yes  Healthy    0   \n",
              "11536     Short         No         No  Healthy    0   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                            Description  \\\n",
              "11532                                                                                                                                                                                                                                                                                                                                                                    been at my place for a while..am hoping to find it a good home   \n",
              "11533  1 month old white + grey kitten for adoption near HUKM, KL, near Bdr Tun Razak Gender / medical record + costs To Be Confirmed. Adopter MUST commit to NEUTER kitten when it is : * 4-6 months old * on heat whichever comes first, provided that it is * 1.4 kg weight MINIMUM Whatsapp for adopption / FREE gift / startup kit / Sign up contract to buy supplies from Pets + Strays, comes with FREE gifts whilst stocks last   \n",
              "11534                                                                                                                                                                                                                                                                                                                                                                                                                             ooooo   \n",
              "11535                                                                                                                                                                                                                                                                                             she is very shy..adventures and independent..she just hates cages..but loves climbing trees and rooftops..however she is very loving.   \n",
              "11536                                                                                                                                                                                                                                                                                                                                        Fili just loves laying around and also loves being under the sun; Very laidback and quiet.   \n",
              "\n",
              "       PhotoAmt  AdoptionSpeed  \n",
              "11532         0              4  \n",
              "11533         1              3  \n",
              "11534         1              0  \n",
              "11535         3              4  \n",
              "11536         1              3  "
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\n",
        "csv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\n",
        "\n",
        "tf.keras.utils.get_file('petfinder_mini.zip', dataset_url,\n",
        "                        extract=True, cache_dir='.')\n",
        "df = pd.read_csv(csv_file, nrows=nrows)\n",
        "\n",
        "label_col = 'AdoptionSpeed'\n",
        "\n",
        "#save a copy\n",
        "df.to_csv(data_path+'df_adoption.csv')\n",
        "\n",
        "#dbck\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJNkGbElPs-L"
      },
      "source": [
        "## dataset3 (tab+txt)\n",
        "\n",
        "Ref: https://github.com/sxjscience/automl_multimodal_benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbKBQrouP7b_",
        "outputId": "a3f0063e-fc27-4cc0-d292-15a9f93516d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/multimodal_text_benchmark\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "data_path = Path('../multimodal_text_benchmark/') #/drive/My Drive\n",
        "os.chdir(data_path)\n",
        "\n",
        "#dbck\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "meSrUibJP7gn",
        "outputId": "caa6f8b1-98f4-42f5-98d8-53a949e80790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/drive/MyDrive/multimodal_text_benchmark\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from auto-mm-bench==1.0.0.dev20220329) (1.0.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.21.28-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 16.5 MB/s \n",
            "\u001b[?25hCollecting javalang>=0.13.0\n",
            "  Downloading javalang-0.13.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from auto-mm-bench==1.0.0.dev20220329) (3.1.0)\n",
            "Collecting yacs>=0.1.8\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from auto-mm-bench==1.0.0.dev20220329) (3.17.3)\n",
            "Collecting unidiff\n",
            "  Downloading unidiff-0.7.3-py2.py3-none-any.whl (14 kB)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 59.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from auto-mm-bench==1.0.0.dev20220329) (4.63.0)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.7/dist-packages (from auto-mm-bench==1.0.0.dev20220329) (0.18.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from auto-mm-bench==1.0.0.dev20220329) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from auto-mm-bench==1.0.0.dev20220329) (2.23.0)\n",
            "Collecting jsonlines\n",
            "  Downloading jsonlines-3.0.0-py3-none-any.whl (8.5 kB)\n",
            "Collecting contextvars\n",
            "  Downloading contextvars-2.4.tar.gz (9.6 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from auto-mm-bench==1.0.0.dev20220329) (6.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from auto-mm-bench==1.0.0.dev20220329) (1.3.5)\n",
            "Collecting py-cpuinfo\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 11.6 MB/s \n",
            "\u001b[?25hCollecting fasttext!=0.9.2,>=0.9.1\n",
            "  Downloading fasttext-0.9.1.tar.gz (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.1-py2.py3-none-any.whl (211 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext!=0.9.2,>=0.9.1->auto-mm-bench==1.0.0.dev20220329) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext!=0.9.2,>=0.9.1->auto-mm-bench==1.0.0.dev20220329) (1.21.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.10.0->auto-mm-bench==1.0.0.dev20220329) (1.5.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from javalang>=0.13.0->auto-mm-bench==1.0.0.dev20220329) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from yacs>=0.1.8->auto-mm-bench==1.0.0.dev20220329) (3.13)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.3 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting botocore<1.25.0,>=1.24.28\n",
            "  Downloading botocore-1.24.28-py3-none-any.whl (8.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 65.3 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 74.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.28->boto3->auto-mm-bench==1.0.0.dev20220329) (2.8.2)\n",
            "Collecting immutables>=0.9\n",
            "  Downloading immutables-0.17-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (116 kB)\n",
            "\u001b[K     |████████████████████████████████| 116 kB 68.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from immutables>=0.9->contextvars->auto-mm-bench==1.0.0.dev20220329) (3.10.0.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from jsonlines->auto-mm-bench==1.0.0.dev20220329) (21.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->auto-mm-bench==1.0.0.dev20220329) (2018.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->auto-mm-bench==1.0.0.dev20220329) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->auto-mm-bench==1.0.0.dev20220329) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->auto-mm-bench==1.0.0.dev20220329) (2021.10.8)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 77.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: fasttext, contextvars, py-cpuinfo\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp37-cp37m-linux_x86_64.whl size=2497842 sha256=e1bcc3031860c8602d683f0d7e0860e57de703194ac43c2fb98306eb3ba414dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/5b/4b/9c582c778bb93aaad8fc855d5e79f49eae34f59e363a22c422\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-py3-none-any.whl size=7681 sha256=efc84482115aab5616c1b05f410b427dd8263a84f4f3565bd497c17323e4ff1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/11/79/e70e668095c0bb1f94718af672ef2d35ee7a023fee56ef54d9\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=437c7b02bc61ce35f6ce8b824ed8e32a5542d970b791ec34c94c563952c17934\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "Successfully built fasttext contextvars py-cpuinfo\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, pybind11, immutables, yacs, unidiff, sentencepiece, py-cpuinfo, jsonlines, javalang, fasttext, contextvars, boto3, auto-mm-bench\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Running setup.py develop for auto-mm-bench\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed auto-mm-bench-1.0.0.dev20220329 boto3-1.21.28 botocore-1.24.28 contextvars-2.4 fasttext-0.9.1 immutables-0.17 javalang-0.13.0 jmespath-1.0.0 jsonlines-3.0.0 py-cpuinfo-8.0.0 pybind11-2.9.1 s3transfer-0.5.2 sentencepiece-0.1.96 unidiff-0.7.3 urllib3-1.25.11 yacs-0.1.8\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "contextvars",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Install the benchmarking suite\n",
        "!pip install -U -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UWPQrf_P7jV",
        "outputId": "6e87793f-c788-4307-8881-61b57b29e194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----all available datasets = ['product_sentiment_machine_hack', 'melbourne_airbnb', 'news_channel', 'wine_reviews', 'imdb_genre_prediction', 'jigsaw_unintended_bias100K', 'fake_job_postings2', 'kick_starter_funding', 'ae_price_prediction', 'google_qa_answer_type_reason_explanation', 'google_qa_question_type_reason_explanation', 'women_clothing_review', 'mercari_price_suggestion100K', 'jc_penney_products', 'news_popularity2', 'bookprice_prediction', 'data_scientist_salary', 'california_house_price']\n",
            "=====selected dataset_name=data_scientist_salary\n"
          ]
        }
      ],
      "source": [
        "# view all available datasets\n",
        "from auto_mm_bench.datasets import create_dataset, TEXT_BENCHMARK_ALIAS_MAPPING\n",
        "datasets = list(TEXT_BENCHMARK_ALIAS_MAPPING.values())\n",
        "print(f'-----all available datasets = {datasets}')\n",
        "\n",
        "# select a dataset\n",
        "dataset_name = datasets[-2]\n",
        "print(f'=====selected dataset_name={dataset_name}')\n",
        "from auto_mm_bench.datasets import dataset_registry\n",
        "train_dataset = dataset_registry.create(dataset_name, 'train')\n",
        "test_dataset = dataset_registry.create(dataset_name, 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "CLbOR918QMV4",
        "outputId": "50416ff8-d21c-4b67-9795-4e2fe63fcb47"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-17f52b08-289c-469e-9c2a-a94af16e48d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>experience</th>\n",
              "      <th>job_description</th>\n",
              "      <th>job_desig</th>\n",
              "      <th>job_type</th>\n",
              "      <th>key_skills</th>\n",
              "      <th>location</th>\n",
              "      <th>salary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15836</th>\n",
              "      <td>1-2 yrs</td>\n",
              "      <td>- Analyze &amp; interpret data and communicate results to clients, often with the aid of mathematical/sta ...</td>\n",
              "      <td>Business Analyst - KPO - IIT / NIT/ Nsit/ DSE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Statistical Modeling, Data Analysis, Business Analysis, Data exploration...</td>\n",
              "      <td>Gurgaon</td>\n",
              "      <td>10to15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15837</th>\n",
              "      <td>3-8 yrs</td>\n",
              "      <td>- Experience in object oriented programming / functional programming;- Strong visual design skills and ...</td>\n",
              "      <td>Front End Developer/ui Developer - Javascript/html</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UI Development, Javascript, HTML, Html5, Photoshop, Visual Design...</td>\n",
              "      <td>Mumbai</td>\n",
              "      <td>15to25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15838</th>\n",
              "      <td>6-10 yrs</td>\n",
              "      <td>Should be ready to travel on short or medium term assignments on need basis;A good team player;Able to ...</td>\n",
              "      <td>Business Analyst</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test Planning, Test Strategy, Requirement Gathering, Defect Management...</td>\n",
              "      <td>Bengaluru</td>\n",
              "      <td>10to15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15839</th>\n",
              "      <td>3-6 yrs</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Associate Manager - Deal Advisory / M&amp;A - Business Research</td>\n",
              "      <td>NaN</td>\n",
              "      <td>mba finance, company profiling, pitch books, project management, capital iq...</td>\n",
              "      <td>Noida</td>\n",
              "      <td>10to15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15840</th>\n",
              "      <td>2-6 yrs</td>\n",
              "      <td>Good understanding of off-balance sheet adjustments and performing distressed debt modeling and recovery ...</td>\n",
              "      <td>Aranca | Opportunity for Fixed Income Credit Research Analyst</td>\n",
              "      <td>NaN</td>\n",
              "      <td>credit research, fixed income, High yield bond, HY bond, distressed debt...</td>\n",
              "      <td>Mumbai, Gurgaon</td>\n",
              "      <td>10to15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17f52b08-289c-469e-9c2a-a94af16e48d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-17f52b08-289c-469e-9c2a-a94af16e48d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-17f52b08-289c-469e-9c2a-a94af16e48d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      experience  \\\n",
              "15836    1-2 yrs   \n",
              "15837    3-8 yrs   \n",
              "15838   6-10 yrs   \n",
              "15839    3-6 yrs   \n",
              "15840    2-6 yrs   \n",
              "\n",
              "                                                                                                    job_description  \\\n",
              "15836     - Analyze & interpret data and communicate results to clients, often with the aid of mathematical/sta ...   \n",
              "15837    - Experience in object oriented programming / functional programming;- Strong visual design skills and ...   \n",
              "15838    Should be ready to travel on short or medium term assignments on need basis;A good team player;Able to ...   \n",
              "15839                                                                                                           NaN   \n",
              "15840  Good understanding of off-balance sheet adjustments and performing distressed debt modeling and recovery ...   \n",
              "\n",
              "                                                           job_desig job_type  \\\n",
              "15836                  Business Analyst - KPO - IIT / NIT/ Nsit/ DSE      NaN   \n",
              "15837             Front End Developer/ui Developer - Javascript/html      NaN   \n",
              "15838                                               Business Analyst      NaN   \n",
              "15839    Associate Manager - Deal Advisory / M&A - Business Research      NaN   \n",
              "15840  Aranca | Opportunity for Fixed Income Credit Research Analyst      NaN   \n",
              "\n",
              "                                                                           key_skills  \\\n",
              "15836     Statistical Modeling, Data Analysis, Business Analysis, Data exploration...   \n",
              "15837            UI Development, Javascript, HTML, Html5, Photoshop, Visual Design...   \n",
              "15838       Test Planning, Test Strategy, Requirement Gathering, Defect Management...   \n",
              "15839  mba finance, company profiling, pitch books, project management, capital iq...   \n",
              "15840     credit research, fixed income, High yield bond, HY bond, distressed debt...   \n",
              "\n",
              "              location  salary  \n",
              "15836          Gurgaon  10to15  \n",
              "15837           Mumbai  15to25  \n",
              "15838        Bengaluru  10to15  \n",
              "15839            Noida  10to15  \n",
              "15840  Mumbai, Gurgaon  10to15  "
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = train_dataset.data.head(nrows)\n",
        "df_test = test_dataset.data.head(nrows)\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkfwMvWtQMW2",
        "outputId": "38cacaf1-8520-4a94-8789-d92eb0004225"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['experience',\n",
              "  'job_description',\n",
              "  'job_desig',\n",
              "  'job_type',\n",
              "  'key_skills',\n",
              "  'location'],\n",
              " 'salary')"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#define global variables: df, x_cols, label_col, num_classes\n",
        "x_cols = train_dataset.feature_columns\n",
        "label_col = train_dataset.label_columns[0]\n",
        "x_cols, label_col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuEW4-hwWJdQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "#save a copy\n",
        "df.to_csv(data_path+'df_salary.csv')\n",
        "test_df.to_csv(data_path+'test_df_salary.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YzZPTKDJvnS"
      },
      "source": [
        "## dataset4 (tab, txt) skippable meeting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "DiY1WbyPih2y",
        "outputId": "f4b6a708-81d7-4ba3-e1a8-2af8ab270572"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e4aa71c7-a4fb-4db0-a1a7-1c1bef1d70ae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>attendee_email</th>\n",
              "      <th>length_of_service</th>\n",
              "      <th>response_status</th>\n",
              "      <th>is_organizer</th>\n",
              "      <th>meeting_lapse</th>\n",
              "      <th>time_since_last_promotion</th>\n",
              "      <th>time_since_new_org_start_date</th>\n",
              "      <th>cumulative_peer_exit_count</th>\n",
              "      <th>age</th>\n",
              "      <th>job_family</th>\n",
              "      <th>is_optional</th>\n",
              "      <th>start_datetime</th>\n",
              "      <th>num_direct_reports</th>\n",
              "      <th>event_id</th>\n",
              "      <th>manager_length_of_service</th>\n",
              "      <th>start_datetime_Dayofyear</th>\n",
              "      <th>time_since_new_manager_start_date</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>517980</th>\n",
              "      <td>FEN studies UKR refugee working group</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ratchfoj@gene.com</td>\n",
              "      <td>1.15731</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>1.162</td>\n",
              "      <td>4.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>Clinical Operations</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2022-03-25 10:30:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72273768</td>\n",
              "      <td>1.78288</td>\n",
              "      <td>84.0</td>\n",
              "      <td>1.162</td>\n",
              "      <td>1.648229e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517981</th>\n",
              "      <td>LPT Field Execution Meeting</td>\n",
              "      <td>NaN</td>\n",
              "      <td>kfrank@gene.com</td>\n",
              "      <td>15.94680</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>1.748</td>\n",
              "      <td>25.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>Sales</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2022-03-25 10:30:00</td>\n",
              "      <td>5.0</td>\n",
              "      <td>72305450</td>\n",
              "      <td>20.31030</td>\n",
              "      <td>84.0</td>\n",
              "      <td>1.748</td>\n",
              "      <td>1.648229e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517982</th>\n",
              "      <td>FEN studies UKR refugee working group</td>\n",
              "      <td>NaN</td>\n",
              "      <td>nunnc3@gene.com</td>\n",
              "      <td>4.94680</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>2.000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>Clinical Development</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2022-03-25 10:30:00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>72258593</td>\n",
              "      <td>12.25270</td>\n",
              "      <td>84.0</td>\n",
              "      <td>2.000</td>\n",
              "      <td>1.648229e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517983</th>\n",
              "      <td>FEN studies UKR refugee working group</td>\n",
              "      <td>NaN</td>\n",
              "      <td>rgee2@gene.com</td>\n",
              "      <td>17.94680</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>3.584</td>\n",
              "      <td>4.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>IT Portfolio and Project Management</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2022-03-25 10:30:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72273768</td>\n",
              "      <td>3.61073</td>\n",
              "      <td>84.0</td>\n",
              "      <td>3.584</td>\n",
              "      <td>1.648229e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517984</th>\n",
              "      <td>iNest HIT overview</td>\n",
              "      <td>NaN</td>\n",
              "      <td>szejkom@gene.com</td>\n",
              "      <td>11.02470</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>29.333333</td>\n",
              "      <td>0.329</td>\n",
              "      <td>15.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>IT Portfolio and Project Management</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2022-03-25 10:30:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>72262832</td>\n",
              "      <td>6.02740</td>\n",
              "      <td>84.0</td>\n",
              "      <td>0.329</td>\n",
              "      <td>1.648229e+09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e4aa71c7-a4fb-4db0-a1a7-1c1bef1d70ae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e4aa71c7-a4fb-4db0-a1a7-1c1bef1d70ae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e4aa71c7-a4fb-4db0-a1a7-1c1bef1d70ae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                          title description  \\\n",
              "517980  FEN studies UKR refugee working group           NaN   \n",
              "517981              LPT Field Execution Meeting         NaN   \n",
              "517982  FEN studies UKR refugee working group           NaN   \n",
              "517983  FEN studies UKR refugee working group           NaN   \n",
              "517984                       iNest HIT overview         NaN   \n",
              "\n",
              "           attendee_email  length_of_service  response_status  is_organizer  \\\n",
              "517980  ratchfoj@gene.com            1.15731                1           1.0   \n",
              "517981    kfrank@gene.com           15.94680                1           0.0   \n",
              "517982    nunnc3@gene.com            4.94680                1           0.0   \n",
              "517983     rgee2@gene.com           17.94680                1           0.0   \n",
              "517984   szejkom@gene.com           11.02470                1           1.0   \n",
              "\n",
              "        meeting_lapse  time_since_last_promotion  \\\n",
              "517980       0.666667                  -1.000000   \n",
              "517981       1.000000                  10.000000   \n",
              "517982       0.666667                  -1.000000   \n",
              "517983       0.666667                  -1.000000   \n",
              "517984       0.750000                  29.333333   \n",
              "\n",
              "        time_since_new_org_start_date  cumulative_peer_exit_count   age  \\\n",
              "517980                          1.162                         4.0  48.0   \n",
              "517981                          1.748                        25.0  43.0   \n",
              "517982                          2.000                         2.0  42.0   \n",
              "517983                          3.584                         4.0  51.0   \n",
              "517984                          0.329                        15.0  41.0   \n",
              "\n",
              "                                 job_family  is_optional       start_datetime  \\\n",
              "517980                  Clinical Operations          0.0  2022-03-25 10:30:00   \n",
              "517981                                Sales          0.0  2022-03-25 10:30:00   \n",
              "517982                 Clinical Development          0.0  2022-03-25 10:30:00   \n",
              "517983  IT Portfolio and Project Management          0.0  2022-03-25 10:30:00   \n",
              "517984  IT Portfolio and Project Management          0.0  2022-03-25 10:30:00   \n",
              "\n",
              "        num_direct_reports  event_id  manager_length_of_service  \\\n",
              "517980                 0.0  72273768                    1.78288   \n",
              "517981                 5.0  72305450                   20.31030   \n",
              "517982                 2.0  72258593                   12.25270   \n",
              "517983                 0.0  72273768                    3.61073   \n",
              "517984                 0.0  72262832                    6.02740   \n",
              "\n",
              "        start_datetime_Dayofyear  time_since_new_manager_start_date  \\\n",
              "517980                      84.0                              1.162   \n",
              "517981                      84.0                              1.748   \n",
              "517982                      84.0                              2.000   \n",
              "517983                      84.0                              3.584   \n",
              "517984                      84.0                              0.329   \n",
              "\n",
              "           timestamp  \n",
              "517980  1.648229e+09  \n",
              "517981  1.648229e+09  \n",
              "517982  1.648229e+09  \n",
              "517983  1.648229e+09  \n",
              "517984  1.648229e+09  "
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(data_path+'iu_2022_101_325.csv', index_col=0)\n",
        "label_col = 'response_status'\n",
        "df.tail()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuVKlYm1izfP",
        "outputId": "b1d2d2c5-f859-4f95-df00-b5524be7c5f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['title',\n",
              " 'description',\n",
              " 'attendee_email',\n",
              " 'length_of_service',\n",
              " 'response_status',\n",
              " 'is_organizer',\n",
              " 'meeting_lapse',\n",
              " 'time_since_last_promotion',\n",
              " 'time_since_new_org_start_date',\n",
              " 'cumulative_peer_exit_count',\n",
              " 'age',\n",
              " 'job_family',\n",
              " 'is_optional',\n",
              " 'start_datetime',\n",
              " 'num_direct_reports',\n",
              " 'event_id',\n",
              " 'manager_length_of_service',\n",
              " 'start_datetime_Dayofyear',\n",
              " 'time_since_new_manager_start_date',\n",
              " 'timestamp']"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFqn80Dsd8a1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5MPha3r5-vS4",
        "8QAyThfFRCfY",
        "T3DKq1fFleje",
        "TD6g_p6Oha8w",
        "L7MSpbgZ1BgY",
        "Gtb7Pz_8fjxl",
        "zNdIjWi4qYLI",
        "_nCCqfiH217a",
        "hPxdDr_h0JCT"
      ],
      "machine_shape": "hm",
      "name": "fastai_multimodal.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}